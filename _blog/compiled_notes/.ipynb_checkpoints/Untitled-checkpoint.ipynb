{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Copies files from notes directory, does some preprocessing, then builds them into a website using jupyter-book\n",
    "'''\n",
    "\n",
    "import shutil   \n",
    "import os\n",
    "from os.path import join as oj\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source / dest\n",
    "src = '/Users/chandan/website/_notes/'  \n",
    "dest = 'notes'\n",
    "\n",
    "# copy stuff\n",
    "try:\n",
    "    shutil.rmtree(dest) # rm\n",
    "except:\n",
    "    pass\n",
    "destination = shutil.copytree(src, dest)   # copy\n",
    "shutil.rmtree(f'{dest}/talks/') # rm\n",
    "\n",
    "\n",
    "# toc\n",
    "toc = \\\n",
    "'''- file: intro\n",
    "  numbered: true\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the files\n",
    "contents = []\n",
    "fnames = []\n",
    "for folder in os.listdir(dest):\n",
    "    if not '.' in folder and not 'cheat' in folder and not 'assets' in folder and not 'misc' in folder:\n",
    "        header_file = folder.lower() + '.md'\n",
    "        header_fname = oj(dest, folder, header_file)\n",
    "        open(header_fname, \"w\").write('# ' + folder)\n",
    "        toc += f'- file: {header_fname}\\n  sections:\\n'\n",
    "        for fname in os.listdir(oj(dest, folder)):\n",
    "            if fname.endswith('.md') and not fname == header_file:\n",
    "                fpath = oj(dest, folder, fname)\n",
    "                content = open(fpath, \"r\").read().replace('# ', '## ')\n",
    "                try:\n",
    "                    title = content.split('title:', 1)[1].split('\\n')[0].lower()\n",
    "                    content = content.replace('{:toc}', f'# {title}')\n",
    "                    open(fpath, \"w\").write(content)\n",
    "                    toc += f'  - file: {fpath}\\n'\n",
    "                except:\n",
    "                    os.remove(fpath)\n",
    "                contents.append(content)\n",
    "                fnames.append(fname[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [x.replace('_', ' ') for x in fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---\\nlayout: notes\\ntitle: comp neuro\\ncategory: research\\n---\\n\\n#  comp neuro\\n\\n## navigation\\n\\n- cognitive maps (tolman 1940s) - idea that rats in mazes learn spatial maps\\n- **place cells** (o\\'keefe 1971) - in the hippocampus - fire to indicate one\\'s current location\\n- remap to new locations\\n- **grid cells** (moser & moser 2005) - in the entorhinal cotex (provides inputs to the hippocampus) - not particular locations but rather hexagonal coordinate system\\n- grid cells fire if the mouse is in any location at the vertex (or center) of one of the hexagons\\n- ![Screen Shot 2019-05-10 at 1.25.02 PM](../assets/mouse.png)\\n- there are grid cells with larger/smaller hexagons, different orientations, different offsets\\n- can look for grid cells signature in fmri: https://www.nature.com/articles/nature08704\\n- other places with grid cell-like behavior\\n- eye movement task\\n- some evidence for \"time cells\" like place cells for time\\n- sound frequency task https://www.nature.com/articles/nature21692\\n- 2d \"bird space\" [task](https://science.sciencemag.org/content/352/6292/1464.full?ijkey=sXaWNaNjkIcik&keytype=ref&siteid=sci)\\n\\n## high-dimensional computing\\n\\n- high-level overview\\n  - current inspiration has all come from single neurons at a time - hd computing is going past this\\n  - the brain\\'s circuits are high-dimensional\\n  - elements are stochastic not deterministic\\n  - can learn from experience\\n  - no 2 brains are alike yet they exhibit the same behavior\\n- basic question of comp neuro: what kind of computing can explain behavior produced by spike trains?\\n  - recognizing ppl by how they look, sound, or behave\\n  - learning from examples\\n  - remembering things going back to childhood\\n  - communicating with language\\n- [HD computing overview paper](https://link.springer.com/content/pdf/10.1007/s12559-009-9009-8.pdf)\\n  - in these high dimensions, most points are close to equidistant from one another (L1 distance), and are approximately orthogonal (dot product is 0)\\n  - memory\\n    - *heteroassociative* - can return stored *X* based on its address *A*\\n    - *autoassociative* - can return stored *X* based on a noisy version of *X* (since it is a point attractor), maybe with some iteration\\n      - this adds robustness to the memory\\n      - this also removes the need for addresses altogether\\n\\n### definitions\\n\\n- what is hd computing?\\n  - compute with random high-dim vectors\\n  - ex. 10k vectors A, B of +1/-1 (also extends to real / complex vectors)\\n- 3 operations\\n  - **addition**: A + B = (0, 0, 2, 0, 2,-2, 0,  ....)\\n  - **multiplication**: A * B =  (-1, -1, -1, 1, 1, -1, 1, ...) - this is **XOR**\\n    - want this to be invertible, dsitribute over addition, preserve distance, and be dissimilar to the vectors being multiplied\\n    - number of ones after multiplication is the distance between the two original vectors\\n    - can represent a dissimilar set vector by using multiplication\\n  - permutation: shuffles values\\n    - ex. rotate (bit shift with wrapping around)\\n    - multiply by rotation matrix (where each row and col contain exactly one 1)\\n    - can think of permutation as a list of numbers 1, 2, ..., n in permuted order\\n    - many properties similar to multiplication\\n    - random permutation randomizes\\n- basic operations\\n  - weighting by a scalar\\n  - similarity = dot product (sometimes normalized)\\n    - A $\\\\cdot$ A = 10k\\n    - A $\\\\cdot$ A = 0 (orthogonal)\\n    - in high-dim spaces, almost all pairs of vectors are dissimilar A $\\\\cdot$ B = 0\\n    - goal: similar meanings should have large similarity\\n  - normalization\\n    - for binary vectors, just take the sign\\n    - for non-binary vectors, scalar weight\\n- data structures\\n- these operations allow for encoding all normal data structures: sets, sequences, lists, databases\\n  - set - can represent with a sum (since the sum is similar to all the vectors)\\n    - can find a stored set using any element\\n    - if we don\\'t store the sum, can probe with the sum and keep subtracting the vectors we find\\n  - multiset = bag (stores set with frequency counts) - can store things with order by adding them multiple times, but hard to actually retrieve frequencies\\n  - sequence - could have each element be an address pointing to the next element\\n    - problem - hard to represent sequences that share a subsequence (could have pointers which skip over the subsquence)\\n    - soln: index elements based on permuted sums\\n      - can look up an element based on previous element or previous string of elements\\n    - could do some kind of weighting also\\n  - pairs - could just multiply (XOR), but then get some weird things, e.g. A * A = **0**\\n    - instead, permute then multiply\\n    - can use these to index (address, value) pairs and make more complex data structures\\n  - named tuples - have smth like (name: x, date: m, age: y)  and store as holistic vector $H = N*X + D *  M + A * Y$\\n    - individual attribute value can be retrieved using vector for individual key\\n  - representation substituting is a little trickier....\\n    - we blur what is a value and whit is a variable\\n    - can do this for a pair or for a named tuple with new values\\n      - this doesn\\'t always work\\n- examples\\n  - context vectors\\n    - standard practice (e.g. LSA): make matrix of word counts, where each row is a word, and each column is a document\\n    - HD computing alternative: each row is a word, but each document is assigned a few ~10 columns at random\\n      - thus, the number of columns doesn\\'t scale with the number of documents\\n      - **can also do this randomness for the rows (so the number of rows < the number of words)**\\n      - can still get semantic vector for a row/column by adding together the rows/columns which are activated by that row/column\\n      - this examples still only uses bag-of-words (but can be extended to more)\\n  - learning rules by example\\n    - particular instance of a rule is a rule (e.g mother-son-baby $\\\\to$ grandmother)\\n      - as we get more examples and average them, the rule gets better\\n      - doesn\\'t always work (especially when things collapse to identity rule)\\n  - analogies from pairs\\n    - ex. what is the dollar of mexico?\\n\\n### ex. identify the language\\n\\n- paper: [LANGUAGE RECOGNITION USING RANDOM INDEXING](https://arxiv.org/pdf/1412.7026.pdf) (joshi et al. 2015)\\n- benefits - very simple and scalable - only go through data once\\n  - equally easy to use 4-grams vs. 5-grams\\n- data\\n  - train: given million bytes of text per language (in the same alphabet)\\n  - test: new sentences for each language\\n- training: compute a 10k profile vector for each language and for each test sentence\\n  - could encode each letter wih a seed vector which is 10k\\n  - instead encode trigrams with **rotate and multiply**\\n    - 1st letter vec rotated by 2 * 2nd letter vec rotated by 1 * 3rd letter vec\\n    - ex. THE = r(r(T)) * r(H) * r(E)\\n    - approximately orthogonal to all the letter vectors and all the other possible trigram vectors...\\n  - profile = sum of all trigram vectors (taken sliding)\\n    - ex. banana = ban + ana + nan + ana\\n    - profile is like a histogram of trigrams\\n- testing\\n  - compare each test sentence to profiles via dot product\\n  - clusters similar languages - cool!\\n  - gets 97% test acc\\n  - can query the letter most likely to follor \"TH\"\\n    - form query vector $Q = r(r(T)) * r(H)$\\n    - query by using multiply X + Q * english-profile-vec\\n    - find closest letter vecs to X - yields \"e\"\\n\\n### details\\n\\n- mathematical background\\n  - randomly chosen vecs are dissimilar\\n  - sum vector is similar to its argument vectors\\n  - product vector and permuted vector are dissimilar to their argument vectors\\n  - multiplication distibutes over addition\\n  - permutation distributes over both additions and multiplication\\n  - multiplication and permutations are invertible\\n  - addition is approximately invertible\\n- comparison to DNNs\\n  - both do statistical learning from data\\n  - data can be noisy\\n  - both use high-dim vecs although DNNs get bad with him dims (e.g. 100k)\\n  - HD is founded on rich mathematical theory\\n  - new codewords are made from existing ones\\n  - HD memory is a separate func\\n  - HD algos are transparent, incremental (on-line), scalable\\n  - somewhat closer to the brain...cerebellum anatomy seems to be match HD\\n  - HD: holistic (distributed repr.) is robust\\n- different names\\n  - Tony plate: holographic reduced representation\\n  - ross gayler: multiply-add-permute arch\\n  - gayler & levi: vector-symbolic arch\\n  - gallant & okaywe: matrix binding with additive termps\\n  - fourier holographic reduced reprsentations (FHRR; Plate)\\n  - ...many more names\\n- theory of sequence indexing and working memory in RNNs\\n  - trying to make key-value pairs\\n  - VSA as a structured approach for understanding neural networks\\n  - reservoir computing = state-dependent network = echos-state network = liquid state machine - try to represen sequential temporal data - builds representations on the fly\\n\\n\\n\\n### papers\\n\\n- [text classification](https://iis-people.ee.ethz.ch/~arahimi/papers/DATE16_HD.pdf) (najafabadi et al. 2016)\\n- [Classification and Recall With Binary Hyperdimensional Computing: Tradeoffs in Choice of Density and Mapping Characteristics](https://ieeexplore.ieee.org/abstract/document/8331890?casa_token=FbderL4T3RgAAAAA:LfP2kRSJwhY5z4OHMqvNDrxmSpyIMLzGs80vGj_IdBXVhVVDwZg1tfIeD2nj0S5N7T2YsRrOcg)\\n  - note: for sparse vectors, might need some threshold before computing mean (otherwise will have too many zeros)\\n\\n\\n\\n## dnns with memory\\n\\n- Neural Statistician (Edwards & Storkey, 2016) summarises a dataset by averaging over their embeddings\\n- [kanerva machine](https://arxiv.org/pdf/1804.01756.pdf)\\n  - like a VAE where the prior is derived from an adaptive memory store\\n\\n\\n\\n## visual sampling\\n\\n- [Emergence of foveal image sampling from learning to attend in visual scenes](https://arxiv.org/abs/1611.09430) (cheung, weiss, & olshausen, 2017) - using neural attention model, learn a retinal sampling lattice\\n  - can figure out what parts of the input the model focuses on\\n\\n\\n\\n## dynamic routing between capsules\\n\\n- hinton 1981 - reference frames requires structured representations\\n  - mapping units vote for different orientations, sizes, positions based on basic units\\n  - mapping units **gate the activity** from other types of units - weight is dependent on if mapping is activated\\n  - top-down activations give info back to mapping units\\n  - this is a hopfield net with three-way connections (between input units, output units, mapping units)\\n  - reference frame is a key part of how we see - need to vote for transformations\\n- olshausen, anderson, & van essen 1993 - dynamic routing circuits\\n  - ran simulations of such things (hinton said it was hard to get simulations to work)\\n  - we learn things in object-based reference frames\\n  - inputs -> outputs has weight matrix gated by control\\n- zeiler & fergus 2013 - visualizing things at intermediate layers - deconv (by dynamic routing)\\n  - save indexes of max pooling (these would be the control neurons)\\n  - when you do deconv, assign max value to these indexes\\n- arathom 02 - map-seeking circuits\\n- tenenbaum & freeman 2000 - bilinear models\\n  - trying to separate content + style\\n- hinton et al 2011 - transforming autoencoders - trained neural net to learn to shift imge\\n- sabour et al 2017 - dynamic routing between capsules\\n  - units output a vector (represents info about reference frame)\\n  - matrix transforms reference frames between units\\n  - recurrent control units settle on some transformation to identify reference frame\\n- notes from this [blog post](https://towardsdatascience.com/capsule-neural-networks-part-2-what-is-a-capsule-846d5418929f)\\n  - problems with cnns\\n    - pooling loses info\\n    - don\\'t account for spatial relations between image parts\\n    - can\\'t transfer info to new viewpoints\\n  - **capsule** - vector specifying the features of an object (e.g. position, size, orientation, hue texture) and its likelihood\\n    - ex. an \"eye\" capsule could specify the probability it exists, its position, and its size\\n    - magnitude (i.e. length) of vector represents probability it exists (e.g. there is an eye)\\n    - direction of vector represents the instatntiation parameters (e.g. position, size)\\n  - hierarchy\\n    - capsules in later layers are functions of the capsules in lower layers, and since capsule has extra properties can ask questions like \"are both eyes similarly sized?\"\\n      - equivariance = we can ensure our net is invariant to viewpoints by checking for all similar rotations/transformations in the same amount/direction\\n    - active capsules at one level make predictions for the instantiation parameters of higher-level capsules\\n      - when multiple predictions agree, a higher-level capsule is activated\\n  - steps in a capsule (e.g. one that recognizes faces)\\n    - receives an input vector (e.g. representing eye)\\n    - apply affine transformation - encodes spatial relationships (e.g. between eye and where the face should be)\\n    - applying weighted sum by the C weights, learned by the routing algorithm\\n      - these weights are learned to group similar outputs to make higher-level capsules\\n    - vectors are squashed so their magnitudes are between 0 and 1\\n    - outputs a vector\\n\\n## hierarchical temporal memory (htm)\\n\\n- binary synapses and learns by modeling the growth of new synapses and the decay of unused synapses\\n- separate aspects of brains and neurons that are essential for intelligence from those that depend on brain implementation\\n\\n### necortical structure\\n\\n- evolution leads to physical/logical hierarchy of brain regions\\n- neocortex is like a flat sheet\\n- neocortex regions are similar and do similar computation\\n  - Mountcastle 1978: vision regions are vision becase they receive visual input\\n  - number of regions / connectivity seems to be genetic\\n- before necortex, brain regions were homogenous: spinal cord, brain stem, basal ganglia, ...\\n- ![cortical_columns](../assets/cortical_columns.png)\\n\\n### principles\\n\\n- common algorithims accross neocortex\\n- hierarchy\\n- **sparse distributed representations (SDR)** - vectors with thousands of bits, mostly 0s\\n  - bits of representation encode semantic properties\\n- inputs\\n  - data from the sense\\n  - copy of the motor commands\\n    - \"sensory-motor\" integration - perception is stable while the eyes move\\n- patterns are constantly changing\\n- necortex tries to control old brain regions which control muscles\\n- **learning**: region accepts stream of sensory data + motor commands\\n  - learns of changes in inputs\\n  - ouputs motor commands\\n  - only knows how its output changes its input\\n  - must learn how to control behavior via *associative linking*\\n- sensory encoders - takes input and turnes it into an SDR\\n  - engineered systems can use non-human senses\\n- behavior needs to be incorporated fully\\n- temporal memory - is a memory of sequences\\n  - everything the neocortex does is based on memory and recall of sequences of patterns\\n- on-line learning\\n  - prediction is compared to what actually happens and forms the basis of learning\\n  - minimize the error of predictions\\n\\n\\n### papers\\n\\n- \"A Theory of How Columns in the Neocortex Enable Learning the Structure of the World\"\\n  - network model that learns the structure of objects through movement\\n  - object recognition\\n    - over time individual columns integrate changing inputs to recognize complete objects\\n    - through existing lateral connections\\n  - within each column, neocortex is calculating a location representation\\n    - locations relative to each other = **allocentric**\\n  - much more motion involved\\n  - multiple columns - integrate spatial inputs - make things fast\\n  - single column - integrate touches over time - represent objects properly\\n- \"Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex\"\\n  - learning and recalling sequences of patterns\\n  - neuron with lots of synapses can learn transitions of patterns\\n  - network of these can form robust memory\\n\\n## forgetting\\n- [Continual Lifelong Learning with Neural Networks: A Review](https://arxiv.org/pdf/1802.07569.pdf)\\n  - main issues is *catastrophic forgetting* / *stability-plasticity dilemma*\\n  - ![Screen Shot 2020-01-01 at 11.49.32 AM](../assets/forgetting.png)\\n  - 2 types of plasticity\\n    - Hebbian plasticity (Hebb 1949) for positive feedback instability\\n    - compensatory homeostatic plasticity which stabilizes neural activity\\n  - approaches: regularization, dynamic architectures (e.g. add more nodes after each task), memory replay\\n\\n## deeptune-style\\n\\n- ponce_19_evolving_stimuli: [https://www.cell.com/action/showPdf?pii=S0092-8674%2819%2930391-5](https://www.cell.com/action/showPdf?pii=S0092-8674(19)30391-5)\\n- bashivan_18_ann_synthesis\\n- [adept paper](https://papers.nips.cc/paper/6738-adaptive-stimulus-selection-for-optimizing-neural-population-responses.pdf)\\n  - use kernel regression from CNN embedding to calculate distances between preset images\\n  - select preset images\\n  - verified with macaque v4 recording\\n  - currently only study that optimizes firing rates of multiple neurons\\n\\t- pick next stimulus in closed-loop (\"adaptive sampling\" = \"optimal experimental design\")\\n- J. Benda, T. Gollisch, C. K. Machens, and A. V. Herz, “From response to stimulus: adaptive sampling in sensory physiology”\\n  - find the smallest number of stimuli needed to fit parameters of a model that predicts the recorded neuron’s activity from the\\n    stimulus\\n\\n  - maximizing firing rates via genetic algorithms\\n\\n  - maximizing firing rate via gradient ascent\\n- C. DiMattina and K. Zhang,“Adaptive stimulus optimization for sensory systems neuroscience\"](https://www.frontiersin.org/articles/10.3389/fncir.2013.00101/full)\\n\\n  - 2 general approaches: gradient-based approaches + genetic algorithms\\n  - can put constraints on stimulus space\\n  - stimulus adaptation\\n  - might want iso-response surfaces\\n  - maximally informative stimulus ensembles (Machens, 2002)\\n  - model-fitting: pick to maximize info-gain w/ model params\\n  - using fixed stimulus sets like white noise may be deeply problematic for efforts to identify non-linear hierarchical network models due to continuous parameter confounding (DiMattina and Zhang, 2010) \\n  - use for model selection\\n\\n\\n## population coding\\n\\n- saxena_19_pop_cunningham: \"Towards the neural population doctrine\"\\n  - correlated trial-to-trial variability\\n    - Ni et al. showed that the correlated variability in V4 neurons during attention and learning — processes that have inherently different timescales — robustly decreases\\n    - ‘choice’ decoder built on neural activity in the first PC performs as well as one built on the full dataset, suggesting that the relationship of neural variability to behavior lies in a relatively small subspace of the state space.\\n  - decoding\\n    - more neurons only helps if neuron doesn\\'t lie in span of previous neurons\\n  - encoding\\n    - can train dnn goal-driven or train dnn on the neural responses directly\\n  - testing\\n    - important to be able to test population structure directly\\n- *population vector coding* - ex. neurons coded for direction sum to get final direction\\n- reduces uncertainty\\n- *correlation coding* - correlations betweeen spikes carries extra info\\n- *independent-spike coding* - each spike is independent of other spikes within the spike train\\n- *position coding* - want to represent a position\\n  - for grid cells, very efficient\\n- *sparse coding*\\n- hard when noise between neurons is correlated\\n- measures of information\\n- eda\\n  - plot neuron responses\\n  - calc neuron covariances\\n\\n## interesting misc papers\\n\\n- berardino 17 eigendistortions\\n  - **Fisher info matrix** under certain assumptions = $Jacob^TJacob$ (pixels x pixels) where *Jacob* is the Jacobian matrix for the function f action on the pixels x\\n  - most and least noticeable distortion directions corresponding to the eigenvectors of the Fisher info matrix\\n- gao_19_v1_repr\\n  - don\\'t learn from images - v1 repr should come from motion like it does in the real world\\n  - repr\\n    - vector of local content\\n    - matrix of local displacement\\n  - why is this repr nice?\\n    - separate reps of static image content and change due to motion\\n    - disentangled rotations\\n  - learning\\n    - predict next image given current image + displacement field\\n    - predict next image vector given current frame vectors + displacement\\n- kietzmann_18_dnn_in_neuro_rvw\\n- friston_10_free_energy\\n  - ![friston_free_energy](../assets/friston_free_energy.png)',\n",
       " '---\\nlayout: notes\\ntitle: transfer learning\\ncategory: research\\n---\\n\\n\\n\\n- [Domain-Adversarial Training of Neural Networks](https://dl.acm.org/ doi/abs/10.5555/2946645.2946704) (ganin et al. 16) - want repr. to be invariant to domain label\\n  - ![Screen Shot 2020-11-10 at 12.05.12 PM](../assets/domain_adv_training.png)\\n- domain adatation given multiple training groups\\n  - [group](http://papers.neurips.cc/paper/3019-mixture-regression-for-covariate-shift.pdf) [distributionally robust](https://arxiv.org/abs/1611.02041) [optimization](https://arxiv.org/abs/1911.08731)\\n  - [domain](https://papers.nips.cc/paper/4312-generalizing-from-several-related-classification-tasks-to-a-new-unlabeled-sample) [generalization](https://arxiv.org/abs/2007.01434)\\n  - \\n- domain adaptation using source/target given all at once\\n  - [importance weighting](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.370.4921&rep=rep1&type=pdf)\\n  - learning [invariant represetntations](https://arxiv.org/abs/1702.05464)\\n- test-time adaptation\\n  - [batch normalization](https://arxiv.org/abs/1603.04779)\\n  - [label shift estimation](https://arxiv.org/abs/1802.03916)\\n  - [rotation prediction](https://arxiv.org/abs/1909.13231)\\n  - [entropy minimization](https://arxiv.org/abs/2006.10726)\\n- [adaptive risk minimization](https://arxiv.org/abs/2007.02931) - combines groups at training time + batches at test-time\\n  - *meta-train* the model using simulated distribution shifts, which is enabled by the training groups, such that it exhibits strong *post-adaptation* performance on each shift',\n",
       " '---\\nlayout: notes\\ntitle: disentanglement\\ncategory: blog\\ntypora-copy-images-to: ../assets\\n---\\n\\n<div class=\"iframe-box\" style=\"margin-top: 0px\">\\n<iframe class=\"iframe\" src=\"https://csinva.github.io/notes/cheat_sheets/disentanglement_cheat_sheet#/\"\\n        frameborder=\"0\" width=\"100%\" height=\"auto\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>\\n</div>\\n\\n#  disentanglement\\n\\n## VAEs\\n\\n*Some good disentangled VAE implementations are [here](https://github.com/YannDubs/disentangling-vae) and more general VAE implementations are [here](https://github.com/AntixK/PyTorch-VAE)*. Tensorflow implementations available [here](https://github.com/google-research/disentanglement_lib)\\n\\nThe goal is to obtain a nice latent representation $\\\\mathbf z$ for our inputs $\\\\mathbf x$. To do this, we learn parameters $\\\\phi$ for the encoder $p_\\\\phi( \\\\mathbf z\\\\vert \\\\mathbf x)$ and $\\\\theta$ for the decoder $q_{\\\\mathbf \\\\theta} ( \\\\mathbf x\\\\vert \\\\mathbf z)$. We do this with the standard vae setup, whereby a code $z$ is sampled, using the output of the encoder (intro to VAEs [here](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)).\\n\\n### disentangled vae losses\\n\\n\\n| reconstruction loss                             | compactness prior loss                           |         total correlation loss             |\\n| ----------------------------------------------- | ------------------------------------------------- |:-------------------------------------------: |\\n| encourages accurate reconstruction of the input | encourages points to be compactly placed in space | encourages latent variables to be independent |\\n\\n\\n- summarizing the losses\\n  - **reconstruction loss** - measures the quality of the reconstruction, the form of the loss changes based on the assumed distribution of the likelihood of each pixel\\n    - *binary cross entropy loss* - corresopnds to bernoulli distr., most common - doesn\\'t penalize (0.1, 0.2) and (0.4, 0.5) the same way, which might be problematic\\n    - *mse loss* - gaussian distr. - tends to focus on a fex pixels that are very wrong\\n    - *l1 loss* - laplace distr.\\n  - **compactness prior loss**\\n    - doesn\\'t use the extra injected latent noise\\n    - tries to push all the points to the same place\\n    - emphasises smoothness of z, using as few dimensions of z as possible, and the main axes of z to capture most of the data variability\\n    - usually assume prior is standard normal, resulting in pushing the code means to 0 and code variance to 1\\n    - we can again split this term $\\\\sum_i \\\\underbrace{\\\\text{KL} \\\\left(p_\\\\phi( \\\\mathbf z_i\\\\vert \\\\mathbf x)\\\\:\\\\vert\\\\vert\\\\:prior(\\\\mathbf z_i) \\\\right)}_{\\\\text{compactness prior loss}} = \\\\underbrace{\\\\sum_i I(x; z)}_{\\\\text{mutual info}} + \\\\underbrace{\\\\text{KL} \\\\left(p_\\\\phi( \\\\mathbf z_i)\\\\:\\\\vert\\\\vert\\\\:prior(\\\\mathbf z_i) \\\\right)}_{\\\\text{factorial prior loss}}$\\n      - (see [factor-vae paper for proof](https://arxiv.org/pdf/1802.05983.pdf))\\n  - **total correlation loss** - encourages factors to be independent\\n    - measures dependence between marginals of the latent vars\\n    - intractable (requires pass through the whole dset)\\n    - instead sample $dec_\\\\phi(\\\\mathbf z\\\\vert \\\\mathbf x)$ and create $\\\\prod_j dec_\\\\phi( \\\\mathbf z_i\\\\vert \\\\mathbf x) $ by permuting across the batch dimension\\n      - now, calculate the kl with the *density-ratio trick* - train a classifier to approximate the ratio from these terms\\n\\n\\n\\n### disentangled vae in code\\n\\n```python\\n## Reconstruction + KL divergence losses summed over all elements and batch\\ndef loss_function(x_reconstructed, x, mu, logvar, beta=1):\\n  \\'\\'\\'\\n  Params\\n  ------\\n  x_reconstructed: torch.Tensor\\n\\t\\tReconstructed input, with values between 0-1\\n\\tx: torch.Tensor\\n\\t\\tinput, values unrestricted\\n  \\'\\'\\'\\n  \\n  ## reconstruction loss (assuming bernoulli distr.)\\n  ## BCE = sum_i [x_rec_i * log(x_i) + (1 - x_rec_i) * log(1-x_i)]\\n\\trec_loss = F.binary_cross_entropy(x_reconstructed, x, reduction=\\'sum\\')\\n\\n  ## compactness prior loss\\n\\t## 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\\n\\tKLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\\n  \\n  ## total correlation loss (calculate tc-vae way)\\n\\tz_sample = mu + torch.randn_like(exp(0.5 * logvar))\\n\\tlog_pz, log_qz, log_prod_qzi, log_q_zCx = func(z_sample, mu, logvar)\\n  ## I[z;x] = KL[q(z,x)\\\\vert\\\\vertq(x)q(z)] = E_x[KL[q(z\\\\vertx)\\\\vert\\\\vertq(z)]]\\n  mi_loss = (log_q_zCx - log_qz).mean()\\n  ## TC[z] = KL[q(z)\\\\vert\\\\vert\\\\prod_i z_i]\\n  tc_loss = (log_qz - log_prod_qzi).mean()\\n  ## dw_kl_loss is KL[q(z)\\\\vert\\\\vertp(z)] instead of usual KL[q(z\\\\vertx)\\\\vert\\\\vertp(z))]\\n  dw_kl_loss = (log_prod_qzi - log_pz).mean()\\n  \\n\\treturn rec_loss + beta * KLD\\n```\\n\\n\\n\\n### various vaes\\n\\n- [vae](https://arxiv.org/abs/1312.6114) (kingma & welling, 2013)\\n- [beta-vae](https://openreview.net/references/pdf?id=Sy2fzU9gl) (higgins et al. 2017) - add hyperparameter $\\\\beta$ to weight the compactness prior term\\n- [beta-vae H](https://arxiv.org/pdf/1804.03599.pdf) (burgess et al. 2018) - add parameter $C$ to control the contribution of the compactness prior term\\n  - $\\\\overbrace{\\\\mathbb  E_{p_\\\\phi(\\\\mathbf z\\\\vert \\\\mathbf x)}}^{\\\\text{samples}} [ \\\\underbrace{-\\\\log q_{\\\\mathbf \\\\theta} ( \\\\mathbf x\\\\vert \\\\mathbf z)}_{\\\\text{reconstruction loss}} ]      \\t\\t+ \\\\textcolor{teal}{\\\\beta}\\\\; \\\\vert\\\\sum_i \\\\underbrace{\\\\text{KL} \\\\left(p_\\\\phi( \\\\mathbf z_i\\\\vert \\\\mathbf x)\\\\:\\\\vert\\\\vert\\\\:prior(\\\\mathbf z_i) \\\\right)}_{\\\\text{compactness prior loss}} -C\\\\vert$\\n  -  C is gradually increased from zero (allowing for a larger compactness prior loss) until good quality reconstruction is achieved\\n- [factor-vae](https://arxiv.org/abs/1802.05983) (kim & minh 2018) - adds total correlation loss term\\n  - computes *total correlation loss term* using discriminator (can we discriminate between the samples when we shuffle over the batch dimension or not?)\\n  - [beta-TC-VAE = beta-total-correlation VAE](https://arxiv.org/abs/1802.04942) (chen et al. 2018) - same objective but computed without need for discriminator\\n    - use minibatch-weighted sampling to compute each of the 3 terms that make up the original VAE *compactness prior loss*\\n    - main idea is to better approximate $q(z)$ by weighting samples appropriately - biased, but easier to compute\\n  - [Interpretable VAEs for nonlinear group factor analysis](https://arxiv.org/abs/1802.06765)\\n- [Wasserstein Auto-Encoders](https://arxiv.org/pdf/1711.01558.pdf) (tolstikhin et al.) - removes the mutual info part of the loss\\n  - wasserstein distance = earth-movers distance, how far apart are 2 distrs\\n  - minimizes wasserstein distance + penalty which is similar to auto-encoding penalty,  without the mutual info term\\n  - another intuition: rather than map each point to a ball (since VAE adds noise to each latent repr), we only constraint the overall distr of Z, potentially making reconstructions less blurry (but potentially making latent space less smooth)\\n  - ![wae](../assets/wae.png)\\n- [Adversarial Latent Autoencoder](https://arxiv.org/pdf/2004.04467.pdf) (pidhorskyi et al. 2020)\\n  - improve quality of generated VAE reconstructions by using a different setup which allows for using a GAN loss\\n  - ![alae](../assets/alae.png)\\n- [Variational Autoencoders Pursue PCA Directions (by Accident)](https://arxiv.org/pdf/1812.06775.pdf)\\n  - local orthogonality of the embedding transformation\\n  - prior $p(z)$ is standard normal, so encoder is assumed to be Gaussian with a certain mean, and **diagonal covariance**\\n  - disentanglement is sensitive to rotations of the latent embeddings but reconstruction err doesn\\'t care\\n  - for linear autoencoder w/ square-error as reconstruction loss, we recover PCA decomp\\n- [Disentangling Disentanglement in Variational Autoencoders](https://arxiv.org/pdf/1812.02833.pdf) (2019)\\n  - independence can be too simplistic, instead 2 things:\\n    - the latent encodings of data having an appropriate level of overlap\\n      - keeps encodings from just being a lookup table\\n      - when encoder is unimodal, $I(x; z)$ gives us a good handle on this\\n    - prior structure on the latents (e.g. independence, sparsity)\\n  - to trade these off, can penalize divergence between $q_\\\\phi(z)$ and $p(z)$\\n  - nonisotropic priors -  isotropic priors are only good up to rotation in the latent space\\n    - by chossing a nonisotropic prior (e.g. nonisotropic gaussian), can learn certain directions more easily\\n  - sparse prior - can help do clustering\\n- [VAE-SNE: a deep generative model for simultaneous dimensionality reduction and clustering](https://www.biorxiv.org/content/10.1101/2020.07.17.207993v1.full.pdf) (graving & couzin 2020) - reduce dims + cluster without specifying number of clusters\\n  - ![Screen Shot 2020-09-10 at 11.40.10 PM](../assets/sne_eq.png)\\n  - **stochastic neighbor regularizer** that optimizes pairwise similarity kernels between original and latent distrs. to strengthen local neighborhood preservation\\n    - can use different neighbor kernels, e.g. t-SNE similarity (van der Maaten & Hinton, 2008) or Gaussian SNE kernel (Hinton & Roweis, 2003)\\n    - perplexity annealing technique (Kobak and Berens, 2019) - decay the size of local neighborhoods during training (helps to preserve structure at multiple scales)\\n  - **Gaussian mixture prior** for learning latent distr. (with very large number of clusters)\\n    - at the end, merge clusters using a sparse watershed (see [todd et al. 2017](https://iopscience.iop.org/article/10.1088/1478-3975/14/1/015002/meta))\\n  - extensive evaluation - test several datasets / methods and evaluate how well the first 2 dimensions preserve the following:\\n    - global - correlation between pairwise distances in orig/latent spaces\\n    - local - both metric (distance- or radius-based) and topological (neighbor-based) neighborhoods which are 1% of total embedding size\\n    - fine-scale - neighborhoods which are <1% of total embedding size\\n    - temporal info (for time-series data only) - correlation between latent and original temporal derivatives\\n    - **likelihood** on out-of-sample data\\n  - further advancements\\n    - embed into polar coordinates (rather than Euclidean) helps a lot\\n    - convolutional VAE-SNE - extract features from images using some pre-trained net and then run VAE-SNE on these features\\n  - background: earlier works also used SNE objective for regularization - starts with van der Maaten 2009 (parametric t-SNE)\\n  - future work: density-preserving versions of t-SNE, modeling hierarchical structure in vae, conditional t-SNE kernel\\n- [A Survey of Inductive Biases for Factorial Representation-Learning](https://arxiv.org/abs/1612.05299) (ridgeway 2016)\\n  - desiderata\\n    - **compact**\\n    - **faithful** - preserve info required for task\\n    - **explicitly** represent the attributes required for the task at hand\\n    - **interpretable** by humans\\n  - factorial representation - attributes are statistically independent and can provide a userful bias for learning\\n    - \"compete\" - factors are more orthogonal\\n    - \"cooperate\" - factors are more similar\\n    - bias on distribution of factors\\n      - PCA - minimize reconstruction err. subject to orthogonal weights\\n      - ICA - maximize non-Gaussianity (can also have sparse ICA)\\n    - bias on factors being invariant to certain types of changes\\n      - ISA (independent subspace analysis) - 2 layer model where first layer is linear, 2nd layer pools first layer (not maxpool, more like avgpool), sparsity at second layer\\n        - i.e. 1st layer cooperates, 2nd layer competes\\n      - VQ - vector quantizer - like ISA but first layer filters now compete and 2nd layer cooperates\\n      - SOM - encourages topographic map by enforcing nearby filters to be similar\\n    - bias in how factors are combined\\n      - linear combination - PCA/ICA\\n      - multilinear models - multiplicative interactions between factors (e.g on top of ISA)\\n      - functional parts - factor components are combined to construct the output\\n        - ex. NMF - parts can only add, not substract to total output\\n        - ex. have each pixel in the output be represented by only one factor in a VQ\\n      - hierarchical layers\\n        - ex. R-ICA - recursive ICA - run ICA on coefficients from previous layer (after some transformation)\\n  - supervision bias\\n    - constraints on some examples\\n      - e.g. some groups have same value for a factor\\n      - e.g. some examples have similar distances (basis for MDS = multidimensional scaling)\\n      - e.g. analogies between examples\\n      - can do all of these things with auto-encoders\\n- more papers\\n  - [infoVAE](https://arxiv.org/abs/1706.02262)\\n  - [dipVAE](https://arxiv.org/abs/1711.00848)\\n  - [vq-vae](https://arxiv.org/abs/1711.00937) - latent var is discrete, prior is learned\\n  - [Learning Disentangled Representations with Semi-Supervised Deep Generative Models](http://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models)\\n    - specify graph structure for some of the vars and learn the rest\\n\\n## GANs\\n\\n### model-based (disentangle during training)\\n\\n- disentangling architectures\\n    - [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657) (chen et al. 2016)\\n      - encourages $I(x; c)$ to be high for a subset of the latent variables $z$\\n        - slightly different than vae - defined under the distribution $p(c) p(x\\\\vert c)$ whereas vae uses $p_{data}(x)enc(z\\\\vert x)$\\n      - mutual info is intractable so optimizes a lower bound\\n    - [Stylegan](https://arxiv.org/abs/1812.04948) (karras et al. 2018)\\n      - introduced perceptual path length and linear separability to measure the disentanglement property of latent space\\n      - ![stylegan](../assets/stylegan.png)\\n      - [Stylegan2](https://arxiv.org/abs/1912.04958) (karras et al. 2019): ![stylegan2](../assets/stylegan2.png)\\n        - $\\\\psi$ scales the deviation of *w* from the average - $\\\\psi=1$ is original, moving towards 0 improves quality but reduces variety\\n        - also has jacobian penalty on mapping from style space $w$ to output image $y$\\n- [DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images](https://arxiv.org/abs/1711.05415)\\n- [Clustering by Directly Disentangling Latent Space](https://arxiv.org/abs/1911.05210) - clustering in the latent space of a gan\\n- [Semi-Supervised StyleGAN for Disentanglement Learning](https://arxiv.org/abs/2003.03461) - further improvements on StyleGAN using labels in the training data\\n- [The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement](https://arxiv.org/abs/2008.10599) (peebles et al. 2020)\\n    - if we perturb a single component of a network’s input, then we would like the change in the output to be independent of the other input components\\n    - minimize off-diagonal entries of Hessian matrix (can be obtained with finite differences)\\n        - smoother + more disentangled + shrinkage in latent space\\n    - Hessian penalty is for a scalar - they define penalty as max penalty over Hessian over all pixels in the generator\\n    - unbiased stochastic estimator for the Hessian penalty (Hutchinson estimator)\\n    - they apply this penalty with $z$ as input, but different intermediate activations as output\\n\\n### post-hoc (disentangle after training)\\n\\n- mapping latent space\\n  - [InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs](https://arxiv.org/pdf/2005.09635.pdf) (shen et al. 2020)\\n  - [Interpreting the Latent Space of GANs for Semantic Face Editing](https://arxiv.org/abs/1907.10786) (shen et al. 2020)\\n    - find latent directions for each binary attribute, as directions which separate the classes using linear svm\\n      - validation accuracies in tab 1 are high...much higher for all data (because they have high confidence level on attribute scores maybe) - for PGGAN but not StyleGAN\\n    - intro of this paper gives good survey of how people have studied GAN latent space\\n    - few papers posthoc analyze learned latent repr.\\n  - [On the \"steerability\" of generative adversarial networks](https://arxiv.org/abs/1907.07171) (jahanian et al. 2020)\\n    - learn to approximate edits to images, such as zooms\\n      - linear walk is as effective as more complex non-linear walks for learning this\\n      - nonlinear setting - learn a neural network which applies a small perturbation in a specific direction (e.g. zooming)\\n        - to move further in this space, repeatedly apply the function\\n  - [A Disentangling Invertible Interpretation Network for Explaining Latent Representations](https://arxiv.org/abs/2004.13166) (esser et al. 2020) - map latent space to interpretable space, with invertible neural network\\n      - interpretable space factorizes, so is disentangled\\n      - individual concepts (e.g. color) can use multiple interpretable latent dims\\n      - instead of user-supplied interpretable concepts, user supplies two sketches which demonstrate a change in a concept - these sketches are used w/ style transfer to create data points which describe the concept\\n      - alternatively, with no user-supplied concepts, try to get independent components in unsupervised way\\n  - [Disentangling in Latent Space by Harnessing a Pretrained Generator](https://arxiv.org/abs/2005.07728) (nitzan et al. 2020)\\n      - learn to map attributes onto latent space of stylegan\\n      - works using two images at a time and 2 encoders\\n        - for each image, predict attributes + identity, then mix the attributes\\n      - results look realy good, but can\\'t vary one attribute at a time (have to transfer all attributes from the new image)\\n  - [ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes](http://openaccess.thecvf.com/content_ECCV_2018/papers/Taihong_Xiao_ELEGANT_Exchanging_Latent_ECCV_2018_paper.pdf) (xiao et al. 2018)\\n      - trains 2 images at a time - swap an attribute that differs between the images and reconstruct images that have the transferred attribute\\n- bias\\n  - [Towards causal benchmarking of bias in computer vision algorithms](https://www.overleaf.com/project/5e6aa80234ed3e0001b9ac23) (balakrishnan et al. 2020) - use human annotations to disentangle latent space\\n    - synthesis approach can alter multiple attributes at a time to produce grid-like matched samples of images we call *transects*\\n    - find directions + orthogonalize same as shen et al. 2020\\n  - [Detecting Bias with Generative Counterfactual Face Attribute Augmentation](https://arxiv.org/abs/1906.06439) (denton et al. 2019) - identify latent dims by training a classifier in the latent space on groundtruth attributes of the training images\\n  - [Explaining Classifiers with Causal Concept Effect (CaCE)](https://arxiv.org/abs/1907.07165) (goyal et al. 2020) - use vae to disentangle / alter concepts to probe classifier\\n- post-hoc\\n    - [Explanation by Progressive Exaggeration](https://arxiv.org/abs/1911.00483) (singla et al. 2019)\\n      - progressively change image to negate the prediction, keeping most features fixed\\n        - want to learn mapping $I(x, \\\\delta)$ which produces realistic image that changes features by $\\\\delta$\\n      - 3 losses: **data consistency** (perturbed samples should look real), **prediction changes** (perturbed samples should appropriately alter prediction), **self-consistency** (applying reverse perturbatino should bring x back to original, and $\\\\delta=0$ should return identity)\\n      - moving finds ways to generate images that do change the wanted attribute (and don\\'t change the others too much)\\n      - minor\\n        - used human experiments\\n        - limited to binary classification\\n    - [Interpreting Deep Visual Representations via Network Dissection](https://arxiv.org/abs/1711.05611) (zhou et al. 2017)\\n    \\n      - obtain image attributes for each z (using classifier, not human labels)\\n        - this classifier may put bias back in\\n    \\n      - to find directions representing an attribute, train a linear model to predict it from z\\n    - [GAN Dissection: Visualizing and Understanding Generative Adversarial Networks](https://arxiv.org/abs/1811.10597) (bau et al. 2018) - identify group of interpretable units based on segmentation of training images\\n      - find directions which allow for altering the attributes\\n    - [GANSpace: Discovering Interpretable GAN Controls](https://arxiv.org/abs/2004.02546) - use PCA in the latent space (w for styleGAN, activation-space at a specific layer for BigGAN) to select directions\\n    - [Editing in Style: Uncovering the Local Semantics of GANs](https://arxiv.org/abs/2004.14367) (collins et al. 2020) - use k-means on gan activations to find meaningful clusters (with quick human annotation)\\n      - add style transfer using target/source image\\n    - [Unsupervised Discovery of Interpretable Directions in the GAN Latent Space](https://arxiv.org/abs/2002.03754) - loss function which tries to recover random shifts made to the latent space\\n\\n## misc\\n- [Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction](https://arxiv.org/abs/2006.08558) (yu, ..., & ma, 2020)\\n  - goal: learn low-dimensional structure from high-dim (labeled or unlabeled) data\\n  - approach: instead of cross-entropy loss, use **maximal coding rate reduction** = MCR loss function to learn linear feature space where:\\n    - *inter-class discriminative* - features of samples from different classes/clusters are uncorrelated + different low-dim linear subspaces\\n    - *intra-class compressible* - features of samples from same class/cluster are correlated (i.e. belong to low-dim linear subspace)\\n    - *maximally diverse* - dimension (or variance) of features for each class/cluster should be as large as possible as long as uncorrelated from other classes/clusters\\n  - related to nonlinear generalized PCA\\n  - given random variable $z$ and precision $\\\\epsilon$, rate distortion $R(z, \\\\epsilon)$ is minimal number of bits to encode $z$ such that expected decoding err is less than $\\\\epsilon$\\n    - can compute from finite samples\\n    - can compute for each class (diagonal matrices represent class/cluster membership in loss function)\\n    - MCR maximizes (rate distortion for all features) - (rate distortion for all data separated into classes)\\n      - like a generalization of information gain\\n  - evaluation\\n    - with label corruption performs better\\n\\n## (semi)-supervised disentanglement\\n\\n**these papers use some form of supervision for the latent space when disentangling**\\n\\n- [Semi-supervised Disentanglement with Independent Vector Variational Autoencoders](https://arxiv.org/pdf/2003.06581.pdf)\\n  - ![Screen Shot 2020-05-21 at 12.47.22 PM](../assets/semi_supervised_vae.png)\\n- [Learning Disentangled Representations with Semi-Supervised Deep Generative Models](https://arxiv.org/abs/1706.00400) - put priors on interpretable variables during training and learn the rest\\n- [Weakly Supervised Disentanglement with Guarantees](https://arxiv.org/abs/1910.09772)\\n  - prove results on disentanglement for rank pairing\\n  - different types of available supervision![Screen Shot 2020-05-21 at 1.05.19 PM](../assets/unsupervised_settings.png)\\n    - **restricted labeling** - given labels for some groundtruth factors (e.g. label \"glasses\", \"gender\" for all images)\\n    - match pairing - given pairs or groups (e.g. these images all have glasses)\\n    - rank pairing - label whether a feature is greater than another (e.g. this image has darker skin tone than this one)\\n- [Weakly Supervised Disentanglement by Pairwise Similarities](https://arxiv.org/abs/1906.01044) - use pairwise supervision\\n\\n## evaluating disentanglement\\n\\n- [Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations](http://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf) (locatello et al. 2019)\\n  - state of disentanglement is very poor...depends a lot on architecture/hyperparameters\\n  - good way to evaluate: make explicit inductive biases, investigate benefits of this disentanglement\\n  - defining disentanglement - compact, interpretable, independent, helpful for downstream tasks, causal inference\\n    - a change in one *factor of variation* should lead to a change in a single factor in the learned repr.\\n  - unsupervised learning of disentangled reprs. is impossible without inductive biases\\n- *note* - vae\\'s come with reconstruction loss + compactness prior loss which can be looked at on their own\\n- data\\n  \\n  - [dsprites dataset](https://github.com/deepmind/dsprites-dataset/) has known latent factors we try to recover\\n- [beta-vae **disentanglement metric score** = higgins metric](https://medium.com/uci-nlp/summary-beta-vae-learning-basic-visual-concepts-with-a-constrained-variational-framework-91ad843b49e8) - see if we can capture known disentangled repr. using pairs of things where only one thing changes\\n  - start with a known generative model that has an observed set of independent and interpretable factors (e.g. scale, color, etc.) that can be used to simulate data.\\n  - create a dataset comprised of pairs of generated data for which a single factor is held constant (e.g. a pair of images which have objects with the same color).\\n  - use the inference network to map each pair of images to a pair of latent variables.\\n  - train a linear classifier to predict which interpretable factor was held constant based on the latent representations. The accuracy of this predictor is the disentanglement metric score.\\n- [Evaluating Disentangled Representations](https://arxiv.org/abs/1910.05587) (sepliarskaia et al. 2019)\\n  - defn 1  (Higgins et al., 2017; Kim and Mnih, 2018; Eastwood and Williams, 2018) = **factorVAE metric**: A disentangled representation is a representation where a change in one latent dimension corresponds to a change in one generative factor while being relatively invariant to changes in other generative factors.\\n  - defn 2 (Locatello et al., 2018; Kumar et al., 2017): A disentangled representation is a representation where a change in a single generative factor leads to a change in a single factor in the learned representation.\\n  - metrics\\n    - **DCI**: Eastwood and Williams (2018) - informativeness based on predicting gt factors using latent factors\\n    - **SAP**: Kumar et al. (2017) - how much does top latent factor match gt more than 2nd latent factor\\n    - **mutual info gap MIG**: Chen et al. 2018 - mutual info to compute the same thing\\n    - **modularity** (ridgeway & mozer, 2018) - if each dimension of r(x) depends on at most a factor of variation using their mutual info\\n\\n## non-deep methods\\n\\n- [unifying vae and nonlinear ica](https://arxiv.org/pdf/1907.04809.pdf) (khemakhem et al. 2020)\\n\\t- ICA\\n\\t  - maximize non-gaussianity of $z$ - use kurtosis, negentropy\\n\\t  - minimize mutual info between components of $z$ - use KL, max entropyd',\n",
       " '---\\nlayout: notes\\ntitle: complexity\\ncategory: research\\n---\\n\\n#  complexity\\n\\nComplexity can be a useful notion for many things in statistical models. It can help answer the following questions:\\n\\n- can I interpret this model?\\n- how many samples should I collect?\\n- is my model a good fit for this problem?\\n- model class selectiondirectionsmore stable (e.g. LOOCV, deletion, weights)interpolating estimator w/ lowest var?\\n- set an err threshold and then look at stability\\n\\n## philosophy\\n\\n- [What is complexity?](http://cogprints.org/357/4/evolcomp.pdf) (edmonds 95)\\n  - complexity is only really useful for comparisons\\n  - properties\\n    - size - makes things potentially complex\\n    - ignorance - complex things represent things we don\\'t understand (e.g. the brain)\\n    - minimum description size - more about **information** than complexity\\n      - potential problem - expressions are not much more complex than the original axioms in the system, even though they can get quite complex\\n      - potential problem - things with lots of useless info would seem more complex\\n    - some variety is necessary but not sufficient\\n    - order - we sometimes find order comes and goes (like double descent) - has a lot to do with language (audience)\\n  - defn: \"that property of a language expression which makes it difficult to formulate its overall behaviour even when given almost complete information about its atomic components and their inter-relations\"\\n    - language matters - what about the system are we describing?\\n    - goal matters - what outcome are interested in?\\n- [On Complexity and Emergence](https://arxiv.org/abs/nlin/0101006) (standish 01)\\n  - definition close to Kolmogorov / shannon entropy\\n  - adds context dependence\\n  - kolmogorov complexity = algorithmic information complexity\\n    - problem 1: must assume a particular Universal Turing machine (which might give differing results)\\n    - problem 2: random sequences have max complexity, even though they contain no information\\n  - soln\\n    - incorporate context - what descriptions are the same?\\n    - $C(x) = \\\\lim _{\\\\ell \\\\to \\\\infty} \\\\log_2 N - \\\\log_2 \\\\omega (\\\\ell, x)$\\n      - where C(x) is the complexity (measured in bits), $\\\\ell(x)$ the length of the description, N the size of the alphabet used to encode the description and ω(ℓ,x) the size of the class of all descriptions of length less than ℓ equivalent to x.\\n  - emergence - ex. game of life\\n- [What is complexity 2](https://link.springer.com/chapter/10.1007/978-3-642-50007-7_2) (Gell-Mann 02)\\n  - AIC - algorithmic information content - contains 2 terms\\n    - effective complexity (EC) = the length of a very concise description of an entity\\'s regularities\\n      - regularities are judged subjectively (e.g. birds would judge a bird song\\'s regularity)\\n    - 2nd term relating to random features\\n- [complexity](http://www.scholarpedia.org/article/Complexity) (Sporns 07)\\n  - complexity = degree to which **components** engage in organized structured **interactions**\\n  - High complexity -> mixture of order and disorder (randomness and regularity) + have a high capacity to generate **emergent** phenomena.\\n  - (simon 1981): complex systems are “made up of a large number of parts that have many interactions.”\\n  - 2 categories\\n    - algorithmic / mdl\\n    - natural complexity (e.g. physical complexity)\\n      - ![300px-Complexity_figure1](../assets/300px-Complexity_figure1.jpg)\\n- [quanta article](https://www.quantamagazine.org/computer-science-and-biology-explore-algorithmic-evolution-20181129/?fbclid=IwAR0rSImplo7lLM0kEYHrHttx8qUimB-482dI9IFxY6dvx0CFeEIqzGuir_w)\\n  - \"the probability of producing some types of outputs is far greater when randomness operates at the level of the program describing it rather than at the level of the output itself\"\\n  - \"they [recently reported in *Royal Society Open Science*](http://rsos.royalsocietypublishing.org/content/5/8/180399) that, compared to statistically random mutations, this mutational bias caused the networks to evolve toward solutions significantly faster.\"\\n\\n## computational complexity\\n\\n- amount of computational resource that it takes to solve a class of problem\\n- [Computational complexity](https://dl.acm.org/citation.cfm?id=1074233) (Papadimitriou)\\n  - like run times of algorithms etc. $O(n)$\\n- [Parameterized complexity](https://www.researchgate.net/profile/Michael_Fellows/publication/2376092_Parameterized_Complexity/links/5419e9240cf25ebee98883da/Parameterized-Complexity.pdf) (Downey and Fellows)\\n  - want to solve problems that are NP-hard or worse, so we isolate input into a parameter\\n\\n## bayesian model complexity\\n\\n- [Bayesian measures of model complexity and fit](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/1467-9868.00353) (spiegelhalter et al.)\\n- AIC\\n- BIC\\n- [TIC](https://arxiv.org/pdf/1803.04947.pdf)\\n\\n## statistical learning theory\\n\\n- VC-dimension - measure of capacity of a function class that can be learned\\n  - cardinality of largest number of points which can be shattered\\n\\n## misc\\n\\n- [rashomon curves](https://arxiv.org/abs/1908.01755) (semenova & rudin, 2019)\\n  - **rashomon effect** - many different explanations exist for same phenomenon\\n  - **rashomon set** - set of almost-equally accurate models for a given problem\\n  - **rashomon ratio** - ratio of volume of set of accurate models to the volume of the hypothesis space\\n  - **rashomon curve** - empirical risk vs rashomon ratio\\n    - **rashomon elbow** - maximize rashomon ratio while minimizing risk\\n      - good for model selection\\n- bennet\\'s logical depth (1988) - computational resources taken to calculate the results of a minimal length problem (combines computational complexity w/ kolmogorov complexity)\\n- Effective measure complexity (Grassberger, 1986) quantifies the complexity of a sequence by the amount of information contained in a given part of the sequence that is needed to predict the next symbol\\n- Thermodynamic depth (Lloyd and Pagels, 1988) relates the entropy of a system to the number of possible historical paths that led to its observed state\\n- lofgren\\'s interpretation and descriptive complexity\\n  - convert between system and description\\n- kaffman\\'s number of conflicting constraints\\n- Effective complexity (Gell-Mann, 1995) measures the minimal description length of a system’s regularities\\n- Physical complexity (Adami and Cerf, 2000) is related to effective complexity and is designed to estimate the complexity of any sequence of symbols that is about a physical world or environment\\n- Statistical complexity (Crutchfield and Young, 1989) is a component of a broader theoretic framework known as computational mechanics, and can be calculated directly from empirical data\\n- Neural complexity (Tononi et al., 1994) - multivariate extension of mutual information that estimates the total amount of statistical structure within an arbitrarily large system.= the difference between the sum of the component’s individual entropies and the joint entropy of the system as a whole\\n- complexity = variance of the model predictions (given that there is zero bias)\\n\\n\\n\\n## estimated\\n\\n- [optimal m estimation in high dimensions](https://www.pnas.org/content/110/36/14563) optimal loss function (optimize over different loss functions, but evaluate with L2)assumes unbiased (so variance is the mse)\\n\\n\\n\\n## entropy characterizations\\n\\n- try to characterize functions in the prediction space\\n- **metric entropy** - want functions to be close (within epsilon)\\n  - **bracket entropy** - function is both upper and lower bounded by bounding functions, which are within epsilon\\n- can do this on an entire function class (e.g. all neural networks) or on a restricted subset (e.g. path during training)\\n- [optimal learning via local entropies and sample compression](https://arxiv.org/pdf/1706.01124.pdf)\\n- [risk bounds for statistical learning](https://arxiv.org/pdf/math/0702683.pdf)\\n- [Chaining Mutual Information and Tightening Generalization Bounds](https://papers.nips.cc/paper/7954-chaining-mutual-information-and-tightening-generalization-bounds.pdf) (asadi et al. 2018)\\n- describing DNN paths\\n  - [Online Regularized Nonlinear Acceleration](https://arxiv.org/pdf/1805.09639.pdf) \\n\\n\\n\\n## deep learning complexity\\n\\n- [a hessian-based complexity measure for dnns](https://arxiv.org/abs/1905.11639)with generalization and computation to a different form of stability\\n  - thm 3 - want function to be smooth wrt to augmented loss\\n  - [complexity measure](http://proceedings.mlr.press/v89/liang19a/liang19a.pdf) (liang et al. 2019)\\n\\n## double descent\\n\\n- [Reconciling modern machine learning and the bias-variance trade-off](https://arxiv.org/abs/1812.11118) (belkin et al. 2018)\\n- [Surprises in High-Dimensional Ridgeless Least Squares Interpolation](https://arxiv.org/abs/1903.08560)\\n  - main result of limiting risk, where $\\\\gamma \\\\in (0, \\\\infty)$:\\n    - $R(\\\\gamma) = \\\\begin{cases} \\\\sigma^2 \\\\frac{\\\\gamma}{1-\\\\gamma} & \\\\gamma < 1\\\\\\\\||\\\\beta||_2^2(1 - \\\\frac 1 \\\\gamma) + \\\\sigma^2 \\\\frac{1} {\\\\gamma - 1} & \\\\gamma > 1\\\\end{cases}$\\n- [linear regression depends on data distr.](https://arxiv.org/abs/1802.05801)\\n- [two models of double descent for weak features](https://arxiv.org/abs/1903.07571)\\n- [double descent curve](https://openreview.net/forum?id=HkgmzhC5F7)\\n- [boosting w/ l2 loss](https://www.tandfonline.com/doi/pdf/10.1198/016214503000125?casa_token=5OE5LZe_mIcAAAAA:-4DdXLa4A6SeXnguyYv1S3bfIbRXrSb1qojj_UkGZpmbNHqjkWMojm0al5xx2yz-7ABcfDXmdvBeCw)\\n- [effective degrees of freedom](https://web.stanford.edu/~hastie/Papers/df_paper_LJrev6.pdf)\\n- [high-dimensional ridge](https://projecteuclid.org/euclid.aos/1519268430)\\n- [Harmless interpolation of noisy data in regression](https://arxiv.org/abs/1903.09139) - bound on how well interpolative solns can generalize to fresh data (goes to zero with extra features)\\n- [Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/abs/1912.02292) (nakkiran et al. 2019)\\n\\n\\n\\n### linear models\\n\\n- [Degrees of Freedom and Model Search](https://arxiv.org/abs/1402.1920) (tibshirani 2014)\\n  - degrees of freedom = quantitative description of the amount of fitting performed by a given procedure\\n- [linear smoothers and additive models](https://projecteuclid.org/download/pdf_1/euclid.aos/1176347115) (buja et al. 1989) see page 469 for degrees of freedom in ridge\\n\\n## minimum description length\\n\\n\\n- mdl in linear regression: want to send y over, X is known to both sides, theta is also sent (used to pick a decoder for y)\\n\\t\\n\\t- normalized maximum likelihood (nml): use theta to make codebook, then send code\\n- [The Minimum Description Length Principle in Coding and Modeling](https://pdfs.semanticscholar.org/65d3/4977d9055f42e51dc1e7d9b4ca2f36c17537.pdf) (barron, rissanen, & yu, 98)\\n  - mdl: represent an entire class of prob. distrs. as models by a single \"universal\" representative model such that it would be able to imitate the behavior of any model in the class. The best model class for a set of observed data, then, is the one whose representative premits the shortest coding of the data\\n  - tradeoff: \"good\" prob. models for the data permit shorter code lengths\\n    - generally agrees w/ low mse\\n  - ex. encode data w/ model defined by mle estimates, quantized optimally to finite precision, then encode estimate w/ prefix code\\n  - mdl\\n    - likelihood = summarize data in accodance / model (e.g. $P(y|x, \\\\theta)$)\\n    - parametric complexity = summarize model params\\n- [Model Selection and the Principle of Minimum Description Length](https://www.tandfonline.com/doi/abs/10.1198/016214501753168398) (hansen & yu 2001)\\n  - mdl: choose the model that gives the shortest description of data\\n    - description length = length of binary string used to code the data\\n    - using a prob distr. for coding/description purposes doesn\\'t require that it actually generate our data\\n  - basic coding\\n    - set A, code C (mapping from A to a set of codewords)\\n    - Q is a distr. on A\\n    - $-\\\\log_2Q$ is the code length for symbols in A\\n      - can construct such a code w/ Huffman coding\\n    - expected code length is minimized when Q = P, the true distr of our data\\n  - different forms\\n    - 2-stage\\n    - mixture\\n    - predictive\\n    - normalized maximum likelihood (NML)\\n- [mdl intro](http://www.scholarpedia.org/article/Minimum_description_length) (Rissanen, 2008) - scholarpedia\\n  - coding just the data would be like maximum likelihood\\n  - minimize $\\\\underset{\\\\text{log-likelihood}}{-\\\\log P(y^n|x^n;\\\\theta)} + \\\\underset{\\\\text{description length}}{L(\\\\theta)}$\\n    - ex. OLS\\n    - if we want to send all the coefficients, assume an order and $L(\\\\theta) = L(p) + L(\\\\theta_1, ... \\\\theta_p)$\\n      - $L(\\\\theta) \\\\approx \\\\frac p 2 \\\\log p$\\n        - quantization for each parameter (must quantize otherwise need to specify infinite bits of precision)\\n    - if we want only a subset of the coefficients, also need to send $L(i_1, ..., i_k)$ for the indexes of the non-zero coefficients\\n  - minimization becomes $\\\\underset p \\\\min \\\\quad [\\\\underset{\\\\text{noise}}{- \\\\log P(y^n|x^n; \\\\hat{\\\\theta}_{OLS})} + \\\\underset{\\\\text{learnable info}}{(p/2) \\\\log n}]$\\n    - *noise* - no more info can be extracted with this class of models\\n    - *learnable info* in the data = precisely the best model\\n    - **stochastic complexity** = *noise* + *learnable info*\\n    - in this case, is same as BIC but often different\\n  - modern mdl - don\\'t assume a model form, try to code the data as short as possible with a *universal* model class\\n    - often can actually construct these codes\\n- Kolmogorov complexity $K(x)$ = the shortest computer program (in binary) that generates x (a binary string) = the \"amount of info\" in x\\n  - complexity of a string x is at most its length\\n  - algorithmically random - any string whose length is close to $|x|$\\n    - more random = higher complexity\\n- Minimum description length original reference \\\\cite{rissanen1978modeling}. What is the minimum length description of the original?\\n  - MDL reviews \\\\cite{barron1998minimum, hansen2001model}.\\n  - Book on stochastic complexity \\\\cite{rissanen1989stochastic}\\n  - *Minimum Description Length*, *MDL*, principle for model selection, of which the original form states that the best model is the one which permits the shortest encoding of the data and the model itself\\n- *note: this type of complexity applies to the description, not the system*\\n- Look into the neurips [paper](https://papers.nips.cc/paper/7954-chaining-mutual-information-and-tightening-generalization-bounds.pdf) on using mutual information and entropy and this [paper](https://projecteuclid.org/download/pdf_1/euclid.aos/1017939142) by barron that related covering balls etc to minimax bounds\\n- [Information Theory in Probability Statistics Learning and Neural Nets](http://www.stat.yale.edu/~arb4/publications_files/COLT97.pdf) (barron 97)\\n- [Information-Theoretic Asymptotics of Bayes Methods](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=54897) (Clarke & Barron 90)\\n\\n### mdl in non-linear models\\n\\n- [MDL-based Decision Tree Pruning](https://www.aaai.org/Papers/KDD/1995/KDD95-025.pdf) (mehta et al. 95)\\n- deep learning\\n  - high-level\\n    - most unsupervised learning can be thought of as mdl\\n    - to compress the data we must take advantage of mutual info between x and y\\n  - [Learning Population Codes by Minimizing Description Length](https://www.cs.toronto.edu/~hinton/absps/mdlpop.pdf) (zemel & hinton 1995)\\n  - [Keeping Neural Networks Simple by Minimizing the Description Length of the Weights](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.3435) (hinton & van camp 1993)\\n  - [The Description Length of Deep Learning Models](https://arxiv.org/pdf/1802.07044.pdf) (blier & ollivier 2018)\\n    - compress using prequential mdl\\n  - [mdl for attention](https://arxiv.org/abs/1902.10658?utm_source=share&utm_medium=ios_app&utm_name=iossmf) (lin 2019)\\n- [Lightlike Neuromanifolds, Occam’s Razor and Deep Learning](https://pdfs.semanticscholar.org/9c61/2ea1d8e8c9ce75427f5fd879a367210c2cc7.pdf) (sun & nielsen 2019)\\n  - \"A new MDL formulation which can explain the double descent risk curve of deep learning\"\\n- [Towards Learning Convolutions from Scratch](https://arxiv.org/pdf/2007.13657v1.pdf) (neyshabur 2020)\\n  - uses some mdl as guiding principles\\n  - training with $\\\\beta$-lasso, fc weights become very local\\n\\n',\n",
       " \"---\\nlayout: notes\\ntitle: interesting science\\ncategory: research\\n---\\n\\n#  interesting science\\n\\nSome interesting papers in science and statistics.\\n\\n- [Moravec's paradox](https://en.wikipedia.org/wiki/Moravec%27s_paradox)\\n- [Held & Hein 1963](https://marom.net.technion.ac.il/files/2016/07/Held-1963.pdf) - moving kittens\\n- [Human attention vqa dataset (VQA HAT) ~ 60k images](https://computing.ece.vt.edu/~abhshkdz/vqa-hat/)\\n- [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)\\n- [Mike the headless chicken](https://en.wikipedia.org/wiki/Mike_the_Headless_Chicken)\\n- [thompson 96 evolved circuit](https://link.springer.com/chapter/10.1007/3-540-63173-9_61)\\n- [The Code for Facial Identity in the Primate Brain](https://www.sciencedirect.com/science/article/pii/S009286741730538X) - can decode 50 face components from 206 neurons\\n- [Stimulating memory](https://www.wired.com/story/ml-brain-boost/)\\n- [herculano-houzel talk](http://www.suzanaherculanohouzel.com/lab) \\n- TMS - changing people’s minds\\n- [Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche](https://advances.sciencemag.org/content/5/9/eaaw2594)\\n  - [“Languages vary a lot in terms of the information that they pack into a syllable and also in the rate that they are spoken at. But the interesting thing is that the two kind of balance each other, so that more information dense languages are spoken slower, and those that are less informationally heavy are spoken faster. This means that there is a steady information rate that is very similar among languages](https://www.technologynetworks.com/neuroscience/news/different-tongue-same-information-17-language-study-reveals-how-we-all-communicate-at-a-similar-323584?fbclid=IwAR2pZfXqaaQbSkOpiXt6x8Uqj_fX2iwxsPp-f7CJaYSZ0Nrm_slez58_Epc)\\n- [BOIDS](https://en.wikipedia.org/wiki/Boids): cool simple algorithm for emergent behavior\\n- [Image Synthesis with a Single (Robust) Classifier](https://papers.nips.cc/paper/8409-image-synthesis-with-a-single-robust-classifier.pdf)\\n- [Shrew skulls shrink for winter survival](https://www.nature.com/news/shrew-skulls-shrink-for-winter-survival-1.22874)\\n- [Performance-optimized hierarchical models predict neural responses in higher visual cortex](https://www.pnas.org/content/111/23/8619) (yamins et al. 2014)\\n- [Natural speech reveals the semantic maps that tile human cerebral cortex](https://www.nature.com/articles/nature17637) (huth et al. 2016)\\n- [A Mathematical Theory of Communication](https://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1948.tb01338.x) (shannon 1948)\\n- [hopfield networks](https://en.wikipedia.org/wiki/Hopfield_network)\\n- benjamini-hoffberg - controlling decision rate\\n- infogan chen et. al 16\\n- visual deep reps ganguli 19\\n- rulefit friedman & popescu 19\\n- [Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet](https://arxiv.org/abs/1904.00760) (brendel & bethge 2019)\\n- [statistical learning by 8-month-old infants 1996](http://science.sciencemag.org/content/274/5294/1926)\\n- [Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html) (zhu et al. 2017)\\n- [cyclegan 17](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html)\\n- [oregan 01 vision philosophy](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/sensorimotor-account-of-vision-and-visual-consciousness/BA1638CB7389102A12B336CE687EC270)\",\n",
       " '---\\nlayout: notes\\ntitle: dl theory\\ncategory: research\\n---\\n\\n*Deep learning theory is a complex emerging field - this post contains links displaying some different interesting research directions*\\n\\n\\n#  dl theory\\n\\n[off convex blog](http://www.offconvex.org/)\\n\\n## theoretical studies\\n\\nDNNs display many surprising properties\\n\\n- surprising: [more parameters yields better generalization](https://arxiv.org/abs/1802.08760) \\n- surprising: lowering training error should be harder\\n\\nMany things seem to contribute to the inductive bias of DNNs: SGD, dropout, early stopping, resnets, convolution, more layers...all of these are tangled together and many things correlate with generalization error...what are the important things and how do they contribute? \\n\\nsome more concrete questions:\\n\\n- what is happening when training err stops going down but val err keeps going down (interpolation regime)?\\n- what are good statistical markers of an effectively trained DNN?\\n- how far apart are 2 nets?\\n\\n\\n\\n### functional approximation\\n\\n- dnns are very hard to study in the parameter space (e.g. swapping two parameters changes things like the Hessian), easier to to study in the function space (e.g. the input-output relationship)\\n- nonlinear approximation (e.g. sparse coding) - 2 steps\\n1. construct a dictionary function (T)\\n  2. learn linear combination of the dictionary elements (g)\\n- background\\n  - $L^2$ function (or function space) is square integrable: $|f|^2 = \\\\int_X |f|^2 d\\\\mu$, and $|f|$ is its $L_2$-norm\\n  - **Hilbert space** - vector space w/ additional structure of inner product which allows length + angle to be measured\\n    - complete - there are enough limits in the space to allow calculus techniques (is a *complete metric space*)\\n- composition allows an approximation of a function through level sets (split it up and approximate on these sets) - Zuowei Shen ([talk](http://www.ipam.ucla.edu/programs/workshops/workshop-iii-geometry-of-big-data/?tab=schedule), [slides](../../../drive/papers/dl_theory/shen_19_composition_dl_slides.pdf)) \\n  - composition operation allows an approximation of a function f through level sets of f --\\n  - one divides up the range of f into equal intervals and approximate the functions on these sets \\n  - [nonlinear approximation via compositions](https://arxiv.org/pdf/1902.10170.pdf) (shen 2019)\\n- how do the weights in each layer help this approximation to be more effective?\\n  - here are some thoughts -- If other layers are like the first layer, the weights \"whiten\" or make the inputs more independent or random projections -- that is basically finding PC directions for low-rank inputs. \\n  - are the outputs from later layers more or less low - rank?\\n  - I wonder how this \"whitening\" helps level set estimation...\\n- [takagi functions](https://arxiv.org/abs/1112.4205)\\n- [nonlinear approximation and (deep) relu nets](https://www.math.tamu.edu/~rdevore/publications/170.pdf) - also comes with slides\\n\\n### inductive bias=implicit regularization: gradient descent finds good minima\\n\\n*DL learns solutions that generalize even though it can find many which don\\'t due to its inductive bias.*\\n\\n- [gd bousquet paper](https://arxiv.org/pdf/1803.08367.pdf) \\n- [in high dims, local minima are usually saddles (ganguli)](http://papers.nips.cc/paper/5486-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization)\\n- early stopping is very similar to ridge regression\\n- ridge regression: soln will lie in $p-dim$ row-space of X (span of the rows)\\n  - when $\\\\lambda \\\\to 0$, will give us min-norm soln (basically because we project onto $col(X)$)\\n  - early stopping in least squares\\n    - if we initialize at 0, GD soln will always be in the row-space of X\\n    - GD will converge to min-norm soln (any soln not in the row-space will necessarily have larger norm)\\n    - similar to ridge (endpoints are the same) and can related their risks very closely when $\\\\lambda = 1/t$, where $t$ is GD time iterate\\n      - assume gradient flow: take step-size to 0\\n- [srebro understanding over-parameterization\\t](https://arxiv.org/abs/1805.12076) \\n  - ex. gunasekar et al 2017: unconstrained matrix completion\\n    - grad descent on U, V yields min nuclear norm solution\\n  - ex. [soudry et al 2017](http://www.jmlr.org/papers/volume19/18-188/18-188.pdf)\\n    - sgd on logistic reg. gives hard margin svm\\n    - deep linear net gives the same thing - doesn\\'t actually changed anything\\n  - ex. [gunaskar, 2018](http://papers.nips.cc/paper/8156-implicit-bias-of-gradient-descent-on-linear-convolutional-networks)\\n    - linear convnets give smth better - minimum l1 norm in discrete fourier transform \\n  - ex. savarese 2019\\n    - infinite width relu net 1-d input\\n    - weight decay minimization minimizes derivative of TV\\n- [implicit bias towards simpler models](https://arxiv.org/abs/1805.08522)\\n- [How do infinite width bounded norm networks look in function space?](https://arxiv.org/pdf/1902.05040.pdf) (savarese...srebro 2019)\\n  - minimal norm fit for a sample is given by a linear spline interpolation (2 layer net)\\n- [analytic theory of generalization + transfer (ganguli 19)](https://arxiv.org/abs/1809.10374)\\n  - deep linear nets learn important structure of the data first (less noisy eigenvectors)\\n  - [similar paper for layer nets](https://arxiv.org/pdf/1904.13262.pdf)\\n- datasets for measuring causality\\n  - Inferring Hidden Statuses and Actions in Video by Causal Reasoning - about finding causality in the video, not interpretation\\n- **PL condition** = Polyak-Lojawsiewicz condition guarantees global convergence of loca methods\\n  - $||\\\\nabla f(x)||^2 \\\\geq \\\\alpha f(x) \\\\geq 0$\\n\\n\\n\\n### semantic biases: what correlations will a net learn?\\n\\n- [imagenet models are biased towards texture](https://arxiv.org/abs/1811.12231) (and removing texture makes them more robust)\\n- [analyzing semantic robustness](https://arxiv.org/pdf/1904.04621.pdf)\\n- [eval w/ simulations](https://arxiv.org/abs/1712.06302) (reviewer argued against this)\\n- [glcm captures superficial statistics](https://arxiv.org/abs/1903.06256)\\n- [deeper, unpruned networks are better against noise](https://arxiv.org/abs/1903.12261)\\n- [analytic theory of generalization + transfer (ganguli 19)](https://arxiv.org/abs/1809.10374)\\n- [causality in dnns talk by bottou](https://www.technologyreview.com/s/613502/deep-learning-could-reveal-why-the-world-works-the-way-it-does/)\\n  \\n  - on mnist, color vs shape will learn color\\n- [A mathematical theory of semantic development in deep neural networks](https://arxiv.org/abs/1810.10531)\\n- [rl agents learn some crazy things](https://arxiv.org/pdf/1803.03453.pdf)\\n  \\n  - [itemized list](https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml)\\n- [Emergence of Invariance and Disentanglement in Deep Representations](https://arxiv.org/abs/1706.01350) (achille & soatto 2018)\\n  -  information in the weights as a measure of complexity of a learned model (information complexity)\\n  - IB Lagrangian between the **weights of a network and the training data**, as opposed to the traditional one between the **activations and the test datum**\\n  - explains tradeoff between over/underfitting\\n- [On Dropout, Overfitting, and Interaction Effects in Deep Neural Networks](https://arxiv.org/abs/2007.00823) (lengerich..caruana, 2020)\\n\\n  - use ANOVA to meaure 1st/2nd/3rd order effects and such\\n    - approximate ANOVA decomp. using boosted trees of depth based on a particular order\\n- they find that increasing dropout rate forces nets to emphasize lower-order effects\\n- [An Investigation of Why Overparameterization Exacerbates Spurious Correlations](https://arxiv.org/abs/2005.04345)\\n  - overparameterization can hurt test error on minority groups despite improving average test error when there are spurious correlations in the data\\n\\n### expressiveness: what can a dnn represent?\\n\\n- [complexity of linear regions in deep networks](https://arxiv.org/pdf/1901.09021.pdf)\\n- [Bounding and Counting Linear Regions of Deep Neural Networks](https://arxiv.org/pdf/1711.02114.pdf)\\n- [On the Expressive Power of Deep Neural Networks](https://arxiv.org/pdf/1606.05336.pdf)\\n- [Deep ReLU Networks Have Surprisingly Few Activation Patterns](https://arxiv.org/pdf/1906.00904.pdf)\\n\\n\\n\\n### complexity + generalization: dnns are low-rank / redundant parameters\\n\\n- measuring complexity\\n  - [functional decomposition](https://arxiv.org/abs/1904.03867) (molnar 2019)\\n    - decompose function into bias + first-order effects (ALE) + interactions\\n    - 3 things: number of features used, interaction strength, main effect complexity\\n- parameters are redundant\\n  - [predicting params](http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning): weight matrices are low-rank, decompose into UV by picking a U\\n  - [pruning neurons](https://arxiv.org/abs/1507.06149)\\n  - [circulant projection](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Cheng_An_Exploration_of_ICCV_2015_paper.pdf)\\n  - [rethinking the value of pruning](https://arxiv.org/pdf/1810.05270.pdf): pruning and training from scratch, upto 30% size\\n  - [Lottery ticket](https://openreview.net/pdf?id=rJl-b3RcF7): pruning and training from initial random weights, upto 1% size ([followup](https://arxiv.org/abs/1903.01611))\\n  - [rank of relu activations](https://arxiv.org/pdf/1810.03372.pdf)\\n  - [random weights are good](https://arxiv.org/pdf/1504.08291.pdf)\\n  - [singular values of conv layers](https://arxiv.org/pdf/1805.10408.pdf)\\n  - [T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order Tensor](https://arxiv.org/abs/1904.02698)\\n- generalization\\n  - [size of the weights is more important](http://eprints.qut.edu.au/43927/)\\n  - [Quantifying the generalization error in deep learning in terms of data distribution and\\n    neural network smoothness](https://arxiv.org/pdf/1905.11427v1.pdf)\\n\\n### kernels\\n\\n- [To understand deep learning we need to understand kernel learning](https://arxiv.org/abs/1802.01396) - overfitted kernel classifiers can still fit the data well\\n- [kernels wiki](https://en.wikipedia.org/wiki/Kernel_method#cite_note-4): kernel memorizes points then uses dists between points to classify\\n- [learning deep kernels](https://arxiv.org/pdf/1811.08357v1.pdf)\\n- [learning data-adaptive kernels](https://arxiv.org/abs/1901.07114)\\n- [kernels that mimic dl](https://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf)\\n- [kernel methods](http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdfs)\\n- [wavelet support vector machines](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.412.362&rep=rep1&type=pdf) - kernels using wavelets\\n- [neural tangent kernel](https://arxiv.org/abs/1806.07572) (jacot et al. 2018)\\n  - at initialization, artificial neural networks (ANNs) are equivalent to Gaussian\\n    processes in the infinite-width limit\\n    - evolution of an ANN during training can also be described by a kernel (kernel gradient descent)\\n  - different types of kernels impose different things on a function (e.g. want more / less low frequencies)\\n    - gradient descent in kernel space can be convex if kernel is PD (even if nonconvex in the parameter space)\\n  - lazy training regime - small change in weights results in big change in loss func\\n    - so final weights are close to original weights\\n    - more precisely, gradient norm is much larger than hessian norm\\n    - jacobian doesn\\'t change during training $\\\\implies$ NTK is pretty constant throughout training\\n- [Scaling description of generalization with number of parameters in deep learning](https://arxiv.org/abs/1901.01608) (geiger et al. 2019)\\n  - number of params = N\\n  - above 0 training err, larger number of params reduces variance but doesn\\'t actually help\\n    - ensembling with smaller N fixes problem\\n  - the improvement of generalization performance with N in this classification task originates from reduced variance of fN when N gets large, as recently observed for mean-square regression\\n- [understanding the neural tangent kernel](https://arxiv.org/pdf/1904.11955.pdf) (arora et al. 2019)\\n  \\n  - method to compute the kernel quickly on a gpu\\n- [On the Inductive Bias of Neural Tangent Kernels](https://arxiv.org/abs/1905.12173) (bietti & mairal 2019)\\n- [Kernel and Deep Regimes in Overparametrized Models](https://arxiv.org/abs/1906.05827) (Woodworth...Srebro 2019)\\n  \\n  - transition between *kernel* and *deep regimes*\\n- [The HSIC Bottleneck: Deep Learning without Back-Propagation](https://arxiv.org/abs/1908.01580) (Ma et al. 2019)\\n  - directly optimize information bottleneck (approximated by HSIC) yields pretty good results\\n\\n### nearest neighbor comparisons\\n\\n- [weighted interpolating nearest neighbors can generalize well (belkin...mitra 2018)](http://papers.nips.cc/paper/7498-overfitting-or-perfect-fitting-risk-bounds-for-classification-and-regression-rules-that-interpolate)\\n- [Statistical Optimality of Interpolated Nearest Neighbor Algorithms](https://arxiv.org/abs/1810.02814)\\n- [nearest neighbor comparison](https://arxiv.org/pdf/1805.06822.pdf)\\n- [nearest embedding neighbors](https://arxiv.org/pdf/1803.04765.pdf)\\n\\n### random projections\\n\\n- [sgd for polynomials](http://proceedings.mlr.press/v32/andoni14.pdf)\\n- [deep linear better than just linear](https://arxiv.org/pdf/1811.10495.pdf)\\n- relation to bousquet - fitting random polynomials\\n- [hierarchical sparse coding for images](https://pdfs.semanticscholar.org/9636/d8aedd476ef19c762923119750aec95bf8ca.pdf) (can’t just repeat sparse coding, need to include input again)\\n- [random projections in the brain](https://www.biorxiv.org/content/biorxiv/early/2017/08/25/180471.full.pdf)….doing locality sensitive hashing (basically nearest neighbors)\\n\\n### implicit dl + optimization\\n\\n- [implicit deep learning](https://arxiv.org/abs/1908.06315) (el ghaoui et al. 2019)\\n  - $\\\\hat y (u) = Cx + D u $, where $x = \\\\phi(Ax + Bu)$\\n    - here, $u$ is a new input\\n    - $x$ is a hidden state which represents some hidden features (which depends on $u$)\\n      - **well-posedness** - want x to be unique for a given u\\n    - $A, B$ are matrices which let us compute $x$ given $u$\\n    - $C, D$ help us do the final prediction (like the final linear layer)\\n  - ex. feedforward nets\\n    - consider net with $L$ layers\\n      - $x_0 = u$\\n      - $x_{l + 1} = \\\\phi_l (W_l x_l)$\\n      - $\\\\hat y (u) = W_L x_L$\\n    - rewriting in implicit form\\n      - $x = (x_L, ..., x_1)$ - concatenate all the activations into one big vector\\n      - ![implicit_dl](../assets/implicit_dl.png)\\n      - ex. $Ax + Bu= \\\\begin{bmatrix} W_{L-1}x_{L-1} \\\\\\\\ W_{L-2} x_{L-2} \\\\\\\\ \\\\vdots \\\\\\\\ W_1x_1 \\\\\\\\ \\\\mathbf 0\\\\end{bmatrix} + \\\\begin{bmatrix} 0 \\\\\\\\ 0 \\\\\\\\ \\\\vdots \\\\\\\\ 0 \\\\\\\\ W_0 u \\\\end{bmatrix}$ \\n- [lifted neural networks](https://arxiv.org/abs/1805.01532) (askari et al. 2018)\\n  - can solve dnn $\\\\hat y = \\\\phi(W_2 \\\\phi (W_1X_0))$ by rewriting using constraints:\\n    - $X_1 = \\\\phi(W_1 X_0)$\\n    - $X_2 = \\\\phi(W_2 X_1)$\\n  - $\\\\begin{align} &\\\\min (y - \\\\hat y)^2\\\\\\\\s.t. X_1 &= \\\\phi(WX_0)\\\\\\\\X_2 &= \\\\phi(WX_1)\\\\end{align}$\\n  - can be written using Lagrangian multipliers: $\\\\min (y - \\\\hat y)^2 + \\\\lambda_1( X_1 - \\\\phi(WX_0)) + \\\\lambda_2(X_2 - \\\\phi(WX_1))$\\n- [Fenchel Lifted Networks: A Lagrange Relaxation of Neural Network Training](https://arxiv.org/abs/1811.08039) (gu et al. 2018)\\n  - in the lifted setting above, can replace Lagrangian with simpler expression using Fenchel conjugates\\n- robust optimization basics\\n  - immunize optimization problems against uncertainty in the data\\n  - do so by having worst-case constraints (e.g. $a < 5, \\\\forall a$)\\n  - *local robustness* - maximize radius surrounding parameter subject to all constraints (no objective to maximize)\\n  - *global robustness* - maximize objective subject to robustness constraint  (trades off robustness with objective value)\\n  - *non-probabilistic robust optimization models* (e.g. Wald\\'s maximin model: $\\\\underset{x}{\\\\max} \\\\underset{u}{\\\\min} f(x, u)$\\n  - also are *probabilistically robust optimization*\\n- distributional robustness - using moments in the dl work\\n  - ch 4 and ch10 of [robust optimization book](https://people.eecs.berkeley.edu/~elghaoui/robbook.html) (bental, el ghaoui, & nemirovski 2009)\\n  - [Certifying Some Distributional Robustness with Principled Adversarial Training](https://arxiv.org/abs/1710.10571) (sinha,  namkoong, & duchi 2018)\\n    - want to guarantee performance under adversarial input perturbations\\n    - considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball\\n      - during training, augments model parameter updates with worst-case perturbations of training data\\n    - little extra cost and achieves guarantees for smooth losses\\n  - [On Distributionally Robust Chance-Constrained Linear Programs](https://pdfs.semanticscholar.org/dc8a/e0f1ee878d68208184d123020f1acd2525bb.pdf) (calafiore & el ghaoui 2006)\\n    - linear programs where data (in the constraints) is random\\n    - want to enforce the constraints up to a given prob. level\\n    - can convert the prob. constraints into convex 2nd-order cone constraints\\n    - under distrs. for the random data, can guarantee constraints\\n- [Differentiable Convex Optimization Layers](http://web.stanford.edu/~boyd/papers/pdf/diff_cvxpy.pdf) (agrawal et al. 2019)\\n\\n### statistical physics\\n\\n- [Statistical Mechanics of Deep Learning](https://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031119-050745) (bahri et al. 2019)\\n  - what is the advantage of depth  - connect to dynamical phase transitions\\n    - there are several function which require only polynomial nodes in each layer for deep nets, but exponential for shallow nets\\n  - what is the shape of the loss landscape - connect to random Gaussian processes, sping glasses, and jamming\\n  - how to pick a good parameter initialization?\\n  - bounding generalization error\\n    - often, generalization bounds take the form $\\\\epsilon_{test} \\\\leq \\\\epsilon_{train} + \\\\frac {\\\\mathcal C (\\\\mathcal F)} p$, where $\\\\mathcal C (\\\\mathcal F)$ is the complexity of a function class and $p$ is the number of examples\\n      - ex. VC-dimension, Rademacher complexity\\n    - alternative framework: algorithmic stability - will generalize if map is stable wrt perturbations of the data $\\\\mathcal D$\\n    - altenative: PAC bounds suggest if distr. of weights doesn\\'t change much during training, generalization will be succesful\\n  - deep linear networks\\n    - student learns biggest singular values of the input-output correlation matrix $\\\\Sigma = \\\\sum_i y_i x_i^T$, so it learns the important stuff first and the noise last\\n  - infinite-width limit\\n    - if parameters are random, indcues a prior distribution $P(\\\\mathcal F)$ over the space of functions\\n    - in th limit of infinite width, this prior is Gaussian, with a specific correlation kernel\\n    - learning is similar to learning the Bayesian posterior $P(f|data)$, but connecting this to sgd is still not clear\\n\\n## empirical studies\\n\\n###  interesting empirical papers\\n\\n- [modularity (“lottery ticket hypothesis”)](https://arxiv.org/abs/1803.03635) \\n  - contemporary experience is that it is difficult to train small architectures from scratch, which would similarly improve training performance - **lottery ticket hypothesis**: large networks that train successfully contain subnetworks that--when trained in isolation--converge in a comparable number of iterations to comparable accuracy\\n- [ablation studies](https://arxiv.org/abs/1812.05687)\\n- [deep learning is robust to massive label noise](https://arxiv.org/pdf/1705.10694.pdf)\\n- [are all layers created equal?](https://arxiv.org/pdf/1902.01996.pdf)\\n- [Truth or backpropaganda? An empirical investigation of deep learning theory](https://openreview.net/forum?id=HyxyIgHFvr)\\n\\n### adversarial + robustness\\n\\n- [robustness may be at odds with accuracy](https://openreview.net/pdf?id=SyxAb30cY7) (madry 2019)\\n  - adversarial training helps w/ little data but hurts with lots of data\\n  - adversarially trained models have more meaningful gradients (and their adversarial examples actually look like other classes)\\n- [Generalizability vs. Robustness: Adversarial Examples for Medical Imaging](https://arxiv.org/abs/1804.00504)\\n- [Towards Robust Interpretability with Self-Explaining Neural Networks](https://arxiv.org/pdf/1806.07538.pdf) \\n- [Adversarial Attacks and Defenses in Images, Graphs and Text: A Review](https://arxiv.org/abs/1909.08072) (xu et al. 2019)\\n  - *Adversarial examples are inputs to machine learning models that an attacker intentionally designed to cause the model to make mistakes*\\n  - threat models\\n    - poisoning attack (insert fake samples into training data) vs. evasion attack (just evade at test time)\\n    - targeted attack (want specific class) vs. non-targeted attack (just change the prediction)\\n  - adversary\\'s knowledge\\n    - white-box - adversary knows everything\\n    - black-box - can only feed inputs and get outputs\\n    - gray-box - might have white box for limited amount of time\\n  - security evaluation\\n    - robustness - minimum norm perturbation to change class\\n    - adversarial loss - biggest change in loss within some epsilon ball\\n  - ![Screen Shot 2020-02-04 at 1.54.49 PM](../assets/adv_attacks_table.png)\\n  - ![Screen Shot 2020-02-04 at 1.54.28 PM](../assets/adv_attack_hierarchy.png)\\n- [AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty](https://arxiv.org/abs/1912.02781) (hendrycks et al. 2020)\\n  - do a bunch of transformations and average images to create each training image\\n\\n### tools for analyzing\\n\\n- dim reduction: [svcca](http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-understanding-and-improvement), diffusion maps\\n- viz tools: [bei wang](http://www.sci.utah.edu/~beiwang/)\\n- [visualizing loss landscape](https://arxiv.org/pdf/1712.09913.pdf)\\n- 1d: plot loss by extrapolating between 2 points (start/end, 2 ends)\\n  - goodfellow et al. 2015, im et al. 2016\\n  - [exploring landscape with this technique](https://arxiv.org/pdf/1803.00885.pdf)\\n  - 2d: plot loss on a grid in 2 directions\\n  - important to think about scale invariance (dinh et al. 2017)\\n  - want to scale direction vector to have same norm in each direction as filter\\n  - use PCA to find important directions (ex. sample w at each step, pca to find most important directions of variance)\\n\\n### misc theoretical areas\\n\\n- deep vs. shallow [rvw](http://cbmm.mit.edu/sites/default/files/publications/CBMM-Memo-058v5.pdf)\\n- [probabilistic framework](https://www.nari.ee.ethz.ch/commth//pubs/files/deep-2016.pdf)\\n- information bottleneck: tishby paper + david cox follow-up\\n- [Emergence of Invariance and Disentanglement in Deep Representations](https://arxiv.org/abs/1706.01350)\\n- [manifold learning](https://www.deeplearningbook.org/version-2015-10-03/contents/manifolds.html)\\n  - [random manifold learning paper](https://ieeexplore.ieee.org/document/7348689/) \\n\\n### comparing representations\\n\\n- [shared representations across nets](https://arxiv.org/abs/1811.11684)\\n- [comparing across random initializations](https://arxiv.org/abs/1810.11750)\\n\\n\\n\\n### simple papers\\n\\n- [rvw of random features approach](https://arxiv.org/pdf/1904.00687.pdf)\\n- [similar nets learn different weights](http://proceedings.mlr.press/v44/li15convergent.pdf)\\n\\n\\n\\n### adam vs sgd\\n\\n- svd parameterization rnn paper: [inderjit paper](https://arxiv.org/pdf/1803.09327.pdf)\\n    - original adam paper: [kingma 15](https://arxiv.org/abs/1412.6980)\\n    - [regularization via SGD](https://arxiv.org/pdf/1806.00900.pdf) (layers are balanced: du 18)\\n    - marginal value of adaptive methods: [recht 17](http://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning)\\n    - comparing representations: [svcca](https://arxiv.org/abs/1706.05806)\\n    - [montanari 18](https://arxiv.org/pdf/1804.06561.pdf) pde mean field view\\n    - [normalized margin bounds](http://papers.nips.cc/paper/7204-spectrally-normalized-margin-bounds-for-neural-networks)\\n\\n### memorization background\\n\\n- [memorization on single training example](https://arxiv.org/abs/1902.04698v2)\\n- [memorization in dnns](https://arxiv.org/pdf/1706.05394.pdf)\\n- “memorization” as the behavior exhibited by DNNs trained on noise, and conduct a series of experiments that contrast the learning dynamics of DNNs on real vs. noise data\\n  - look at critical samples - adversarial exists nearby\\n- networks that generalize well have deep layers that are approximately linear with respect to batches of similar inputs\\n  - networks that memorize their training data are highly non-linear with respect to similar inputs, even in deep layers\\n  - expect that with respect to a single class, deep layers are approximately linear\\n- [example forgetting paper](https://openreview.net/forum?id=BJlxm30cKm)\\n- [secret sharing](https://arxiv.org/abs/1802.08232)\\n- [memorization in overparameterized autoencoders](https://arxiv.org/abs/1810.10333)\\n  - autoencoders don\\'t lean identity, but learn projection onto span of training examples = memorization of training examples\\n  - sometimes output individual training images, not just project onto space of training images\\n- https://arxiv.org/pdf/1810.10333.pdf\\n- https://arxiv.org/pdf/1909.12362.pdf\\n- https://pdfs.semanticscholar.org/a624/6278cb5e2d0ab79fe20fe20a41c586732a11.pdf\\n- [Auto-encoders: reconstruction versus compression](http://www.yann-ollivier.org/rech/publs/aagen.pdf)\\n\\n### probabilistic inference\\n\\n- multilayer idea\\n- [equilibrium propagation](https://www.frontiersin.org/articles/10.3389/fncom.2017.00024/full)\\n\\n### architecture search background\\n\\n- ideas: nested search (retrain each time), joint search (make arch search differentiable), one-shot search (train big net then search for subnet)\\n  - [asap: online pruning + training](https://arxiv.org/pdf/1904.04123.pdf)\\n  - [rvw](https://arxiv.org/abs/1904.00438)\\n  - [original (dumb) strategy](https://arxiv.org/abs/1611.01578)\\n  - [progressive nas](http://openaccess.thecvf.com/content_ECCV_2018/papers/Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper.pdf)\\n  - [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/abs/1802.03268) - sharing params\\n  - [randomly replace layers with the identity when training](https://link.springer.com/chapter/10.1007/978-3-319-46493-0_39)\\n  - [learning both weights and connections](https://pdfs.semanticscholar.org/1ff9/a37d766e3a4f39757f5e1b235a42dacf18ff.pdf)\\n  - [single-path one-shot search](https://arxiv.org/abs/1904.00420)\\n\\n### bagging and boosting\\n\\n- [analyzing bagging](https://projecteuclid.org/download/pdf_1/euclid.aos/1031689014) (buhlmann and yu 2002)\\n- [boosting with the L2 loss](http://zmjones.com/static/statistical-learning/buhlmann-jasa-2003.pdf) (buhlmann & yu 2003)\\n- [boosting algorithms as gradient descent](http://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf) (mason et al. 2000)\\n\\n## basics\\n\\n- [good set of class notes](https://people.csail.mit.edu/madry/6.883/)\\n- demos to gain intuition\\n  - [colah](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) \\n    - [tf playground](https://playground.tensorflow.org/)\\n    - [convnetJS](https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html)\\n    - [ml playground](http://ml-playground.com/)\\n\\n  - overview / reviews\\n\\n    - [mathematics of dl](https://arxiv.org/abs/1712.04741)\\n    - [stanford class](https://stats385.github.io/) ([good readings](https://stats385.github.io/readings))\\n    - [dali 2018 talks](http://dalimeeting.org/dali2018/workshopTheoryDL.html)\\n    - [overview (myths)](http://www.mit.edu/~rakhlin/papers/myths.pdf) \\n- some people involved: nathan srebro, sanjeev arora, jascha sohl-dickstein, tomaso poggio, stefano soatto, ben recht, [olivier bousquet](https://arxiv.org/abs/1803.08367), jason lee, simon shaolei du\\n  - interpretability: cynthia rudin, rich caruana, been kim, nicholas papernot, finale doshi-velez\\n  - neuro: eero simoncelli, haim sompolinsky',\n",
       " '{:toc}\\n\\n## Ideas for deep learning from neuroscience\\n\\nThis aims to be a primer on aspects of neuroscience which could be relevant to deep learning researchers. These two communities are becoming more intertwined, and could benefit greatly from each other. However, current literature in neuroscience has a steep learning curve, requiring learning much about biology. This primer aims to equip deep learning researchers with the basic computational principles of the brain, to draw inspiration and provide a new perspective on neural computation.\\n\\n### Explaining concepts from neuroscience to inform deep learning\\n\\nModern deep learning evokes many parallels with the human brain. Here, we explore how these two concepts are related and how neuroscience can inform deep learning going forward <dt-fn>Note that this post largely ignores the important reverse question: how can deep learning inform neuroscience?</dt-fn>\\n\\nThe brain currently outperforms deep learning in a number of different ways: efficiency, parallel computation, not forgetting, robustness. Thus, in these areas and others, the brain can offer high-level inspiration as well as more detailed algorithmic ideas on how to solve complex problems.\\n\\nWe begin with some history and perspective before further exploring these concepts at 3 levels: (1) the neuron level, (2) the network level, and (3) high-level concepts.\\n\\n### Brief history\\nThe history of deep learning is intimately linked with neuroscience. In vision, the idea of hierarchical processing dates back to Hubel and Weisel <dt-cite key=\"hubel1962receptive\"></dt-cite> and the modern idea of convolutional neural networks dates back to the necognitron<dt-cite key=\"fukushima1982neocognitron\"></dt-cite>.\\n\\nRanges from neurally-inspired -> biologically plausible\\n\\nComputational neuroscientists often discuss understanding computation at Marr\\'s 3 levels of understanding: (1) computational, (2) algorithmic, and (3) mechanistic<dt-cite key=\"marr1976understanding\"></dt-cite>. The first two levels are most crucial to understanding here, while the third may yield insights for the field of neuromorphic computing <dt-cite key=\"schuman2017survey\"></dt-cite>.\\n\\n### Cautionary notes\\n\\nThere are dangers in deep learning researchers constraining themselves to biologically plausible algorithms. First, the underlying hardware of the brain and modern von Neumman-based architectures is drastically different and one should not assume that the same algorithms will work on both systems. Several examples, such as backpropagation, were derived by deviating from the mindset of mimicking biology.\\n\\nSecond, the brain does not solve probleDangers for going too far.... One wouldn\\'t want to draw inspiration from the retina to put a hole in the camera.\\n\\n\\n<img width=\"50%\" src=\"figs/retina.png\"></img>\\nGallery of brain failures. Example, inside-out retina, V1 at back...\\n\\n\\n## Neuron level\\n\\nThe fundamental unit of the brain is the neuron, which takes inputs from other neurons and then provides an output.\\n\\nIndividual neurons perform varying computations. Some neurons have been show to linearly sum their inputs <dt-cite key=\"singh2017consensus\"></dt-cite>\\n\\n - neurons are complicated (perceptron -> ... -> detailed comparmental model)\\n\\nFor more information, see a very good review on modeling individual neurons<dt-cite key=\"herz2006modeling\"></dt-cite>.\\n\\n - converting to spikes introduces noise <dt-cite key=\"carandini2004amplification\"></dt-cite>- perhaps just price of long-distance communication\\n\\n## Network level\\n\\n Artificial neural networks can compute in several different ways. There is some evidence in the visual system that neurons in higher layers of visual areas can, to some extent, be predicted linearly by higher layers of deep networks<dt-cite key=\"yamins2014performance\"></dt-cite>. However, this certainly isn\\'t true in general. Key factors\\n\\n For the simplest intuition, here we provide an example of a canonical circuit for computing the maximum of a number of elements: the winner-take-all circuit.\\n\\n Other network structures, such as that of the hippocampus are surely useful as well.\\n\\n Questions at this level bear on population coding, or how groups of neurons jointly represent information.\\n\\n## High-level concepts\\n\\n Key concepts differentiate the learning process. Online,\\n- learning\\n\\n- high-level\\n  - attention\\n  - memory\\n  - robustness\\n  - recurrence\\n  - topology\\n  - glial cells\\n\\n- inspirations\\n  - canonical cortical microcircuits\\n  - nested loop architectures\\n  - avoiding catostrophic forgetting through synaptic complexity\\n  - learning asymmetric recurrent generative models\\n- spiking networks ([bindsnet](https://github.com/Hananel-Hazan/bindsnet))\\n- neural priors\\n  - cox...\\n\\n## Conclusion\\n\\nA convergence of ideas from neuroscience and deep learning can be useful.',\n",
       " '---\\nlayout: notes\\ntitle: scattering transform\\ncategory: research\\n---\\n\\n#  scattering transform\\n\\n**some papers involving the scattering transform and similar developments bringing structure to replace learned filters**\\n\\n- some of the researchers involved\\n  - edouard oyallan, joan bruna, stephan mallat, Helmut Bölcskei, max welling\\n\\n## goals\\n\\n- benefits        \\n   - all filters are defined\\n   - more interpretable\\n   - more biophysically plausible\\n- scattering transform - computes a translation invariant repr. by cascading wavelet transforms and modulus pooling operators, which average the amplitude of iterated wavelet coefficients\\n\\n## review-type\\n- [Understanding deep convolutional networks](https://arxiv.org/abs/1601.04920) (mallat 2016)\\n- [Mathematics of deep learning](https://arxiv.org/abs/1712.04741) (vidal et al. 2017)\\n- [Geometric deep learning: going beyond euclidean data](https://arxiv.org/abs/1611.08097) (bronstein et al. 2017)\\n\\n## initial papers\\n\\n- [classification with scattering operators](https://arxiv.org/abs/1011.3023) (bruna & mallat 2010)\\n- [recursive interferometric repr.](https://www.di.ens.fr/data/publications/papers/Eusipco2010InterConfPap.pdf) (mallat 2010)\\n- [group invariant scattering](https://arxiv.org/abs/1101.2286) (mallat 2012)\\n  - introduces scat transform\\n- [Generic deep networks with wavelet scattering](https://arxiv.org/abs/1312.5940) (oyallan et al. 2013)\\n- [Invariant Scattering Convolution Networks](https://arxiv.org/abs/1203.1513) (bruna & mallat 2012)\\n   - introduces the scattering transform implemented as a cnn\\n- [Deep scattering spectrum](https://arxiv.org/abs/1304.6763) (anden & mallat 2013)\\n\\n### scat_conv\\n\\n- [Deep roto-translation scattering for object classification](https://arxiv.org/abs/1412.8659) (oyallan & mallat 2014)\\n    - can capture rounded figures\\n    - can further impose robustness to rotation variability (although not full rotation invariance)\\n- [Visualizing and improving scattering networks](https://arxiv.org/pdf/1709.01355.pdf) (cotter et al. 2017)\\n  - add deconvnet to visualize\\n- [Scattering Networks for Hybrid Representation Learning](https://hal.inria.fr/hal-01837587/document) (oyallon et al. 2018)\\n    - using early layers scat is good enough\\n- [i-RevNet: Deep Invertible Networks](https://arxiv.org/abs/1802.07088) (jacobsen et al. 2018)\\n- [Scaling the scattering transform: Deep hybrid networks](http://openaccess.thecvf.com/content_ICCV_2017/papers/Oyallon_Scaling_the_Scattering_ICCV_2017_paper.pdf) (oyallon et al. 2017)\\n    - use 1x1 convolutions to collapse accross channels\\n- jacobsen_17 \"Hierarchical Attribute CNNs\"\\n    - modularity\\n- cheng_16 \"Deep Haar scattering networks\"\\n\\n### neuro style\\n\\n- https://arxiv.org/pdf/1809.10504.pdf\\n\\n### papers by other groups\\n\\n- cohen_16 \"[Group equivariant convolutional networks](http://www.jmlr.org/proceedings/papers/v48/cohenc16.pdf)\"\\n  - introduce G-convolutions which share more wieghts than normal conv\\n- worrall_17 \"[Interpretable transformations with encoder-decoder networks](http://openaccess.thecvf.com/content_ICCV_2017/papers/Worrall_Interpretable_Transformations_With_ICCV_2017_paper.pdf)\"\\n  - look at interpretability\\n- bietti_17 \"[Invariance and stability of deep convolutional representations](http://papers.nips.cc/paper/7201-invariance-and-stability-of-deep-convolutional-representations)\"\\n  - theory paper\\n- cotter_18 \"[Deep learning in the wavelet domain](https://arxiv.org/pdf/1811.06115.pdf)\"\\n\\n### wavelet style transfer\\n\\n- [Photorealistic Style Transfer via Wavelet Transforms>](https://arxiv.org/pdf/1903.09760v1.pdf)\\n\\n## helmut lab papers\\n\\n- [Deep Convolutional Neural Networks Based on Semi-Discrete Frames](https://arxiv.org/abs/1504.05487) (wiatowski et al. 2015)\\n  - allowing for different and, most importantly, general semidiscrete frames (such as, e.g., Gabor frames, wavelets, curvelets, shearlets, ridgelets) in distinct network layers\\n  - translation-invariant, and we develop deformation stability results\\n- wiatoski_18 \"[A mathematical theory of deep convolutional neural networks for feature extraction](https://ieeexplore.ieee.org/document/8116648/)\"\\n  - encompasses general convolutional transforms - general semi-discrete frames (including Weyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned filters), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating, e.g., sub-sampling and averaging\\n    - all of these elements can be different in different network layers.\\n  - translation invariance result of vertical nature in the sense of the features becoming progressively more translation-invariant with increasing network depth\\n  - deformation sensitivity bounds that apply to signal classes such as, e.g., band-limited functions, cartoon functions, and Lipschitz functions.\\n- wiatowski_18 \"[Energy Propagation in Deep Convolutional Neural Networks](https://arxiv.org/pdf/1704.03636.pdf)\"\\n\\n## nano papers\\n\\n- yu_06 \"A Nanoengineering Approach to Regulate the Lateral Heterogeneity of Self-Assembled Monolayers\"\\n  - regulate heterogeneity of self-assembled monlayers\\n    - used nanografting + self-assembly chemistry\\n- bu_10 nanografting - makes more homogenous morphology\\n- fleming_09 \"dendrimers\"\\n  - scanning tunneling microscopy - provides highest spatial res\\n  - combat this for insulators\\n- lin_12_moire\\n  - prob moire effect with near-field scanning optical microscopy\\n- chen_12_crystallization\\n\\n### l2 functions\\n\\n- $L^2$ function is a function $f: X \\\\to \\\\mathbb{R}$ that is square integrable: $|f|^2 = \\\\int_X |f|^2 d\\\\mu$ with respect to the measure $\\\\mu$\\n  - $|f|$ is its $L^2$-norm\\n- **measure ** = nonnegative real function from a delta-ring F such that $m(\\\\empty) = 0$ and $m(A) = \\\\sum_n m(A_n)$\\n- **Hilbert space** H: a vectors space with an innor product $<f, g>$ such that the following norm turns H into a complete metric space: $|f| = \\\\sqrt{<f, f>}$\\n- **diffeomorphism** is an [isomorphism](https://en.wikipedia.org/wiki/Isomorphism) of [smooth manifolds](https://en.wikipedia.org/wiki/Smooth_manifold). It is an [invertible function](https://en.wikipedia.org/wiki/Invertible_function) that [maps](https://en.wikipedia.org/wiki/Map_(mathematics)) one [differentiable manifold](https://en.wikipedia.org/wiki/Differentiable_manifold) to another such that both the function and its inverse are [smooth](https://en.wikipedia.org/wiki/Smooth_function).\\n\\n\\n### reversible/invertible models\\n\\n- [williams  inverting w/ autoregressive models](https://arxiv.org/abs/1806.00400)\\n\\n- [arora reversibility](https://arxiv.org/pdf/1511.05653.pdf)\\n- iResNet\\n\\n## unsupervised learning\\n\\n- [Data-Efficient Image Recognition with Contrastive Predictive Coding](https://arxiv.org/pdf/1905.09272.pdf)\\n  - pre-training with CPC on ImageNet yields super good results\\n- [Unsupervised Visual Representation Learning by Context Prediction](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.html) (efros 15)\\n  - predict relative location of different patches\\n- [Billion-scale semi-supervised learning for image classification](https://arxiv.org/pdf/1905.00546.pdf) (fair 19)\\n  - unsupervised learning + model distillation succeeds on imagenet',\n",
       " '---\\nlayout: notes\\ntitle: ml in medicine\\ncategory: research\\n---\\n\\n**some rough notes on ml in medicine**\\n\\n#  ml in medicine\\n\\n## general\\n\\n- 3 types\\n\\t- disease and patient categorization (e.g. classification)\\n\\t- fundamental biological study\\n\\t- treatment of patients\\n- philosophy\\n  - want to focus on problems doctors can\\'t do\\n  - alternatively, focus on automating problems parents can do to screen people at home in cost-effective way\\n- websites are often easier than apps for patients\\n- [The clinical artificial intelligence department: a prerequisite for success](https://informatics.bmj.com/content/27/1/e100183) (cosgriff et al. 2020) - we need designated departments for clinical ai so we don\\'t have to rely on 3rd-party vendors and can test for things like distr. shift\\n- [challenges in ai healthcare (news)](https://www.statnews.com/2019/06/19/what-if-ai-in-health-care-is-next-asbestos/)\\n  - adversarial examples\\n  - things can\\'t be de-identified\\n  - algorithms / data can be biased\\n  - correlation / causation get confused\\n- healthcare is 20% of US GDP\\n\\n### high-level\\n\\n- focus on building something you want to deploy\\n  - clinically useful - more efficient, cutting costs?\\n  - effective - does it improve the current baseline\\n  - focused on patient care - what are the unintended consequences\\n- need to think a lot about regulation\\n  - USA: FDA\\n  - Europe: CE (more convoluted)\\n- intended use\\n  - very specific and well-defined\\n\\n### criticisms\\n\\n- [Dissecting racial bias in an algorithm used to manage the health of populations ](https://science.sciencemag.org/content/366/6464/447)(obermeyer et al. 2019)\\n\\n## medical system\\n\\n### evaluation\\n\\n- doctors are evaluated infrequently (and things like personal traits are often included)\\n- US has pretty good care but it is expensive per patient\\n- expensive things (e.g. Da Vinci robot)\\n- even if ml is not perfect, it may still outperform some doctors\\n\\n### medical education\\n\\n- rarely textbooks (often just slides)\\n- 1-2% miss rate for diagnosis can be seen as acceptable\\n- [how doctors think](https://www.newyorker.com/magazine/2007/01/29/whats-the-trouble)\\n  - 2 years: memorizing facts about physiology, pharmacology, and pathology\\n  - 2 years learning practical applications for this knowledge, such as how to decipher an EKG and how to determine the appropriate dose of insulin for a diabetic\\n  - little emphasis on metal logic for making a correct diagnosis and avoiding mistakes\\n  - see work by pat croskerry\\n  - there is limited data on misdiagnosis rates\\n  - **representativeness** error - thinking is overly influenced by what is typically true\\n  - **availability** error - tendency to judge the likelihood of an event by the ease with which relevant examples come to mind\\n    - common infections tend to occur in epidemics, afflicting large numbers of people in a single community at the same time\\n    - confirmation bias\\n  - **affective** error - decisions based on what we wish were true (e.g. caring too much about patient)\\n  - See one, do one, teach one - teaching axiom\\n\\n### political elements\\n\\n- [why doctors should organize](https://www.newyorker.com/culture/annals-of-inquiry/why-doctors-should-organize)\\n- big pharma\\n- day-to-day\\n  - Doctors now face a burnout epidemic: thirty-five per cent of them show signs of high depersonalization\\n  - according to one recent [report](https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2730353?resultClick=1), only thirteen per cent of a physician’s day, on average, is spent on doctor-patient interaction\\n  - [study](https://www.nytimes.com/2017/11/14/well/live/the-patients-vs-paperwork-problem-for-doctors.html) during an average, eleven-hour workday, six hours are spent at the keyboard, maintaining electronic health records.\\n  - medicare\\'s r.v.u - changes how doctors are reimbursed, emphasising procedural over cognitive things\\n  - ai could help - make simple diagnoses faster, reduce paperwork, help patients manage their own diseases like diabetes\\n  - ai could also make things worse - hospitals are mostly run by business people\\n\\n## medical communication\\n\\n### \"how do doctors think?\"\\n\\n- easy to misinterpret things to be causal\\n- often no intuition for even relatively simple engineered features, such as averages\\n- doctors require context for features (e.g. this feature is larger than the average)\\n- often have some rules memorized (otherwise memorize what needs to be looked up)\\n  - unclear how well doctors follow rules\\n  - some rules are 1-way (e.g. only follow it if it says there is danger, otherwise use your best judgement)\\n    - 2-way rules are better\\n    - without proper education 1-way rules can be dangerously used as 2-way rules\\n    - doesn\\'t make sense to judge 1-way rules on both sepcificity and sensitivity\\n- rules are often ambiguous (e.g. what constitutes vomiting)\\n- **doctors adapt to personal experience** - may be unfair to evaluate them on larger dataset\\n- sometimes said that doctors know 10 medications by heart\\n- [Overconfidence in Clinical Decision Making](https://www.amjmed.com/article/S0002-9343(08)00152-6/pdf) (croskerry 2008)\\n  - most uncertainty: family medicine [FM] and emergency medicine [EM]\\n  - some uncertainty: internal medicine\\n  - little uncertainty: specialty disciplines\\n  - 2 systems at work: intuitive (uses context, heuristics) vs analytic (systematic, rule-based)\\n    - a combination of both performs best\\n  - doctors are often black boxes as well - validated infrequently, unclear how closely they follow rules\\n  - doctors adapt to local conditions - should be evaluated only on local dataset\\n-  [potential liabilities for physicians using ai](https://jamanetwork.com/journals/jama/fullarticle/2752750) (price et al. 2019)\\n- [What\\'s the trouble. How doctors think](https://www.newyorker.com/magazine/2007/01/29/whats-the-trouble). New Yorker. 2007\\n- [JAMA Users’ Guide to the Medical Literature](https://jamanetwork.com/journals/jama/article-abstract/192850?utm_campaign=articlePDF&utm_medium=articlePDFlink&utm_source=articlePDF&utm_content=jamapediatrics.2019.1075)\\n- [TRIPOD 22 points paper](https://www.tripod-statement.org/Portals/0/Tripod Checklist Prediction Model Development and Validation PDF.pdf)\\n- [basic stats in the step1 exam](https://step1.medbullets.com/topic/dashboard?id=101&specialty=101)\\n- [How to Read Articles That Use Machine Learning: Users’ Guides to the Medical Literature](https://jamanetwork.com/journals/jama/fullarticle/2754798) (liu et al. 2019\\n- [Carmelli et al. 2018](https://www.annemergmed.com/article/S0196-0644(18)30327-5/pdf) - primer for CDRs but also a good example of what sort of article I have envisioned creating.\\n- [Looking through the retrospectoscope: reducing bias in emergency medicine chart review studies.](https://www.ncbi.nlm.nih.gov/pubmed/24746846) (kaji et al. 2018)\\n\\n### communicating findings\\n\\n- [don\\'t use ROC curves, use deciles](https://modelplot.github.io/intro_modelplotpy.html)\\n- [need to evaluate use, not just metric](https://jamanetwork.com/journals/jama/fullarticle/2748179)\\n- internal/external validity = training/testing error\\n- model -> fitted model\\n- retrospective (more confounding, looks back) vs prospective study\\n- internal/external validity = train/test (although external was usually using different patient population, so is stronger)\\n- specificity/sensitivity = precision/recall\\n\\n## examples\\n\\n### succesful examples of ai in medicine\\n\\n- [ECG](https://www.nejm.org/doi/full/10.1056/NEJM199112193252503) (NEJM, 1991)\\n- EKG has a small interpretation on it\\n- there used to be bayesian networks / expert systems but they went away...\\n\\n### icu interpretability example\\n\\n- goal: explain the model not the patient (that is the doctor’s job)\\n- want to know interactions between features\\n- some features are difficult to understand\\n  - e.g. max over this window, might seem high to a doctor unless they think about it\\n- some features don’t really make sense to change (e.g. was this thing measured)\\n- doctors like to see trends - patient health changes over time and must include history\\n- feature importance under intervention\\n\\n### high-performance ai studies\\n\\n- chest-xray: chexnet\\n- echocardiograms: madani, ali, et al. 2018\\n- skin: esteva, andre, et al. 2017\\n- pathology: campanella, gabriele, et al.. 2019\\n- mammogram: kerlikowske, karla, et al. 2018\\n\\n\\n\\n## improving medical studies\\n\\n- Machine learning methods for developing precision treatment rules with observational data (Kessler et al. 2019)\\n  - goal: find precision treatment rules\\n  - problem: need large sample sizes but can\\'t obtain them in RCTs\\n  - recommendations\\n    - screen important predictors using large observational medical records rather than RCTs\\n      - important to do matching / weighting to account for bias in treatment assignments\\n      - alternatively, can look for natural experiment / instrumental variable / discontinuity analysis\\n      - has many benefits\\n    - modeling: should use ensemble methods rather than individual models',\n",
       " '---\\nlayout: notes\\ntitle: dl for neuro\\ncategory: research\\n---\\n\\n\\n\\n#  dl for neuro\\n\\n\\n\\n## Ideas for neuroscience using deep learning\\n\\nlist of comparisons: https://docs.google.com/document/d/1qil2ylAnw6XrHPymYjKKYNDJn2qZQYA_Qg2_ijl-MaQ/edit\\n\\nModern deep learning evokes many parallels with the human brain. Here, we explore how these two concepts are related and how deep learning can help understand neural systems using big data.\\n\\n- https://medium.com/the-spike/a-neural-data-science-how-and-why-d7e3969086f2\\n\\n## Brief history\\n\\nThe history of deep learning is intimately linked with neuroscience, with the modern idea of convolutional neural networks dates back to the necognitron<dt-cite key=\"fukushima1982neocognitron\"></dt-cite>.\\n\\n### pro big-data\\n\\nArtificial neural networks can compute in several different ways. There is some evidence in the visual system that neurons in higher layers of visual areas can, to some extent, be predicted linearly by higher layers of deep networks<dt-cite key=\"yamins2014performance\"></dt-cite>. However, this certainly isn\\'t true in general.\\n\\n- when comparing energy-efficiency, must normalize network performance by energy / number of computations / parameters\\n\\n### anti big-data\\n\\n- could neuroscientist  understand microprocessor\\n- no canonical microcircuit\\n\\n## Data types\\n\\n|              | EEG      | ECoG              | Local Field potential (LFP) -> microelectrode array | single-unit | calcium imaging | fMRI     |\\n| ------------ | -------- | ----------------- | --------------------------------------------------- | ----------- | --------------- | -------- |\\n| scale        | high     | high              | low                                                 | tiny        | low             | high     |\\n| spatial res  | very low | low               | mid-low                                             | x           | low             | mid-low  |\\n| temporal res | mid-high | high              | high                                                | super high  | high            | very low |\\n| invasiveness | non      | yes (under skull) | very                                                | very        | non             | non      |\\n\\n- [ovw of advancements in neuroengineering](https://medium.com/neurotechx/timeline-of-global-highlights-in-neuroengineering-2005-2018-75e4637b9e38)\\n- cellular\\n  - extracellular microeelectrodes\\n  - intracellular microelectrode\\n  - **neuropixels**\\n- optical\\n  - calcium imaging / fluorescence imaging\\n  - whole-brain light sheet imaging\\n  - voltage-sensitive dyes / voltage imaging\\n  - **adaptive optics**\\n  - fNRIS - like fMRI but cheaper, allows more immobility, slightly worse spatial res\\n  - **oct** - noninvasive - can look at retina (maybe find biomarkers of alzheimer\\'s)\\n  - fiber photometry - optical fiber implanted delivers excitation light\\n- alteration\\n  - optogenetic stimulation\\n  - tms\\n    - genetically-targeted tms: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4846560/\\n  - local microstimulation with invasive electrodes\\n- high-level\\n  - EEG/ECoG\\n  - MEG\\n  - fMRI/PET\\n    - molecular fmri (bartelle)\\n  - MRS\\n  - event-related optical signal = near-infrared spectroscopy\\n- implantable\\n  - neural dust\\n\\n## general projects\\n\\n- could a neuroscientist understand a deep neural network? - use neural tracing to build up wiring diagram / function\\n- prediction-driven dimensionality reduction\\n- deep heuristic for model-building\\n- joint prediction of different input/output relationships\\n- joint prediction of neurons from other areas\\n\\n## datasets\\n\\n- [non-human primate optogenetics datasets](https://osf.io/mknfu/)\\n- [vision dsets](https://www.visualdata.io/)\\n    - MRNet: knee MRI diagnosis\\n- [datalad lots of stuff](http://datalad.org/datasets.html)\\n- springer 10k calcium imaging recording: https://figshare.com/articles/Recordings_of_ten_thousand_neurons_in_visual_cortex_during_spontaneous_behaviors/6163622 \\n\\n  - springer 2: 10k neurons with 2800 images\\n\\n  - stringer et al. data\\n\\n  - 10000 neurons from visual cortex\\n- neuropixels probes\\n    - [10k neurons visual coding](https://portal.brain-map.org/explore/circuits/visual-coding-neuropixels) from allen institute\\n    - this probe has also been used in [macaques](https://www.cell.com/neuron/pdf/S0896-6273(19)30428-3.pdf)\\n- [allen institute calcium imaging](http://observatory.brain-map.org/visualcoding)\\n    - An experiment is the unique combination of one mouse, one imaging depth (e.g. 175 um from surface of cortex), and one visual area (e.g. “Anterolateral visual area” or “VISal”)\\n- predicting running, facial cues\\n    - dimensionality reduction\\n\\t\\t- enforcing bottleneck in the deep model\\n      - how else to do dim reduction?\\n- responses to 2800 images\\n- overview: http://www.scholarpedia.org/article/Encyclopedia_of_computational_neuroscience\\n- keeping up to date: https://sanjayankur31.github.io/planet-neuroscience/\\n- *lots of good data*: http://home.earthlink.net/~perlewitz/index.html\\n- connectome\\n\\n  - fly brain: http://temca2data.org/\\n- *models*\\n  - senseLab: https://senselab.med.yale.edu/\\n    - modelDB - has NEURON code\\n  - model databases: http://www.cnsorg.org/model-database \\n  - comp neuro databases: http://home.earthlink.net/~perlewitz/database.html\\n- *raw misc data*\\n  - crcns data: http://crcns.org/\\n    - visual cortex data (gallant)\\n    - hippocampus spike trains\\n  - allen brain atlas: http://www.brain-map.org/\\n    - includes calcium-imaging dataset: http://help.brain-map.org/display/observatory/Data+-+Visual+Coding\\n  - wikipedia page: https://en.wikipedia.org/wiki/List_of_neuroscience_databases\\n- *human fMRI datasets*: https://docs.google.com/document/d/1bRqfcJOV7U4f-aa3h8yPBjYQoLXYLLgeY6_af_N2CTM/edit\\n- Kay et al 2008 has data on responses to images\\n- *calcium imaging* for spike sorting: http://spikefinder.codeneuro.org/\\n\\n  - spikes: http://www2.le.ac.uk/departments/engineering/research/bioengineering/neuroengineering-lab/software\\n\\n\\n\\n\\n<script type=\"text/bibliography\">\\n@article{hubel1962receptive,\\n  title={Receptive fields, binocular interaction and functional architecture in the cat\\'s visual cortex},\\n  author={Hubel, David H and Wiesel, Torsten N},\\n  journal={The Journal of physiology},\\n  volume={160},\\n  number={1},\\n  pages={106--154},\\n  year={1962},\\n  publisher={Wiley Online Library},\\n  url={http://onlinelibrary.wiley.com/wol1/doi/10.1113/jphysiol.1962.sp006837/abstract}\\n}\\n\\n@article{singh2017consensus,\\n  title={A consensus layer V pyramidal neuron can sustain interpulse-interval coding},\\n  author={Singh, Chandan and Levy, William B},\\n  journal={PloS one},\\n  volume={12},\\n  number={7},\\n  pages={e0180839},\\n  year={2017},\\n  publisher={Public Library of Science},\\n  url={http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180839}\\n}\\n\\n@article{herz2006modeling,\\n  title={Modeling single-neuron dynamics and computations: a balance of detail and abstraction},\\n  author={Herz, Andreas VM and Gollisch, Tim and Machens, Christian K and Jaeger, Dieter},\\n  journal={science},\\n  volume={314},\\n  number={5796},\\n  pages={80--85},\\n  year={2006},\\n  publisher={American Association for the Advancement of Science},\\n  url={http://science.sciencemag.org/content/314/5796/80.long}\\n}\\n\\n@article{carandini2004amplification,\\n  title={Amplification of trial-to-trial response variability by neurons in visual cortex},\\n  author={Carandini, Matteo},\\n  journal={PLoS biology},\\n  volume={2},\\n  number={9},\\n  pages={e264},\\n  year={2004},\\n  publisher={Public Library of Science},\\n  url={http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0020264}\\n}\\n\\n@article{yamins2014performance,\\n  title={Performance-optimized hierarchical models predict neural responses in higher visual cortex},\\n  author={Yamins, Daniel LK and Hong, Ha and Cadieu, Charles F and Solomon, Ethan A and Seibert, Darren and DiCarlo, James J},\\n  journal={Proceedings of the National Academy of Sciences},\\n  volume={111},\\n  number={23},\\n  pages={8619--8624},\\n  year={2014},\\n  publisher={National Acad Sciences},\\n  url={http://www.pnas.org/content/111/23/8619}\\n}\\n\\n@incollection{fukushima1982neocognitron,\\n  title={Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition},\\n  author={Fukushima, Kunihiko and Miyake, Sei},\\n  booktitle={Competition and cooperation in neural nets},\\n  pages={267--285},\\n  year={1982},\\n  publisher={Springer}\\n}\\n\\n@article{marr1976understanding,\\n  title={From understanding computation to understanding neural circuitry},\\n  author={Marr, David and Poggio, Tomaso},\\n  year={1976},\\n  url={https://dspace.mit.edu/handle/1721.1/5782}\\n}\\n\\n@article{schuman2017survey,\\n  title={A survey of neuromorphic computing and neural networks in hardware},\\n  author={Schuman, Catherine D and Potok, Thomas E and Patton, Robert M and Birdwell, J Douglas and Dean, Mark E and Rose, Garrett S and Plank, James S},\\n  journal={arXiv preprint arXiv:1705.06963},\\n  year={2017},\\n  url={https://arxiv.org/abs/1705.06963}\\n}\\n</script>',\n",
       " \"---\\nlayout: notes\\ntitle: uncertainty\\ncategory: research\\n---\\n\\n**some notes on uncertainty in machine learning, particularly deep learning**\\n\\n#  uncertainty\\n\\n## basics\\n\\n- **calibration** - predicted probabilities should match real probabilities\\n  - platt scaling - given trained classifier and new calibration dataset, basically just fit a logistic regression from the classifier predictions -> labels\\n  - isotonic regression - nonparametric, requires more data than platt scaling\\n    - piecewise-constant non-decreasing function instead of logistic regression\\n- ensemble uncertainty\\n  - [DNN ensemble uncertainty works](http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles) - predict mean and variance w/ each network then ensemble (don't need to do bagging, random init is enough)\\n  - can also use ensemble of [snapshots during training](https://arxiv.org/abs/1704.00109) (huang et al. 2017)\\n  - alternatively [batch ensemble](https://arxiv.org/pdf/2002.06715.pdf) (wen et al. 2020) - have several rank-1 keys that index different weights hidden within one neural net\\n- neural network basic uncertainty: predicted probability = confidence, max margin, entropy of predicted probabilities across classes\\n- [Single-Model Uncertainties for Deep Learning](https://arxiv.org/abs/1811.00908) (tagovska & lopez-paz 2019) - use simultaneous quantile regression\\n- quantile regression - use quantile loss to penalize models differently + get confidence intervals\\n  - [can easily do this with sklearn](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html)\\n  - quantile loss = $\\\\begin{cases} \\\\alpha \\\\cdot \\\\Delta & \\\\text{if} \\\\quad \\\\Delta > 0\\\\\\\\\\\\\\\\(\\\\alpha - 1) \\\\cdot \\\\Delta & \\\\text{if} \\\\quad \\\\Delta < 0\\\\end{cases}$\\n    - $\\\\Delta =$ actual - predicted\\n    - ![Screen Shot 2019-06-26 at 10.06.11 AM](../assets/quantile_losses.png)\\n\\n## complementarity\\n\\n### rejection learning\\n\\n- **rejection learning** - allow models to *reject* (not make a prediction) when they are not confidently accurate ([chow 1957](https://ieeexplore.ieee.org/abstract/document/5222035/?casa_token=UiIdn8AjFjYAAAAA:XvnZPA7rJlvwxD-bIh2dNG4SPfnHtDYWcBUmAFYRxD6Xk8QE5osnKLs8tAlib_doL8OxqYjMLDE), [cortes et al. 2016](https://link.springer.com/chapter/10.1007/978-3-319-46379-7_5))\\n- [To Trust Or Not To Trust A Classifier](http://papers.nips.cc/paper/7798-to-trust-or-not-to-trust-a-classifier.pdf) (jiang, kim et al 2018) - find a trusted region of points based on nearest neighbor density (in some embedding space)\\n    - trust score uses density over some set of nearest neighbors\\n    - do clustering for each class - trust score = distance to once class's cluster vs the other classes\\n\\n### complementarity\\n\\n- **complementarity** - ML should focus on points hard for humans + seek human input on points hard for ML\\n    - note: goal of perception isn't to learn categories but learn things that are associated with actions\\n- [Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer](http://papers.nips.cc/paper/7853-predict-responsibly-improving-fairness-and-accuracy-by-learning-to-defer) (madras et al. 2018) - adaptive rejection learning - build on rejection learning considering the strengths/weaknesses of humans\\n- [Learning to Complement Humans](https://arxiv.org/abs/2005.00582) (wilder et al. 2020) - 2 approaches for how to incorporate human input:\\n    - discriminative approach - jointly train predictive model and policy for deferring to human (witha cost for deferring)\\n    - decision-theroetic approach - train predictive model + policy jointly based on value of information (VOI)\\n    - do real-world experiments w/ humans to validate:  scientific discovery (a galaxy classification task) and medical diagnosis (detection of breast cancer metastasis)\\n- [Gaining Free or Low-Cost Transparency with Interpretable Partial Substitute](https://arxiv.org/pdf/1802.04346.pdf) (wang, 2019) - given a black-box model, find a subset of the data for which predictions can be made using a simple rule-list ([tong wang](https://scholar.google.com/citations?hl=en&user=KB6A0esAAAAJ&view_op=list_works&sortby=pubdate) has a few papers like this)\\n    - [Interpretable Companions for Black-Box Models](https://arxiv.org/abs/2002.03494) (pan, wang, et al. 2020) - offer an interpretable, but slightly less acurate model for each decision\\n      - human experiment evaluates how much humans are able to tolerate\\n\\n\\n## nearest-neighbor methods\\n\\n- [Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning](https://arxiv.org/pdf/1803.04765.pdf) (papernot & mcdaniel, 2018)\\n- [distance-based confidence scores](https://arxiv.org/pdf/1709.09844.pdf) (mandelbaum et al. 2017) - use either distance in embedding space or adversarial training to get uncertainties for DNNs\\n- [deep kernel knn](https://arxiv.org/pdf/1811.02579.pdf) (card et al. 2019) - predict labels based on weighted sum of training instances, where weights are given by distance in embedding space\\n    - add an uncertainty based on conformal methods\\n- **local outlier factor** (breunig et al. 2000) - score based on nearest neighbor density\\n- idea: gradients should be larger if you are on the image manifold\\n\\n## outlier-detection\\n\\n- overview from [sklearn](https://scikit-learn.org/stable/modules/outlier_detection.html)\\n- **elliptic envelope** - assume data is Gaussian and fit elliptic envelop (maybe robustly) to tell when data is an outlier\\n- [isolation forest](https://ieeexplore.ieee.org/abstract/document/4781136) (liu et al. 2008) - lower average number of random splits required to isolate a sample means more outlier\\n- [one-class svm](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM) - estimates the support of a high-dimensional distribution using a kernel (2 approaches:)\\n  - separate the data from the origin (with max margin between origin and points) (scholkopf et al. 2000)\\n  - find a sphere boundary around a dataset with the volume of the sphere minimized ([tax & duin 2004](https://link.springer.com/article/10.1023/B:MACH.0000008084.60811.49))\\n- [detachment index](https://escholarship.org/uc/item/9d34m0wz) (kuenzel 2019) - based on random forest\\n  - for covariate $j$, detachment index $d^j(x) = \\\\sum_i^n w (x, X_i) \\\\vert X_i^j - x^j \\\\vert$\\n    - $w(x, X_i) = \\\\underbrace{1 / T\\\\sum_{t=1}^{T}}_{\\\\text{average over T trees}} \\\\frac{\\\\overbrace{1\\\\{ X_i \\\\in L_t(x) \\\\}}^{\\\\text{is }   X_i \\\\text{ in the same leaf?}}}{\\\\underbrace{\\\\vert L_t(x) \\\\vert}_{\\\\text{num points in leaf}}}$ is $X_i$ relevant to the point $x$?\\n\\n## predicting uncertainty for DNNs\\n\\n- [Inhibited Softmax for Uncertainty Estimation in Neural Networks](https://arxiv.org/abs/1810.01861) (mozejko et al. 2019) - directly predict uncertainty by adding an extra output during training\\n- [Learning Confidence for Out-of-Distribution Detection in Neural Networks](https://arxiv.org/pdf/1802.04865.pdf) (devries et al. 2018) - predict both prediction *p* and confidence *c*\\n  - during training, learn using $p' = c \\\\cdot p + (1 - c) \\\\cdot y$\\n- [Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers](https://arxiv.org/abs/1805.08206) (geifmen et al. 2019)\\n    - just predicting uncertainty is biased\\n    - estimate uncertainty of highly confident points using earlier snapshots of the trained model\\n- [Contextual Outlier Interpretation](https://arxiv.org/abs/1711.10589) (liu et al. 2018) - describe outliers with 3 things: outlierness score, attributes that contribute to the abnormality, and contextual description of its neighborhoods\\n- [Energy-based Out-of-distribution Detection](https://arxiv.org/abs/2010.03759)\\n- [Test-Time Training with Self-Supervision for Generalization under Distribution Shifts](https://proceedings.icml.cc/paper/2020/file/1d3b7f1f8a7625f8d5e700dcf0d9ae68-Paper.pdf) (sun et al. 2020)\\n\\n## bayesian approaches\\n\\n- **epistemic uncertainty** - uncertainty in the DNN model parameters\\n  - without good estimates of this, often get aleatoric uncertainty wrong (since $p(y\\\\vert x) = \\\\int p(y \\\\vert x, \\\\theta) p(\\\\theta \\\\vert data) d\\\\theta$\\n- **aleatoric uncertainty** -  inherent and irreducible data noise (e.g. features contradict each other)\\n  - this can usually be gotten by predicting a distr. $p(y \\\\vert x)$ instead of a point estimate\\n  - ex. logistic reg. already does this\\n  - ex. regression - just predict mean and variance of Gaussian\\n- [gaussian processes](https://distill.pub/2019/visual-exploration-gaussian-processes/)\\n- [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](http://proceedings.mlr.press/v48/gal16.pdf)  \\n  - dropout at test time gives you uncertainty\\n- [SWAG](https://papers.nips.cc/paper/9472-a-simple-baseline-for-bayesian-uncertainty-in-deep-learning.pdf) (maddox et al. 2019) - start with pre-trained net then get Gaussian distr. over weights by training with large constant setp-size\\n\\n### bayesian neural networks\\n\\n- [blog posts on basics](https://medium.com/neuralspace/probabilistic-deep-learning-bayes-by-backprop-c4a3de0d9743)\\n  - want $p(\\\\theta|x) = \\\\frac {p(x|\\\\theta) p(\\\\theta)}{p(x)}$\\n    - $p(x)$ is hard to compute\\n- [slides on basics](https://wjmaddox.github.io/assets/BNN_tutorial_CILVR.pdf)\\n- [Bayes by backprop (blundell et al. 2015)](https://arxiv.org/abs/1505.05424) - efficient way to train BNNs using backprop\\n\\t- Instead of training a single network, trains an ensemble of networks, where each network has its weights drawn from a shared, learned probability distribution. Unlike other ensemble methods, the method typically only doubles the number of parameters yet trains an infinite ensemble using unbiased Monte Carlo estimates of the gradients.\\n- [Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision](https://arxiv.org/pdf/1906.01620.pdf)\\n- [icu bayesian dnns](https://aiforsocialgood.github.io/icml2019/accepted/track1/pdfs/38_aisg_icml2019.pdf)\\n  - focuses on epistemic uncertainty\\n  - could use one model to get uncertainty and other model to predict\",\n",
       " '---\\nlayout: notes\\ntitle: interpretability\\ncategory: research\\n---\\n\\n**some interesting papers on interpretable machine learning, largely organized based on this [interpretable ml review](https://arxiv.org/abs/1901.04592) (murdoch et al. 2019) and notes from this [interpretable ml book](https://christophm.github.io/interpretable-ml-book/) (molnar 2019)**\\n\\n#  interpretability\\n\\n## reviews\\n\\n### definitions\\nThe definition of interpretability I find most useful is that given in [murdoch et al. 2019](https://arxiv.org/abs/1901.04592): basically that interpretability requires a pragmatic approach in order to be useful. As such, interpretability is only defined with respect to a specific audience + problem and an interpretation should be evaluated in terms of how well it benefits a specific context. It has been defined and studied more broadly in a variety of works:\\n\\n- [Explore, Explain and Examine Predictive Models](https://pbiecek.github.io/ema/) (biecek & burzykowski, in progress) - another book on exploratory analysis with interpretability\\n- [Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges](https://arxiv.org/abs/1803.07517) (ras et al. 2018)\\n- [Explainable Deep Learning: A Field Guide for the Uninitiated](https://arxiv.org/pdf/2004.14545.pdf)\\n- [Interpretable Deep Learning in Drug Discovery](https://arxiv.org/abs/1903.02788)\\n- [Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges](https://link.springer.com/chapter/10.1007/978-3-030-32236-6_51)\\n- [Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI](https://arxiv.org/abs/1910.10045)\\n\\n\\n\\n### overviews\\n\\n- [Towards a Generic Framework for Black-box Explanation Methods](https://hal.inria.fr/hal-02131174v2/document) (henin & metayer 2019)\\n  - sampling - selection of inputs to submit to the system to be explained\\n  - generation - analysis of links between selected inputs and corresponding outputs to generate explanations\\n    1. *proxy* - approximates model (ex. rule list, linear model)\\n    2. *explanation generation* - explains the proxy (ex. just give most important 2 features in rule list proxy, ex. LIME gives coefficients of linear model, Shap: sums of elements)\\n  - interaction (with the user)\\n  - this is a super useful way to think about explanations (especially local), but doesn\\'t work for SHAP / CD which are more about how much a variable contributes rather than a local approximation\\n\\n<img class=\"medium_image\" src=\"../assets/black_box_explainers.png\"/>\\n\\n<img class=\"medium_image\" src=\"../assets/black_box_explainers_legend.png\"/>\\n\\n<img class=\"medium_image\" src=\"../assets/explainers_table.png\"/>\\n\\n- [feature (variable) importance measurement review (VIM)](https://www.sciencedirect.com/science/article/pii/S0951832015001672) (wei et al. 2015)\\n  - often-termed sensitivity, contribution, or impact\\n  - some of these can be applied to data directly w/out model (e.g. correlation coefficient, rank correlation coefficient, moment-independent VIMs)\\n  - <img class=\"medium_image\" src=\"../assets/vims.png\"/>\\n- [Pitfalls to Avoid when Interpreting Machine Learning Models](https://arxiv.org/pdf/2007.04131.pdf) (molnar et al. 2020)\\n\\n## evaluating interpretability\\n\\nEvaluating interpretability can be very difficult (largely because it rarely makes sense to talk about interpretability outside of a specific context). The best possible evaluation of interpretability requires benchmarking it with respect to the relevant audience in a context. For example, if an interpretation claims to help understand radiology models, it should be tested based on how well it helps radiologists when actually making diagnoses. The papers here try to find more generic alternative ways to evaluate interp methods (or just define desiderata to do so).\\n\\n- [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/pdf/1702.08608.pdf) (doshi-velez & kim 2017)\\n  - ![Screen Shot 2020-08-03 at 10.58.13 PM](../assets/interp_eval_table.png)\\n- [Benchmarking Attribution Methods with Relative Feature Importance](https://arxiv.org/abs/1907.09701) (yang & kim 2019)\\n  - train a classifier, add random stuff (like dogs) to the image, classifier should assign them little importance\\n- [Visualizing the Impact of Feature Attribution Baselines](https://distill.pub/2020/attribution-baselines/)\\n  - **top-k-ablation**: should identify top pixels, ablate them, and want it to actually decrease\\n  - **center-of-mass ablation**: also could identify center of mass of saliency map and blur a box around it (to avoid destroying feature correlations in the model)\\n  - should we be **true-to-the-model** or **true-to-the-data**?\\n- [Evaluating Feature Importance Estimates](https://arxiv.org/abs/1806.10758) (hooker et al. 2019)\\n  - **remove-and-retrain test accuracy decrease**\\n- [Quantifying Interpretability of Arbitrary Machine Learning Models Through Functional Decomposition](https://arxiv.org/pdf/1904.03867.pdf) (molnar 2019)\\n- [An Evaluation of the Human-Interpretability of Explanation](https://arxiv.org/pdf/1902.00006.pdf) (lage et al. 2019)\\n- [How do Humans Understand Explanations from Machine Learning Systems?: An Evaluation of the Human-Interpretability of Explanation](https://arxiv.org/pdf/1802.00682.pdf) (narayanan et al. 2018)\\n- [On the (In)fidelity and Sensitivity for Explanations](https://arxiv.org/abs/1901.09392)\\n- [Benchmarking Attribution Methods with Relative Feature Importance](https://arxiv.org/abs/1907.09701) (yang & kim 2019)\\n- [Do Explanations Reflect Decisions? A Machine-centric Strategy to Quantify the Performance of Explainability Algorithms](https://arxiv.org/abs/1910.07387)\\n- [On Validating, Repairing and Refining Heuristic ML Explanations](https://arxiv.org/abs/1907.02509)\\n- [How Much Can We See? A Note on Quantifying Explainability of Machine Learning Models](https://arxiv.org/abs/1910.13376)\\n- [Manipulating and Measuring Model Interpretability](https://arxiv.org/abs/1802.07810) (sangdeh et al. ... wallach 2019)\\n  - participants who were shown a clear model with a small number of features were better able to simulate the model’s predictions\\n  - no improvements in the degree to which participants followed the model’s predictions when it was beneficial to do so.\\n  - increased transparency hampered people’s ability to detect when the model makes a sizable mistake and correct for it, seemingly due to information overload\\n- [Towards a Framework for Validating Machine Learning Results in Medical Imaging](https://dl.acm.org/citation.cfm?id=3332193)\\n- [An Integrative 3C evaluation framework for Explainable Artificial Intelligence](https://aisel.aisnet.org/amcis2019/ai_semantic_for_intelligent_info_systems/ai_semantic_for_intelligent_info_systems/10/)\\n- [Evaluating Explanation Without Ground Truth in Interpretable Machine Learning](https://arxiv.org/pdf/1907.06831.pdf) (yang et al. 2019)\\n  - predictability (does the knowledge in the explanation generalize well)\\n  - fidelity (does explanation reflect the target system well)\\n  - persuasibility (does human satisfy or comprehend explanation well)\\n\\n### criticisms\\n\\n- [Sanity Checks for Saliency Maps](https://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf) (adebayo et al. 2018)\\n  - **Model Parameter Randomization Test** - attributions should be different for trained vs random model, but they aren\\'t for many attribution methods\\n- [Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging](https://www.medrxiv.org/content/10.1101/2020.07.28.20163899v1.full.pdf) (arun et al. 2020) - CXR images from SIIM-ACR Pneumothorax Segmentation + RSNA Pneumonia Detection\\n  - metrics: localizers (do they overlap with GT segs/bounding boxes), variation with model weight randomization, repeatable (i.e. same after retraining?), reproducibility (i.e. same after training different model?)\\n- [Interpretable Deep Learning under Fire](https://arxiv.org/abs/1812.00891) (zhang et al. 2019)\\n- [How can we fool LIME and SHAP? Adversarial Attacks on Post hoc Explanation Methods](https://arxiv.org/abs/1911.02508)\\n  - we can build classifiers which use important features (such as race) but explanations will not reflect that\\n  - basically classifier is different on X which is OOD (and used by LIME and SHAP)\\n- [Saliency Methods for Explaining Adversarial Attacks](https://arxiv.org/abs/1908.08413)\\n- [Fooling Neural Network Interpretations via Adversarial Model Manipulation](https://arxiv.org/abs/1902.02041) (heo, joo, & moon 2019) - can change model weights so that it keeps predictive accuracy but changes its interpretation\\n  - motivation: could falsely look like a model is \"fair\" because it places little saliency on sensitive attributes\\n    - output of model can still be checked regardless\\n  - fooled interpretation generalizes to entire validation set\\n  - can force the new saliency to be whatever we like\\n    - passive fooling - highlighting uninformative pixels of the image\\n    - active fooling - highlighting a completely different object, the firetruck\\n  - **model does not actually change that much** - predictions when manipulating pixels in order of saliency remains similar, very different from random (fig 4)\\n\\n## intrinsic interpretability (i.e. how can we fit simpler models)\\n\\nFor an implementation of many of these models, see the python [imodels package](https://github.com/csinva/imodels). \\n\\n### decision rules\\n\\nFor more on rules, see **[logic notes](https://csinva.io/notes/ai/logic.html)**.\\n\\n- 2 basic concepts for a rule\\n  - converage = support\\n  - accuracy = confidence = consistency\\n    - measures for rules: precision, info gain, correlation, m-estimate, Laplace estimate\\n- these algorithms usually don\\'t support regression, but you can get regression by cutting the outcome into intervals\\n\\n#### rule sets\\n\\n**Rule sets commonly look like a series of independent if-then rules. Unlike trees / lists, these rules can be overlapping and might not cover the whole space. Final predictions can be made via majority vote, using most accurate rule, or averaging predictions.**\\n\\n- popular way to learn rule sets\\n  - [A Simple, Fast, and Effective Rule Learner](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.1184&rep=rep1&type=pdf) (cohen, & singer, 1999) - SLIPPER - repeatedly boosting a simple, greedy rule-builder\\n  - [Lightweight Rule Induction](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.4619) (weiss & indurkhya, 2000) - specify number + size of rules and classify via majority vote\\n  - [Maximum Likelihood Rule Ensembles](https://dl.acm.org/doi/pdf/10.1145/1390156.1390185?casa_token=Lj3Ypp6bLzoAAAAA:t4p9YRPHEXJEL723ygEW5BJ9qft8EeU5934vPJFf1GrF1GWm1kctIePQGeaRiKHJa6ybpqtTqGg1Ig) (Dembczyński et al. 2008) - MLRules - rule is base estimator in ensemble - build by greedily maximizing log-likelihood\\n- more recent global versions of learning rule sets\\n  - [interpretable decision set](https://dl.acm.org/citation.cfm?id=2939874) (lakkaraju et al. 2016) - set of if then rules\\n    - short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes\\n  - [A Bayesian Framework for Learning Rule Sets for Interpretable Classification](http://www.jmlr.org/papers/volume18/16-003/16-003.pdf) (wang et al. 2017) - rules are a bunch of clauses OR\\'d together (e.g. if (X1>0 AND X2<1) OR (X2<1 AND X3>1) OR ... then Y=1)\\n- when learning sequentially, often useful to prune at each step (Furnkranz, 1997)\\n\\n#### rule lists\\n\\n- oneR algorithm - select feature that carries most information about the outcome and then split multiple times on that feature\\n- **sequential covering** - keep trying to cover more points sequentially\\n- pre-mining frequent patterns (want them to apply to a large amount of data and not have too many conditions)\\n  - FP-Growth algorithm (borgelt 2005) is fast\\n  - Aprior + Eclat do the same thing, but with different speeds\\n- [interpretable classifiers using rules and bayesian analysis](https://projecteuclid.org/download/pdfview_1/euclid.aoas/1446488742) (letham et al. 2015)\\n  - start by pre-mining frequent patterns rules\\n    - current approach does not allow for negation (e.g. not diabetes) and must split continuous variables into categorical somehow (e.g. quartiles)\\n    - mines things that frequently occur together, but doesn\\'t look at outcomes in this step - okay (since this is all about finding rules with high support)\\n  - learn rules w/ prior for short rule conditions and short lists\\n    - start w/ random list \\n    - sample new lists by adding/removing/moving a rule\\n    - at the end, return the list that had the highest probability\\n  - [scalable bayesian rule lists](https://dl.acm.org/citation.cfm?id=3306086) (yang et al. 2017) - faster algorithm for computing\\n- [learning certifiably optimal rules lists](https://dl.acm.org/citation.cfm?id=3098047) (angelino et al. 2017) - optimization for categorical feature space\\n  - can get upper / lower bounds for loss = risk + $\\\\lambda$ * listLength\\n- [Expert-augmented machine learning](https://arxiv.org/abs/1903.09731) (gennatas et al. 2019)\\n  - make rule lists, then compare the outcomes for each rule with what clinicians think should be outcome for each rule\\n  - look at rules with biggest disagreement and engineer/improve rules or penalize unreliable rules\\n\\n#### trees\\n\\nTrees suffer from the fact that they have to cover the entire decision space and often we end up with replicated subtrees.\\n\\n- [optimal classification trees methodology paper](https://link.springer.com/content/pdf/10.1007%2Fs10994-017-5633-9.pdf) (bertsimas & dunn 2017) - globally optimal decision tree with expensive optimization\\n  - [optimal classification trees vs PECARN](https://jamanetwork.com/journals/jamapediatrics/article-abstract/2733157) (bertsimas et al. 2019)\\n  - [supplemental tables](https://cdn-jamanetwork-com.libproxy.berkeley.edu/ama/content_public/journal/peds/0/poi190021supp1_prod.pdf?Expires=2147483647&Signature=EnVKjyPUrh7o2GVSU7Bxr4ZYL~5T27-sPKh14TANiL5mpXfj3YPTnUetEBPc~njVrg2VKY5TqqXCFxtR4xr6DfGLgobA~Kl92A1Jubmj9XgSL3U3so1~4O~YKob1WcS5uFI3HBpq9J-o-IkAsRq1qsnTFFzlvH7zlkwO9TW-dxnU9vtvU-QzhPNJ0cdAX-c7rrnZV0p0Fg~gzaEz5lvPP30Nort4kDTxd-FNDW5OYJFqusWF9e~3QK2S6Y4nRjv~IavQ10fQ24fSvEK5Nd1qetME8j2one0LA~KZjOk7avp76aV5os9msn-2hdPcEM7YWtLTUq12a9oVaD6pXKe3ZA__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)\\n- [Building more accurate decision trees with the additive tree](https://www.pnas.org/content/116/40/19887) (luna et al. 2019)\\n  - present additive tree (AddTree), which builds a single decision tree, which is between a single CART tree and boosted decision stumps\\n  - cart can be seen as a boosting algorithm on stumps\\n    - can rewrite boosted stumps as a tree very easily\\n    - previous work: can grow tree based on Adaboost idea = AdaTree\\n  - ![Screen Shot 2020-03-11 at 11.10.13 PM](../assets/additive_trees.png)\\n- [optimal sparse decision trees](https://arxiv.org/abs/1904.12847) (hu et al. 2019) - optimal decision trees for binary variables\\n- extremely randomized trees - randomness goes further, not only feature is selected randomly but also split has some randomness\\n- issues: replicated subtree problem (Pagallo & Haussler, 1990)\\n- [Bayesian Treed Models](http://www-stat.wharton.upenn.edu/~edgeorge/Research_papers/treed-models.pdf) (chipman et al. 2001) - impose priors on tree parameters\\n  - tree structure e.g. depth, splitting criteria\\n  - values in terminal nodes coditioned on tree structure\\n  - residual noise\\'s standard deviation\\n\\n\\n\\n### linear (+algebraic) models\\n\\n#### supersparse models\\n\\n- [Supersparse linear integer models for optimized medical scoring systems](https://link.springer.com/content/pdf/10.1007/s10994-015-5528-6.pdf) (ustun & rudin 2016)\\n  - [2helps2b paper](https://www.ncbi.nlm.nih.gov/pubmed/29052706)\\n  - ![Screen Shot 2019-06-11 at 11.17.35 AM](../assets/2helps2b.png)\\n\\n#### gams (generalized additive models)\\n\\n- gam takes form $g(\\\\mu) = b + f(x_0) + f(x_1) + f(x_2) + ...$\\n  - usually assume some basis for the $f$, like splines or polynomials (and we select how many either manually or with some complexity penalty)\\n- [Demystifying Black-box Models with Symbolic Metamodels](https://papers.nips.cc/paper/9308-demystifying-black-box-models-with-symbolic-metamodels.pdf)\\n  - GAM parameterized with Meijer G-functions (rather than pre-specifying some forms, as is done with symbolic regression)\\n- [Neural Additive Models: Interpretable Machine Learning with Neural Nets](https://arxiv.org/abs/2004.13912) - GAM where we learn $f$ with a neural net\\n\\n#### symbolic regression\\n\\n- learn form of the equation using priors on what kinds of thinngs are more difficult\\n- [Building and Evaluating Interpretable Models using Symbolic Regression and Generalized Additive Models](https://openreview.net/pdf?id=BkgyvQzmW)\\n  - gams - assume model form is additive combination of some funcs, then solve via GD\\n  - however, if we don\\'t know the form of the model we must generate it\\n- [Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Black Box Simulators](https://arxiv.org/abs/2002.01080)\\n\\n### example-based (e.g. prototypes)\\n\\n- [prototypes II](https://arxiv.org/abs/1806.10574) (chen et al. 2018)\\n  - can have prototypes smaller than original input size\\n  - l2 distance\\n  - require the filters to be identical to the latent representation of some training image patch\\n  - cluster image patches of a particular class around the prototypes of the same class, while separating image patches of different classes\\n  - maxpool class prototypes so spatial size doesn\\'t matter\\n  - also get heatmap of where prototype was activated (only max really matters)\\n  - train in 3 steps\\n    - train everything: classification + clustering around intraclass prototypes + separation between interclass prototypes (last layer fixed to 1s / -0.5s)\\n    - project prototypes to data patches\\n    - learn last layer\\n- [prototypes I](https://arxiv.org/pdf/1710.04806.pdf) (li et al. 2017)\\n  - uses encoder/decoder setup\\n  - encourage every prototype to be similar to at least one encoded input\\n  - learned prototypes in fact look like digits\\n  - correct class prototypes go to correct classes\\n  - loss: classification + reconstruction + distance to a training point\\n\\n### bayesian models\\n\\n- e.g. naive bayes\\n\\n### interpretable neural nets\\n\\n- [Symbolic Semantic Segmentation and Interpretation of COVID-19 Lung Infections in Chest CT volumes based on Emergent Languages](https://arxiv.org/pdf/2008.09866v1.pdf) (chowdhury et al. 2020) - combine some segmentation with the classifier\\n- [Concept Bottleneck Models](https://arxiv.org/pdf/2007.04612.pdf) (koh et al. 2020) - predict concepts before making final prediction\\n- [Concept Whitening for Interpretable Image Recognition](https://arxiv.org/pdf/2002.01650.pdf) (chen et al. 2020) - force network to separate \"concepts\" (like in TCAV) along different axes\\n- [Towards Explainable Deep Neural Networks (xDNN)](https://arxiv.org/abs/1912.02523) (angelov & soares 2019) - more complex version of using prototypes\\n- [MonoNet: Towards Interpretable Models by Learning Monotonic Features](https://arxiv.org/abs/1909.13611) - enforce output to be a monotonic function of individuaul features\\n- [Interpretability Beyond Classification Output: Semantic Bottleneck Networks](https://arxiv.org/abs/1907.10882) - add an interpretable intermediate bottleneck representation\\n- [Improved Deep Fuzzy Clustering for Accurate and Interpretable Classifiers](https://ieeexplore.ieee.org/abstract/document/8858809) - extract features with a DNN then do fuzzy clustering on this\\n- [Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet](https://arxiv.org/abs/1904.00760)\\n  - CNN is restricted to look at very local features only and still does well (and produces an inbuilt saliency measure)\\n  - [learn shapes not texture](https://openreview.net/pdf?id=Bygh9j09KX)\\n  - [code](https://github.com/wielandbrendel/bag-of-local-features-models)\\n- [Towards Robust Interpretability with Self-Explaining Neural Networks](https://arxiv.org/pdf/1806.07538.pdf) (alvarez-melis & jaakkola 2018) - building architectures that explain their predictions\\n- [Harnessing Deep Neural Networks with Logic Rules](https://arxiv.org/pdf/1603.06318.pdf)\\n- [iCaps: An Interpretable Classifier via Disentangled Capsule Networks](https://arxiv.org/abs/2008.08756) (jung et al. 2020)\\n  - the class capsule also includes classification-irrelevant information\\n    - uses a novel class-supervised disentanglement algorithm\\n  - entities represented by the class capsule overlap\\n    - adds additional regularizer\\n- [WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation](https://openaccess.thecvf.com/content_cvpr_2017/html/Durand_WILDCAT_Weakly_Supervised_CVPR_2017_paper.html) (durand et al. 2017) - constrains architecture\\n  - after extracting conv features, replace linear layers with special pooling layers, which helps with spatial localization\\n    - each class gets a pooling map\\n    - prediction for a class is based on top-k spatial regions for a class\\n    - finally, can combine the predictions for each class\\n\\n\\n### misc models\\n\\n- learning AND-OR Templates for Object Recognition and Detection (zhu_13)\\n- ross et al. - constraing model during training\\n- scat transform idea (mallat_16 rvw, oyallan_17)\\n- force interpretable description by piping through something interpretable (ex. tenenbaum scene de-rendering)\\n- learn concepts through probabilistic program induction\\n- force biphysically plausible learning rules\\n- [The Convolutional Tsetlin Machine](https://arxiv.org/pdf/1905.09688.pdf) - uses easy-to-interpret conjunctive clauses\\n  - [The Tsetlin Machine](https://arxiv.org/pdf/1804.01508.pdf)\\n- [Making Bayesian Predictive Models Interpretable: A Decision Theoretic Approach](https://arxiv.org/abs/1910.09358)\\n- [Case-Based Reasoning for Assisting Domain Experts in Processing Fraud Alerts of Black-Box Machine Learning Models](https://arxiv.org/abs/1907.03334)\\n- [Beyond Sparsity: Tree Regularization of Deep Models for Interpretability](https://arxiv.org/pdf/1711.06178.pdf)\\n  - regularize so that deep model can be closely modeled by tree w/ few nodes\\n- [Tensor networks](https://www.perimeterinstitute.ca/research/research-initiatives/tensor-networks-initiative) - like DNN that only takes boolean inputs and deals with interactions explicitly\\n  - widely used in physics\\n\\n#### programs\\n\\n- **program synthesis** - automatically find a program in an underlying programming language that satisfies some user intent\\n  - **ex. program induction** - given a dataset consisting of input/output pairs, generate a (simple?) program that produces the same pairs\\n- [probabilistic programming](https://en.wikipedia.org/wiki/Probabilistic_programming) - specify graphical models via a programming language\\n\\n## posthoc interpretability (i.e. how can we interpret a fitted model)\\n\\n### model-agnostic\\n\\n- local surrogate ([LIME](https://arxiv.org/abs/1602.04938)) - fit a simple model locally to on point and interpret that\\n   - select data perturbations and get new predictions\\n     - for tabular data, this is just varying the values around the prediction\\n     - for images, this is turning superpixels on/off\\n     - superpixels determined in unsupervised way\\n   - weight the new samples based on their proximity\\n   - train a kernel-weighted, interpretable model on these points\\n   - LEMNA - like lime but uses lasso + small changes\\n- [anchors](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982/15850) (ribeiro et al. 2018) - find biggest square region of input space that contains input and preserves same output (with high precision)\\n   1. does this search via iterative rules\\n- [What made you do this? Understanding black-box decisions with sufficient input subsets](https://arxiv.org/pdf/1810.03805.pdf)\\n   - want to find smallest subsets of features which can produce the prediction\\n     - other features are masked or imputed\\n- [VIN](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.7500&rep=rep1&type=pdf) (hooker 04) - variable interaction networks - globel explanation based on detecting additive structure in a black-box, based on ANOVA\\n- [local-gradient](http://www.jmlr.org/papers/v11/baehrens10a.html) (bahrens et al. 2010) - direction of highest slope towards a particular class / other class\\n- [golden eye](https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10618-014-0368-8&casa_token=AhKnW6Xx4L0AAAAA:-SEMsMjDX3_rU5gyGx6plcmF5A_ufXvsWJHzjCUIGWHGW0fqOe50yhWKYOK6UIPDHQaUwEkE3RK17XOByzo) (henelius et al. 2014) - randomize different groups of features and search for groups which interact\\n- **[shapley value](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predicti)** - average marginal contribution of a feature value across all possible sets of feature values\\n  - \"how much does prediction change on average when this feature is added?\"\\n  - tells us the difference between the actual prediction and the average prediction\\n  - estimating: all possible sets of feature values have to be evaluated with and without the j-th feature\\n    - this includes sets of different sizes\\n    - to evaluate, take expectation over all the other variables, fixing this variables value\\n  - shapley sampling value - sample instead of exactly computing\\n    - quantitative input influence is similar to this...\\n  - satisfies 3 properties\\n      - local accuracy - basically, explanation scores sum to original prediction\\n      - missingness - features with $x\\'_i=0$ have 0 impact\\n      - consistency - if a model changes so that some simplified input’s contribution increases or stays the same regardless of the other inputs, that input’s attribution should not decrease.\\n  - interpretation: Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value\\n  - recalculate via sampling other features in expectation\\n  - followup [propagating shapley values](https://arxiv.org/pdf/1911.11888.pdf) (chen, lundberg, & lee 2019) - can work with stacks of different models\\n- [probes](https://nlp.stanford.edu/~johnhew/interpreting-probes.html) - check if a representation (e.g. BERT embeddings) learned a certain property (e.g. POS tagging) by seeing if we can predict this property (maybe linearly) directly from the representation\\n    - problem: if the post-hoc probe is a complex model (e.g. MLP), it can accurately predict a property even if that property isn\\'t really contained in the representation\\n    - potential solution: benchmark against control tasks, where we construct a new random task to predict given a representation, and see how well the post-hoc probe can do on that task\\n- [Explaining individual predictions when features are dependent: More accurate approximations to Shapley values](https://arxiv.org/abs/1903.10464) (aas et al. 2019) - tries to more accurately compute conditional expectation\\n- [Feature relevance quantification in explainable AI: A causal problem](https://arxiv.org/abs/1910.13413) (janzing et al. 2019) - argues we should just use unconditional expectation\\n- [quantitative input influence](https://ieeexplore.ieee.org/abstract/document/7546525) - similar to shap but more general\\n- permutation importance - increase in the prediction error after we permuted the feature\\'s values\\n  - $\\\\mathbb E[Y] - \\\\mathbb E[Y\\\\vert X_{\\\\sim i}]$\\n  - If features are correlated, the permutation feature importance can be biased by unrealistic data\\n  instances (PDP problem)\\n  - not the same as model variance\\n  - Adding a correlated feature can decrease the importance of the associated feature\\n- [L2X: information-theoretical local approximation](https://arxiv.org/pdf/1802.07814.pdf) (chen et al. 2018) - locally assign feature importance based on mutual information with function\\n- [Learning Explainable Models Using Attribution Priors + Expected Gradients](https://arxiv.org/abs/1906.10670) - like doing integrated gradients in many directions (e.g. by using other points in the training batch as the baseline)\\n    - can use this prior to help improve performance\\n- [Variable Importance Clouds: A Way to Explore Variable Importance for the Set of Good Models](https://arxiv.org/pdf/1901.03209.pdf) \\n- [All Models are Wrong, but Many are Useful: Learning a Variable\\'s Importance by Studying an Entire Class of Prediction Models Simultaneously](https://arxiv.org/abs/1801.01489) (Aaron, Rudin, & Dominici 2018)\\n- [Interpreting Black Box Models via Hypothesis Testing](https://arxiv.org/abs/1904.00045)\\n\\n#### feature interactions\\n\\nHow interactions are defined and summarized is a very difficult thing to specify. For example, interactions can change based on monotonic transformations of features (e.g. $y= a \\\\cdot b$, $\\\\log y = \\\\log a + \\\\log b$). Nevertheless, when one has a specific question it can make sense to pursue finding and understanding interactions.\\n\\n- build-up = context-free, less faithful: score is contribution of only variable of interest ignoring other variables\\n- break-down = occlusion = context-dependent, more faithful: score is contribution of variable of interest given all other variables (e.g. permutation test - randomize var of interest from right distr.)\\n- *H-statistic*: 0 for no interaction, 1 for complete interaction\\n  - how much of the variance of the output of the joint partial dependence is explained by the interaction instead of the individuals\\n  - $H^2_{jk} = \\\\underbrace{\\\\sum_i [\\\\overbrace{PD(x_j^{(i)}, x_k^{(i)})}^{\\\\text{interaction}} \\\\overbrace{- PD(x_j^{(i)}) - PD(x_k^{(i)})}^{\\\\text{individual}}]^2}_{\\\\text{sum over data points}} \\\\: / \\\\: \\\\underbrace{\\\\sum_i [PD(x_j^{(i)}, x_k^{(i)})}_{\\\\text{normalization}}]^2$\\n  - alternatively, using ANOVA decomp: $H_{jk}^2 = \\\\sum_i g_{ij}^2 / \\\\sum_i (\\\\mathbb E [Y \\\\vert X_i, X_j])^2$\\n  - same assumptions as PDP: features need to be independent\\n- alternatives\\n  - variable interaction networks (Hooker, 2004) - decompose pred into main effects + feature interactions\\n  - PDP-based feature interaction (greenwell et al. 2018)\\n- feature-screening (feng ruan\\'s work)\\n  - want to find beta which is positive when a variable is important\\n  - idea: maximize difference between (distances for interclass) and (distances for intraclass)\\n  - using an L1 distance yields better gradients than an L2 distance\\n- methods for finding frequent item sets\\n  - [random intersection trees](https://arxiv.org/pdf/1303.6223.pdf)\\n  - [fp-growth](https://www.softwaretestinghelp.com/fp-growth-algorithm-data-mining/)\\n  - eclat\\n\\n#### vim (variable importance measure) framework\\n\\n- VIM\\n\\t1. a quantitative indicator that quantifies the change of model output value w.r.t. the change or permutation of one or a set of input variables\\n\\t2. an indicator that quantifies the contribution of the uncertainties of one or a set of input variables to the uncertainty of model output variable\\n\\t3. an indicator that quantifies the strength of dependence between the model output variable and one or a set of input variables. \\n- difference-based - deriv=based methods, local importance measure, morris\\' screening method\\n    - **LIM** (local importance measure) - like LIME\\n      - can normalize weights by values of x, y, or ratios of their standard deviations\\n      - can also decompose variance to get the covariances between different variables\\n      - can approximate derivative via adjoint method or smth else\\n    - **morris\\' screening method**\\n      - take a grid of local derivs and look at the mean / std of these derivs\\n      - can\\'t distinguish between nonlinearity / interaction\\n    - using the squared derivative allows for a close connection w/ sobol\\'s total effect index\\n      - can extend this to taking derivs wrt different combinations of variables\\n- parametric regression\\n  - correlation coefficient, linear reg coeffeicients\\n  - **partial correlation coefficient** (PCC) - wipe out correlations due to other variables\\n    - do a linear regression using the other variables (on both X and Y) and then look only at the residuals\\n  - rank regression coefficient - better at capturing nonlinearity\\n  - could also do polynomial regression\\n  - more techniques (e.g. relative importance analysis RIA)\\n    - nonparametric regression\\n      - use something like LOESS, GAM, projection pursuit\\n      - rank variables by doing greedy search (add one var at a time) and seeing which explains the most variance\\n- hypothesis test\\n  - **grid-based hypothesis tests**: splitting the sample space (X, Y) into grids and then testing whether the patterns of sample distributions across different grid cells are random\\n    - ex. see if means vary\\n    - ex. look at entropy reduction\\n  - other hypothesis tests include the squared rank difference, 2D kolmogorov-smirnov test, and distance-based tests\\n- variance-based vim (sobol\\'s indices)\\n  - **ANOVA decomposition** - decompose model into conditional expectations $Y = g_0 + \\\\sum_i g_i (X_i) + \\\\sum_i \\\\sum_{j > i} g_{ij} (X_i, X_j) + \\\\dots + g_{1,2,..., p}$\\n    - $g_0 = \\\\mathbf E (Y)\\\\\\\\ g_i = \\\\mathbf E(Y \\\\vert X_i) - g_0 \\\\\\\\ g_{ij} = \\\\mathbf E (Y \\\\vert X_i, X_j) - g_i - g_j - g_0\\\\\\\\...$\\n    - take variances of these terms\\n    - if there are correlations between variables some of these terms can misbehave\\n    - note: $V(Y) = \\\\sum_i V (g_i) + \\\\sum_i \\\\sum_{j > i} V(g_{ij}) + ... V(g_{1,2,...,p})$ - variances are orthogonal and all sum to total variance\\n    - [anova decomposition basics](https://statweb.stanford.edu/~owen/mc/A-anova.pdf) - factor function into means, first-order terms, and interaction terms\\n  - $S_i$: **Sobol’s main effect** index: $=V(g_i)=V(E(Y \\\\vert X_i))=V(Y)-E(V(Y \\\\vert X_i))$\\n    - small value indicates $X_i$ is non-influential\\n    - usually used to select important variables\\n  - $S_{Ti}$: **Sobol\\'s total effect** index - include all terms (even interactions) involving a variable\\n  - equivalently, $V(Y) - V(E[Y \\\\vert X_{\\\\sim i}])$\\n    - usually used to screen unimportant variables\\n      - it is common to normalize these indices by the total variance $V(Y)$\\n    - three methods for computation - Fourire amplitude sensitivity test, meta-model, MCMC\\n    - when features are correlated, these can be strange (often inflating the main effects)\\n      - can consider $X_i^{\\\\text{Correlated}} = E(X_i \\\\vert X_{\\\\sim i})$ and $X_i^{\\\\text{Uncorrelated}} = X_i - X_i^{\\\\text{Correlated}}$\\n  - this can help us understand the contributions that come from different features, as well as the correlations between features (e.g. $S_i^{\\\\text{Uncorrelated}} = V(E[Y \\\\vert X_i^{\\\\text{Uncorrelated}}])/V(Y)$\\n    - [sobol indices connected to shapley value](https://epubs.siam.org/doi/pdf/10.1137/130936233)\\n      - $SHAP_i = \\\\underset{S, i \\\\in S}{\\\\sum} V(g_S) / \\\\vert S \\\\vert$\\n- moment-independent vim\\n  - want more than just the variance ot the output variables\\n  - e.g. **delta index** = average dist. between $f_Y(y)$ and $f_{Y \\\\vert X_i}(y)$ when $X_i$ is fixed over its full distr.\\n    - $\\\\delta_i = \\\\frac 1 2 \\\\mathbb E \\\\int \\\\vert f_Y(y) - f_{Y\\\\vert X_i} (y) \\\\vert dy = \\\\frac 1 2 \\\\int \\\\int \\\\vert f_{Y, X_i}(y, x_i) - f_Y(y) f_{X_i}(x_i) \\\\vert dy \\\\,dx_i$\\n    - moment-independent because it depends on the density, not just any moment (like measure of dependence between $y$ and $X_i$\\n  - can also look at KL, max dist..\\n- graphic vim - like curves\\n  - e.g. scatter plot, meta-model plot, regional VIMs, parametric VIMs\\n  - CSM - relative change of model ouput mean when range of $X_i$ is reduced to any subregion\\n  - CSV - same thing for variance\\n\\n#### importance curves\\n\\n- **pdp plots** - marginals (force value of plotted var to be what you want it to be)\\n- separate into **ice plots**  - marginals for instance\\n  - average of ice plots = pdp plot\\n  - sometimes these are centered, sometimes look at derivative\\n- both pdp ice suffer from many points possibly not being real\\n- possible solution: **Marginal plots M-plots** (bad name - uses conditional, not marginal)\\n  - only use points conditioned on certain variable\\n  - problem: this bakes things in (e.g. if two features are correlated and only one important, will say both are important)\\n- **ALE-plots** - take points conditioned on value of interest, then look at differences in predictions around a window\\n  - this gives pure effect of that var and not the others\\n  - needs an order (i.e. might not work for caterogical)\\n  - doesn\\'t give you individual curves\\n  - recommended very highly by the book...\\n  - they integrate as you go...\\n- summary: To summarize how each type of plot (PDP, M, ALE) calculates the effect of a feature at a certain grid value v:\\n   - Partial Dependence Plots: “Let me show you what the model predicts on average when each data instance has the value v for that feature. I ignore whether the value v makes sense for all data instances.” \\n- M-Plots: “Let me show you what the model predicts on average for data instances that have values close to v for that feature. The effect could be due to that feature, but also due to correlated features.” \\n  - ALE plots: “Let me show you how the model predictions change in a small “window” of the feature around v for data instances in that window.” \\n\\n### example-based explanations\\n\\n- influential instances - want to find important data points\\n- deletion diagnostics - delete a point and see how much it changed\\n- [influence funcs](https://arxiv.org/abs/1703.04730) (koh & liang, 2017): use **Hessian** ($\\\\theta x \\\\theta$) to give effect of upweighting a point\\n  - influence functions = inifinitesimal approach - upweight one person by infinitesimally small weight and see how much estimate changes (e.g. calculate first derivative)\\n  - influential instance - when data point removed, has a strong effect on the model (not necessarily same as an outlier)\\n  - requires access to gradient (e.g. nn, logistic regression)\\n  - take single step with Newton\\'s method after upweighting loss\\n  - yield change in parameters by removing one point\\n  - yield change in loss at one point by removing a different point (by multiplying above by cahin rule)\\n  - yield change in parameters by modifying one point\\n\\n### tree ensembles\\n\\n- MDI = Gini importance\\n- Breiman proposes permutation tests: Breiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1). Springer: 5–32\\n- [Explainable AI for Trees: From Local Explanations to Global Understanding](https://arxiv.org/abs/1905.04610) (lundberg et al. 2019)\\n  - shap-interaction scores - distribute among pairwise interactions + local effects\\n  - plot lots of local interactions together - helps detect trends\\n  - propose doing shap directly on loss function (identify how features contribute to loss instead of prediction)\\n  - can run supervised clustering (where SHAP score is the label) to get meaningful clusters\\n    - alternatively, could do smth like CCA on the model output\\n- [conditional variable importance for random forests](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-307) (strobl et al. 2008)\\n  - propose permuting conditioned on the values of variables not being permuted\\n    - to find region in which to permute, define the grid within which the values of $X_j$ are permuted for each tree by means of the partition of the feature space induced by that tree\\n  - many scores (such as MDI, MDA) measure marginal importance, not conditional importance\\n    - as a result, correlated variables get importances which are too high\\n- [treeshap](https://arxiv.org/abs/1802.03888) (lundberg, erion & lee, 2019): prediction-level\\n  - individual feature attribution: want to decompose prediction into sum of attributions for each feature\\n    - each thing can depend on all features\\n  - Saabas method: basic thing for tree\\n    - you get a pred at end\\n    - count up change in value at each split for each variable\\n  - three properties\\n    - local acc - decomposition is exact\\n    - missingness - features that are already missing are attributed no importance\\n      - for missing feature, just (weighted) average nodes from each split\\n    - consistency - if F(X) relies more on a certain feature j, $F_j(x)$ should \\n      - however Sabaas method doesn\\'t change $F_j(X)$ for $F\\'(x) = F(x) + x_j$\\n  - these 3 things iply we want shap values\\n  - average increase in func value when selecting i (given all subsets of other features)\\n  - for binary features with totally random splits, same as Saabas\\n  - **can cluster based on explanation similarity** (fig 4)\\n    - can quantitatively evaluate based on clustering of explanations\\n  - their fig 8 - qualitatively can see how different features alter outpu\\n  - gini importance is like weighting all of the orderings\\n- [understanding variable importances in forests of randomized trees](http://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-tre) (louppe et al. 2013)\\n  - consider fully randomized trees\\n    - assume all categorical\\n    - randomly pick feature at each depth, split on all possibilities\\n    - also studied by biau 2012\\n    - extreme case of random forest w/ binary vars?\\n  - real trees are harder: correlated vars and stuff mask results of other vars lower down\\n  - asymptotically, randomized trees might actually be better\\n- [Actionable Interpretability through Optimizable Counterfactual Explanations for Tree Ensembles](https://arxiv.org/pdf/1911.12199v1.pdf) (lucic et al. 2019)\\n\\n### neural nets (dnns)\\n\\n#### dnn visualization\\n\\n- [good summary on distill](https://distill.pub/2017/feature-visualization/)\\n- **visualize intermediate features**\\n    1. visualize filters by layer\\n      - doesn\\'t really work past layer 1\\n    2. *decoded filter* - rafegas & vanrell 2016\\n      - project filter weights into the image space\\n      - pooling layers make this harder\\n    3. *deep visualization* - yosinski 15\\n    4. [Understanding Deep Image Representations by Inverting Them](https://arxiv.org/abs/1412.0035) (mahendran & vedaldi 2014) - generate image given representation\\n- penalizing activations\\n    - [interpretable cnns](http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0490.pdf) (zhang et al. 2018) - penalize activations to make filters slightly more intepretable\\n      - could also just use specific filters for specific classes...\\n    - teaching compositionality to cnns - mask features by objects\\n- approaches based on maximal activation\\n\\t-\\timages that maximally activate a feature \\n\\t\\t- [deconv nets](https://arxiv.org/pdf/1311.2901.pdf) - Zeiler & Fergus (2014) use deconvnets (zeiler et al. 2011) to map features back to pixel space\\n\\t\\t\\t- given one image, get the activations (e.g. maxpool indices) and use these to get back to pixel space\\n\\t\\t\\t- everything else does not depend on the original image\\n\\t\\t\\t- might want to use optimization to generate image that makes optimal feature instead of picking from training set\\n     \\t - before this, erhan et al. did this for unsupervised features\\n     \\t - dosovitskiy et al 16 - train generative deconv net to create images from neuron activations\\n     \\t - aubry & russel 15 do similar thing\\n\\t\\t- [deep dream](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) - reconstruct image from feature map\\n         - could use natural image prior\\n         - could train deconvolutional NN\\n         - also called *deep neuronal tuning* - GD to find image that optimally excites filters\\n    - *neuron feature* - weighted average version of a set of maximum activation images that capture essential properties - rafegas_17\\n      - can also define *color selectivity index* - angle between first PC of color distribution of NF and intensity axis of opponent color space\\n      - *class selectivity index* - derived from classes of images that make NF\\n    - saliency maps for each image / class\\n      - simonyan et al 2014\\n    - [Diagnostic Visualization for Deep Neural Networks Using Stochastic Gradient Langevin Dynamics](https://arxiv.org/pdf/1812.04604.pdf) - sample deep dream images generated by gan\\n- [posthoc prototypes](https://openreview.net/forum?id=r1xyx3R9tQ)\\n    - **counterfactual explanations** - like adversarial, counterfactual explanation describes smallest change to feature vals that changes the prediction to a predefined output\\n      - maybe change fewest number of variables not their values\\n      - counterfactual should be reasonable (have likely feature values)\\n      - human-friendly\\n      - usually multiple possible counterfactuals (Rashomon effect)\\n      - can use optimization to generate counterfactual\\n      - **anchors** - opposite of counterfactuals, once we have these other things won\\'t change the prediction\\n    - prototypes (assumed to be data instances)\\n      - prototype = data instance that is representative of lots of points\\n      - criticism = data instances that is not well represented by the set of prototypes\\n      - examples: k-medoids or MMD-critic\\n        - selects prototypes that minimize the discrepancy between the data + prototype distributions\\n- [Explaining Deep Learning Models with Constrained Adversarial Examples](https://arxiv.org/abs/1906.10671)\\n- [Understanding Deep Architectures by Visual Summaries](http://bmvc2018.org/papers/0794.pdf)\\n- [Semantics for Global and Local Interpretation of Deep Neural Networks](https://arxiv.org/abs/1910.09085)\\n- [Iterative augmentation of visual evidence for weakly-supervised lesion localization in deep interpretability frameworks](https://arxiv.org/abs/1910.07373)\\n- [explaining image classifiers by counterfactual generation](https://arxiv.org/pdf/1807.08024.pdf) \\n    - generate changes (e.g. with GAN in-filling) and see if pred actually changes\\n    - can search for smallest sufficient region and smallest destructive region\\n    - ![Screen Shot 2020-01-20 at 9.14.44 PM](../assets/fido.png)\\n\\n#### dnn concept-based explanations\\n\\n- [concept activation vectors](https://arxiv.org/abs/1711.11279)\\n    - Given: a user-defined set of examples for a concept (e.g., ‘striped’), and random\\n            examples, labeled training-data examples for the studied class (zebras) \\n        - given trained network\\n        - TCAV can quantify the model’s sensitivity to the concept for that class. CAVs are learned by training a linear classifier to distinguish between the activations produced by\\n            a concept’s examples and examples in any layer\\n        - CAV - vector orthogonal to the classification boundary\\n        - TCAV uses the derivative of the CAV direction wrt input\\n    - [automated concept activation vectors](https://arxiv.org/abs/1902.03129) - Given a set of concept discovery images, each image is segmented with different resolutions to find concepts that are captured best at different sizes. (b) After removing duplicate segments, each segment is resized tothe original input size resulting in a pool of resized segments of the discovery images. (c) Resized segments are mapped to a model’s activation space at a bottleneck layer. To discover the concepts associated with the target class, clustering with outlier removal is performed. (d) The output of our method is a set of discovered concepts for each class, sorted by their importance in prediction\\n- [Explaining The Behavior Of Black-Box Prediction Algorithms With Causal Learning](https://arxiv.org/abs/2006.02482) - specify some interpretable features and learn a causal graph of how the classifier uses these features\\n- [On Completeness-aware Concept-Based Explanations in Deep Neural Networks](https://arxiv.org/abs/1910.07969)\\n\\n#### dnn feature importance\\n\\n- saliency maps\\n    1. occluding parts of the image\\n      - sweep over image and remove patches\\n      - which patch removals had highest impact on change in class?\\n    2. text usually uses attention maps\\n      - ex. karpathy et al LSTMs\\n      - ex. lei et al. - most relevant sentences in sentiment prediction\\n      - [attention is not explanation](https://arxiv.org/abs/1902.10186) (jain & wallace, 2019)\\n      - [attention is not **not** explanation](https://arxiv.org/abs/1908.04626) (wiegreffe & pinter, 2019)\\n        - influence = pred with a word - pred with a word masked\\n          - attention corresponds to this kind of influence\\n      - deceptive attention - we can successfully train a model to make similar predictions but have different attention\\n    3. class-activation map - sum the activations across channels (weighted by their weight for a particular class)\\n    4. [RISE](https://arxiv.org/pdf/1806.07421.pdf) (Petsiuk et al. 2018) - randomized input sampling\\n       1. randomly mask the images, get prediction\\n       2. saliency map = sum of masks weighted by the produced predictions\\n- gradient-based methods - visualize what in image would change class label\\n  - gradient * input\\n  - [integrated gradients](http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf) (sundararajan et al. 2017) - just sum up the gradients from some baseline to the image (in 1d, this is just $f(x) - f(baseline))$\\n    - in higher dimensions, such as images, we pick the path to integrate by starting at some baseline (e.g. all zero) and then get gradients as we interpolate between the zero image and the real image\\n    - if we picture 2 features, we can see that integrating the gradients will not just yield $f(x) - f(baseline)$, because each time we evaluate the gradient we change both features\\n    - [explanation distill article](https://distill.pub/2020/attribution-baselines/) \\n      - ex. any pixels which are same in original image and modified image will be given 0 importance\\n      - lots of different possible choices for baseline (e.g. random Gaussian image, blurred image, random image from the training set)\\n      - could also average over distributions of baseline (this yields **expected gradients**)\\n      - when we do a Gaussian distr., this is very similar to smoothgrad\\n  - lrp\\n  - taylor decomposition\\n  - deeplift\\n  - guided backpropagation - springenberg et al\\n  - lets you better create maximally specific image\\n  - selvaraju 17 - *grad-CAM*\\n  - [grad-cam++](https://arxiv.org/abs/1710.11063)\\n  - [competitive gradients](https://arxiv.org/pdf/1905.12152.pdf) (gupta & arora 2019)\\n  - Label  \"wins\" a pixel if either (a) its map assigns that pixel a positive score higher than the scores assigned by every other label ora negative score lower than the scores assigned by every other label. \\n  - final saliency map consists of scores assigned by the chosen label to each pixel it won, with the map containing a score 0 for any pixel it did not win.\\n  - can be applied to any method which satisfies completeness (sum of pixel scores is exactly the logit value)\\n- newer methods\\n    - [Score-CAM:Improved Visual Explanations Via Score-Weighted Class Activation Mapping](https://arxiv.org/abs/1910.01279)\\n    - [Removing input features via a generative model to explain their attributions to classifier\\'s decisions](https://arxiv.org/abs/1910.04256)\\n    - [NeuroMask: Explaining Predictions of Deep Neural Networks through Mask Learning](https://ieeexplore.ieee.org/abstract/document/8784063)\\n    - [Interpreting Undesirable Pixels for Image Classification on Black-Box Models](https://arxiv.org/abs/1909.12446)\\n    - [Guideline-Based Additive Explanation for Computer-Aided Diagnosis of Lung Nodules](https://link.springer.com/chapter/10.1007/978-3-030-33850-3_5)\\n    - [Learning how to explain neural networks: PatternNet and PatternAttribution](https://arxiv.org/abs/1705.05598) - still gradient-based\\n        - [Generalized PatternAttribution for Neural Networks with Sigmoid Activations](https://ieeexplore.ieee.org/abstract/document/8851761)\\n    - [Learning Reliable Visual Saliency for Model Explanations](https://ieeexplore.ieee.org/abstract/document/8884184)\\n    - [Neural Network Attributions: A Causal Perspective](https://arxiv.org/abs/1902.02302)\\n    - [Gradient Weighted Superpixels for Interpretability in CNNs](https://arxiv.org/abs/1908.08997)\\n    - [Decision Explanation and Feature Importance for Invertible Networks](https://arxiv.org/abs/1910.00406) (mundhenk et al. 2019)\\n    - [Efficient Saliency Maps for Explainable AI](https://deepai.org/publication/efficient-saliency-maps-for-explainable-ai) \\n- interactions\\n    - [hierarchical interpretations for neural network predictions](https://arxiv.org/abs/1806.05337) (singh et al. 2019)\\n      - [contextual decomposition](https://arxiv.org/abs/1801.05453) (murdoch et al. 2018)\\n      - ACD followup work\\n        - [Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models](https://openreview.net/forum?id=BkxRRkSKwr)\\n    - [Detecting Statistical Interactions from Neural Network Weights](https://arxiv.org/abs/1705.04977) - interacting inputs must follow strongly weighted connections to a common hidden unit before the final output\\n      - [Neural interaction transparency (NIT)](https://dl.acm.org/citation.cfm?id=3327482) (tsang et al. 2017)\\n    - [Recovering Pairwise Interactions Using Neural Networks](https://arxiv.org/pdf/1901.08361.pdf)\\n\\n#### textual explanations\\n\\n- [Adversarial Inference for Multi-Sentence Video Description](https://arxiv.org/pdf/1812.05634.pdf) - adversarial techniques during inference for a better multi-sentence video description\\n- [Object Hallucination in Image Captioning](https://aclweb.org/anthology/D18-1437) - image relevance metric - asses rate of object hallucination\\n   - CHAIR metric - what proportion of words generated are actually in the image according to gt sentences and object segmentations\\n- [women also snowboard](https://arxiv.org/pdf/1803.09797.pdf) - force caption models to look at people when making gender-specific predictions\\n- [Fooling Vision and Language Models Despite Localization and Attention Mechanism](http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Fooling_Vision_and_CVPR_2018_paper.pdf) -  can do adversarial attacks on captioning and VQA\\n- [Grounding of Textual Phrases in Images by Reconstruction](https://arxiv.org/pdf/1511.03745.pdf) - given text and image provide a bounding box (supervised problem w/ attention)\\n- [Natural Language Explanations of Classifier Behavior](https://ieeexplore.ieee.org/abstract/document/8791710)\\n\\n### model summarization / distillation\\n\\n- [piecewise linear interp](https://arxiv.org/pdf/1806.10270.pdf)\\n- [Computing Linear Restrictions of Neural Networks](https://arxiv.org/abs/1908.06214) - calculate function of neural network restricting its points to lie on a line\\n- [Interpreting CNN Knowledge via an Explanatory Graph](https://arxiv.org/abs/1708.01785) (zhang et al. 2017) - create a graph that responds better to things like objects than individual neurons\\n- model distillation (model-agnostic)\\n  - Trepan - approximate model w/ a decision tree\\n  - [BETA](https://arxiv.org/abs/1707.01154) (lakkaraju et al. 2017) - approximate model by a rule list\\n\\n\\n\\n#### connecting dnns with tree-models\\n\\n- [Distilling a Neural Network Into a Soft Decision Tree](https://arxiv.org/pdf/1711.09784.pdf) (frosst & hinton 2017) - distills DNN into DNN-like tree which uses sigmoid neuron decides which path to follow\\n  - training on distilled DNN predictions outperforms training on original labels\\n  - to make the decision closer to a hard cut, can multiply by a large scalar before applying sigmoid\\n  - parameters updated with backprop\\n  - regularization to ensure that all paths are taken equally likely\\n- [Neural Random Forests](https://link.springer.com/article/10.1007/s13171-018-0133-y) (biau et al. 2018) - convert DNN to RF\\n  - first layer learns a node for each split\\n  - second layer learns a node for each leaf (by only connecting to nodes on leaves in the path)\\n  - finally map each leaf to a value\\n  - relax + retrain\\n- [Deep Neural Decision Forests](https://openaccess.thecvf.com/content_iccv_2015/papers /Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf) (2015)\\n  - dnn learns small intermediate representation, which outputs all possible splits in a tree\\n  - these splits are forced into a tree-structure and optimized via SGD\\n  - neurons use sigmoid function\\n- [Gradient Boosted Decision Tree Neural Network](https://arxiv.org/abs/1910.09340) - build DNN based on decision tree ensemble - basically the same but with gradient-boosted trees\\n- [Neural Decision Trees](https://arxiv.org/abs/1702.07360) - treat each neural net like a node in a tree\\n\\n\\n\\n## different problems / perspectives\\n\\n### improving models\\n\\n- [Interpretations are useful: penalizing explanations to align neural networks with prior knowledge](https://arxiv.org/abs/1909.13584) (rieger et al. 2020)\\n- [Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations](https://arxiv.org/abs/1703.03717)\\n- [Explain to Fix: A Framework to Interpret and Correct DNN Object Detector Predictions](https://arxiv.org/pdf/1811.08011.pdf)\\n- [Understanding Misclassifications by Attributes](https://arxiv.org/abs/1910.07416)\\n\\n### recourse\\n\\n- [actionable recourse in linear classification](https://arxiv.org/pdf/1809.06514.pdf) (ustun et al. 2019)\\n  - want model to provide actionable inputs (e.g. income) rather than immutable variables (e.g. age, marital status)\\n    - drastic changes in actionable inputs are basically immutable\\n  - **recourse** - can person obtain desired prediction from fixed mode by changing actionable input variables (not just standard explainability)\\n\\n### interp for rl\\n\\n- heatmaps\\n- visualize most interesting states / rollouts\\n- language explanations\\n- interpretable intermediate representations (e.g. bounding boxes for autonomous driving)\\n- policy extraction - distill a simple model from a bigger model (e.g. neural net -> tree)\\n\\n### interpretation over sets / perturbations\\n\\nThese papers don\\'t quite connect to prediction, but are generally about finding stable interpretations across a set of models / choices.\\n\\n- [All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously](https://www.jmlr.org/papers/volume20/18-760/18-760.pdf) (fisher, rudin, & dominici, 2019) - also had title *Model class reliance: Variable importance measures for any machine learning model class, from the “Rashomon” perspective*\\n  - **model reliance** = MR - like permutation importance, measures how much a model relies on covariates of interest for its accuracy\\n    - defined (for a feature) as the ratio of expected loss after permuting (with all possible permutation pairs) to before permuting\\n      - could also be defined as a difference or using predictions rather than loss\\n    - connects to U-statistics - can shows unbiased etc.\\n    - related to *Algorithm Reliance (AR)* - fitting with/without a feature and measuring the difference in loss (see [gevrey et al. 03](https://www.sciencedirect.com/science/article/abs/pii/S0304380002002570))\\n  - **model-class reliance** = MCR = highest/lowest degree of MR within a class of well-performing models\\n    - with some assumptions on model class complexity (in the form of a *covering number*), can create uniform bounds on estimation error\\n    - MCR can be efficiently computed for (regularized) linear / kernel linear models\\n  - **Rashomon set** = class of well-performing models\\n    - \"Rashomon\" effect of statistics - many prediction models may fit the data almost equally well (breiman 01)\\n    - \"This set can be thought of as representing models that might be arrived at due to differences in data measurement, processing, filtering, model parameterization, covariate selection, or other analysis choices\"\\n    - can study these tools for describing rank of risk predictions, variance of predictions, e.g. confidence intervals\\n    - ![Screen Shot 2020-09-27 at 8.20.35 AM](../assets/mcr_depiction.png)\\n  - **confidence intervals** - can get finite-sample interval for anything, not just loss (e.g. norm of coefficients, prediction for a specific point)\\n  - connections to causality\\n    - when function is conditional expectation, then MR is similar to many things studies in causal literature\\n    - conditional importance measures a different notion (takes away things attributed to spurious variables)\\n      - can be hard to do conditional permutation well when some feature pairs are rare so can use weighting, matching, or imputation\\n  - here, application is to see on COMPAS dataset whether one can build an accurate model which doesn\\'t rely on race / sex (in order to audit black-box COMPAS models)\\n- [A Theory of Statistical Inference for Ensuring the Robustness of Scientific Results](https://arxiv.org/abs/1804.08646) (coker, rudin, & king, 2018)\\n  - Inference = process of using facts we know to learn about facts we do not know\\n  - **hacking intervals** - the range of a summary statistic one may obtain given a class of possible endogenous manipulations of the data\\n    - **prescriptively constrained** hacking intervals - explicitly define reasonable analysis perturbations\\n      - ex. hyperparameters (e.g. k in kNN), matching algorithm, adding a new feature\\n    - **tethered hacking intervals** - take any model with small enough loss on the data\\n      - rather than choosing $\\\\alpha$, we choose error tolerance\\n      - for MLE, equivalent to profile likelihood confidence intervals\\n      - ex. SVM distance from point to boundary, Kernel regression prediction for a specific new point, feature selection\\n      - ex. linear regression ATE, individual treatment effect\\n    - PCS intervals could be seen as slightly broder, including data cleaning and problem translations\\n  - different theories of inference have different counterfactual worlds\\n    - p-values - data from a superpopulation\\n    - Fisher’s exact p-values - fix the data and randomize counterfactual treatment assignments\\n    - Causal sensitivity analysis - unmeasured confounders from a defined set\\n    - bayesian credible intervals - redrawing the data from the same data generating process, given the observed data and assumed prior and likelihood model\\n    - hacking intervals - counterfactual researchers making counterfactual analysis choices\\n  - 2 approaches to replication\\n    - replicating studies - generally replication is very low\\n    - *p*-curve approach: look at distr. of p-values, check if lots of things are near 0.05\\n- [A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning](https://arxiv.org/pdf/1908.01755.pdf) (semenova, rudin, & parr, 2020)\\n  - **rashomon ratio** - ratio of the volume of the set of accurate models to the volume of the hypothesis space\\n    - can use this to perform model selection over different hypothesis spaces using empirical risk v. rashomon ratio (*rashomon curve*)\\n- [Underspecification Presents Challenges for Credibility in Modern Machine Learning](https://arxiv.org/pdf/2011.03395.pdf) (D’Amour et al. 2020)\\n  - shortcuts = spurious correlations cause failure because of ambiguity in the data\\n  - *stress tests* probe a broader set of requirements\\n    - ex. subgroup analyses, domain shift, contrastive evaluations (looking at transformations of an individual example, such as counterfactual notions of fairness)\\n  - suggestions\\n    - need to test models more thoroughly\\n    - need criteria to select among good models (e.g. explanations)\\n- [Predictive Multiplicity in Classification](https://arxiv.org/pdf/1909.06677.pdf) (marx et al. 2020)\\n  - predictive multiplicity = ability of a prediction problem to admit competing models with conflicting predictions\\n\\n## misc new papers\\n\\n- [iNNvestigate neural nets](https://arxiv.org/abs/1808.04260) - provides a common interface and out-of-thebox implementation\\n- [tensorfuzz](https://arxiv.org/abs/1807.10875) - debugging\\n- [ICIE 1.0: A Novel Tool for Interactive Contextual Interaction Explanations](https://link.springer.com/chapter/10.1007/978-3-030-13463-1_6)\\n- [ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases](https://arxiv.org/abs/1711.11443)\\n  - cnns are more accurate, robust, and biased then we might expect on imagenet\\n- [Bridging Adversarial Robustness and Gradient Interpretability](https://arxiv.org/abs/1903.11626)\\n- [explaining a black-box w/ deep variational bottleneck](https://arxiv.org/abs/1902.06918)\\n- [Global Explanations of Neural Networks: Mapping the Landscape of Predictions](https://arxiv.org/abs/1902.02384)\\n- [neural stethoscopes](https://arxiv.org/pdf/1806.05502.pdf) \\n- [xGEMs](https://arxiv.org/pdf/1806.08867.pdf) \\n- [maximally invariant data perturbation](https://arxiv.org/pdf/1806.07004.pdf)\\n- hard coding\\n  - [SSIM layer](https://arxiv.org/abs/1806.09152)\\n  - Inverting Supervised Representations with Autoregressive Neural Density Models \\n- [nonparametric var importance](http://proceedings.mlr.press/v80/feng18a/feng18a.pdf)\\n- [supervised local modeling](https://arxiv.org/abs/1807.02910 ) \\n- [detect adversarial cnn attacks w/ feature maps](https://digitalcollection.zhaw.ch/handle/11475/8027) \\n- [adaptive dropout](https://arxiv.org/abs/1807.08024)\\n- [lesion detection saliency](https://arxiv.org/pdf/1807.07784.pdf) \\n- [How Important Is a Neuron?](https://arxiv.org/abs/1805.12233)\\n- [symbolic execution for dnns](https://arxiv.org/pdf/1807.10439.pdf)\\n- [L-shapley abd C-shapley](https://arxiv.org/pdf/1808.02610.pdf)\\n- [A Simple and Effective Model-Based Variable Importance Measure](https://arxiv.org/pdf/1805.04755.pdf)\\n  - measures the feature importance (defined as the variance of the 1D partial dependence function) of one feature conditional on different, fixed points of the other feature. When the variance is high, then the features interact with each other, if it is zero, they don’t interact.\\n- [Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic Corrections](https://arxiv.org/pdf/1802.07384.pdf)\\n- [DeepPINK: reproducible feature selection in deep neural networks](https://arxiv.org/pdf/1809.01185.pdf)\\n- \"Explaining Deep Learning Models -- A Bayesian Non-parametric Approach\"\\n- [Detecting Potential Local Adversarial Examples for Human-Interpretable Defense](https://arxiv.org/pdf/1809.02397.pdf)\\n- [Interpreting Layered Neural Networks via Hierarchical Modular Representation](https://arxiv.org/pdf/1810.01588.pdf)\\n- [Entropic Variable Boosting for Explainability & Interpretability in Machine Learning](https://arxiv.org/abs/1810.07924)\\n- [Understanding Individual Decisions of CNNs via Contrastive Backpropagation](https://arxiv.org/abs/1812.02100v1)\\n- [Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation](https://arxiv.org/abs/1902.00407)\\n- [A Game Theoretic Approach to Class-wise Selective Rationalization](https://arxiv.org/abs/1910.12853)\\n- [Additive Explanations for Anomalies Detected from Multivariate Temporal Data](https://dl.acm.org/citation.cfm?id=3358121)\\n- [Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability](https://arxiv.org/abs/1910.06358)\\n- [Consistent Regression using Data-Dependent Coverings](https://arxiv.org/abs/1907.02306)\\n- [Contextual Local Explanation for Black Box Classifiers](https://arxiv.org/abs/1910.00768)\\n- [Contextual Prediction Difference Analysis](https://arxiv.org/abs/1910.09086)\\n- [Contrastive Relevance Propagation for Interpreting Predictions by a Single-Shot Object Detector](https://ieeexplore.ieee.org/abstract/document/8851770)\\n- [CXPlain: Causal Explanations for Model Interpretation under Uncertainty](https://arxiv.org/abs/1910.12336)\\n- [Ensembles of Locally Independent Prediction Models](https://arxiv.org/abs/1911.01291)\\n- [Explaining Black-Box Models Using Interpretable Surrogates](https://link.springer.com/chapter/10.1007/978-3-030-29908-8_1)\\n- [Explaining Classifiers with Causal Concept Effect (CaCE)](https://arxiv.org/abs/1907.07165)\\n- [Generative Counterfactual Introspection for Explainable Deep Learning](https://arxiv.org/abs/1907.03077)\\n- [Grid Saliency for Context Explanations of Semantic Segmentation](https://arxiv.org/abs/1907.13054)\\n- [Optimal Explanations of Linear Models](https://arxiv.org/abs/1907.04669)\\n- [Privacy Risks of Explaining Machine Learning Models](https://arxiv.org/abs/1907.00164)\\n- [RL-LIM: Reinforcement Learning-based Locally Interpretable Modeling](https://arxiv.org/abs/1909.12367)\\n- [The many Shapley values for model explanation](https://arxiv.org/abs/1908.08474)\\n- [XDeep: An Interpretation Tool for Deep Neural Networks](https://arxiv.org/abs/1911.01005)\\n- [Shapley Decomposition of R-Squared in Machine Learning Models](https://arxiv.org/abs/1908.09718)\\n- [Understanding Global Feature Contributions Through Additive Importance Measures](https://arxiv.org/abs/2004.00668) (covert, lundberg, & lee 2020)\\n  - SAGE score looks at reduction in predictive accuracy due to subsets of features\\n- [Personal insights for altering decisions of tree-based ensembles over time](https://dl.acm.org/doi/abs/10.14778/3380750.3380752)\\n- [Gradient-Adjusted Neuron Activation Profiles for Comprehensive Introspection of Convolutional Speech Recognition Models](https://arxiv.org/abs/2002.08125)[\\n- [Learning Global Transparent Models from Local Contrastive Explanations](https://arxiv.org/abs/2002.08247)\\n- [Boosting Algorithms for Estimating Optimal Individualized Treatment Rules](https://arxiv.org/abs/2002.00079)\\n- [Explaining Knowledge Distillation by Quantifying the Knowledge](https://arxiv.org/abs/2003.03622)\\n- [Adversarial TCAV -- Robust and Effective Interpretation of Intermediate Layers in Neural Networks](https://arxiv.org/abs/2002.03549)\\n- [Problems with Shapley-value-based explanations as feature importance measures](https://arxiv.org/abs/2002.11097)*\\n- [When Explanations Lie: Why Many Modified BP Attributions Fail](https://www.researchgate.net/profile/Leon_Sixt/publication/338115768_When_Explanations_Lie_Why_Many_Modified_BP_Attributions_Fail/links/5e4e226292851c7f7f48becb/When-Explanations-Lie-Why-Many-Modified-BP-Attributions-Fail.pdf)\\n- [Estimating Training Data Influence by Tracking Gradient Descent](https://arxiv.org/abs/2002.08484)\\n- [Interpreting Interpretations: Organizing Attribution Methods by Criteria](https://arxiv.org/abs/2002.07985)\\n- [Explaining Groups of Points in Low-Dimensional Representations](https://arxiv.org/abs/2003.01640)\\n- [Causal Interpretability for Machine Learning -- Problems, Methods and Evaluation](https://arxiv.org/abs/2003.03934)\\n- [Cyclic Boosting - An Explainable Supervised Machine Learning Algorithm - IEEE Conference Publication](https://ieeexplore.ieee.org/abstract/document/8999347)\\n- [A Causality Analysis for Nonlinear Classification Model with Self-Organizing Map and Locally Approximation to Linear Model](https://www.semanticscholar.org/paper/A-Causality-Analysis-for-Nonlinear-Classification-Kirihata-Maekawa/4b76830be36ae14d878f7c0a7ff2508bfe172f64)\\n- [Black-Box Saliency Map Generation Using Bayesian Optimisation](https://arxiv.org/abs/2001.11366)\\n- [ON NETWORK SCIENCE AND MUTUAL INFORMATION FOR EXPLAINING DEEP NEURAL NETWORKS Brian Davis1∗](https://umangsbhatt.github.io/reports/icassp_2020.pdf)\\n\\n\\n\\n',\n",
       " '[TOC]\\n\\n## data\\n\\n- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html#sklearn.datasets.fetch_openml\\n  - sklearn also lets you generate some nice synthetic datasets\\n- pmlb\\n\\n## misc\\n\\n- anonymous github: [https://anonymous.4open.science](https://anonymous.4open.science/)\\n- posters\\n  - new [simple poster template](https://www.youtube.com/watch?v=1RwJbhkCA58&feature=youtu.be)\\n  - infographics don\\'t work great\\n  - \"perfection is not when you have nothing to add, it\\'s when you have nothing to take away\"\\n- [draw arch](http://alexlenail.me/NN-SVG/index.html)\\n- [viz architectures](https://tensorspace.org/index.html)\\n- medical ideas: many diseases manifest themselves in the activity of neurons, not in the structure\\n- ovw![ai_generations](../assets/ai_generations.png)\\n\\n## levels\\n\\n- *Marr\\'s three levels*\\n  1. computation - behavior\\n  2. algorithm - causal connectomics\\n  3. basic implementation - structural connectmoics, molecular biology\\n\\n## questions\\n\\n- *problems that are solved, or soon will be*\\n  - how do single neurons compute?\\n  - what is the connectome of a small nervous system, like that of C. elegans (300 neurons)?\\n  - how can we image a live brain of 100,000 neurons at cellular and millisecond resolution?\\n    - hydra was completed\\n  - how does sensory transduction work?\\n- *problems that we should be able to solve in the next 50 years*\\n  - can we add senses to the brain?\\n    - like cochlear implant\\n    - like vibrations\\n  - how do circuits of neurons compute?\\n  - what is the complete connectome of the mouse brain (70e6 neurons)?\\n  - how can we image a live mouse brain at cellular and millisecond resolution?\\n  - what causes psychiatric and neurological illness?\\n  - how do learning and memory work?\\n    - short-term vs. long-term\\n    - declarative vs. non-declarative\\n    - encodes relationships between things not things themselves\\n    - memory retrieval\\n  - why do we sleep and dream?\\n    1. sleep is restorative (but then why high neural activity?)\\n    2. allows the brain to run simulations\\n    3. consolidating memories and forgetting\\n  - where is consciousness?\\n    - at this point, sounds and vision should line up (delayed appropriately)\\n  - how do we make decisions?\\n  - how does the brain represent abstract ideas?\\n  - what does neural baseline activity represent?\\n  - how does the brain solve timing?\\n    - moving eyes\\n    - blinking\\n    - hearing and vision time differences\\n  - how does sensorimotor learning build a model of the world?\\n- *problems that we should be able to solve, but who knows when*\\n  - how do brains simulate the future?\\n  - how does the mouse brain compute?\\n  - what is the complete connectome of the human brain (8e10 neurons)?\\n  - how can we image a live human brain at cellular and millisecond resolution?\\n  - how could we cure psychiatric and neurological diseases?\\n  - how could we make everybody’s brain function best?\\n  - brain and quantum?\\n    - some work in quantum neural nets\\n  - how is info coded in neural activity?\\n    - like measuring tansistors and guessing what computer is doing\\n    - neuron gets lots of inputs\\n  - do glial cells and other signaling molecules compute?\\n  - what is intelligence?\\n    - what is iq?\\n  - how do specialized systems integrate?\\n- *problems we may never solve*\\n  - what are emotions?\\n    - brain states that quickly assign values\\n    - in the amygdala\\n  - how does the human brain compute?\\n  - how can cognition be so flexible and generative?\\n  - how and why does conscious experience arise?\\n    - thing that flickers on when you wake up that was not there\\n    - evolutionary to manage all the different systems\\n- *meta-questions*\\n  - what counts as an explanation of how the brain works? (and which disciplines would be needed to provide it?)\\n  - how could we build a brain? (how do evolution and development do it?)\\n  - what are the different ways of understanding the brain? (what is function, algorithm, implementation?)\\n- ref David Eaglemen article: http://discovermagazine.com/2007/aug/unsolved-brain-mysteries\\n- ref Adolphs 2015, \"The unsolved problems of neuroscience\"\\n\\n## small misc things\\n\\n- neuroscience is like IT - given computer, figure out how it works\\n- https://grand-challenge.org/all_challenges/\\n\\n#### rna barcoding\\n- allows for tagging different neurons\\n  - can then optically get differences\\n  - also can sequence and get differences (http://www.cell.com/neuron/pdf/S0896-6273%2816%2930421-4.pdf)\\n- future of electrophysiology: https://www.technologynetworks.com/neuroscience/articles/shining-a-light-on-the-future-of-electrophysiology-286992\\n\\n#### brain transplant\\n\\n- computational hypothesis of the mind\\n\\n#### tms\\n- temporary cure for autism\\n- can change people\\'s minds\\n\\n#### quantum brain\\n\\n- quantum brain?\\n\\n#### brain on a chip\\n\\n- neuromorphic chips\\n- grow cells in vivo\\n\\n#### connectomics\\n\\n- C Elegans\\n  - 302 neurons\\n  - no evidence of Hebbian learning\\n  - develop synaptogenesis rules?\\n\\n#### history\\n\\n- biology U: phenomenon (high level) -> element (low level) -> synthesis (high level)\\n\\n\\n#### cogsci\\n\\n- model-free vs model-based\\n\\n\\n\\n#### biomechanics\\n\\n- brain exists to make suggestions to motor system\\n- reflexes don\\'t need cortex, but still tricky\\n  - ex. wipe skin when irritated in frog - works when leg at different points, leg stopped\\n- idea: t-sne on neural dynamics: someone at Emory\\n- spinal chord is where reflexes are, then brainstem, and cortex is quite slow\\n\\n\\n## scientists\\n\\n- ml\\n  - *Geoffrey Hinton* - emeritus, Toronto\\n    - *Yann Lecun* (NYU) - heads fb AI\\n      - was his research associate\\n  - *Michael Jordan* - Berkeley\\n    - students: Ng, Blei; postdoc: Bengio\\n    - *Andrew Ng* - Stanford\\n    - *David Blei* - topic modeling\\n    - *Yoshua Bengio* - McGill\\n      - *Ian Goodfellow* - GANs\\n  - *Jeff Hawkins* - established redwood\\n    - left to found numenta\\n  - *Terry Sejnowski*\\n    - coinvented Boltzmann machines\\n  - *Daphne Koller* - co-founder of Coursera\\n    - representation, inference, learning, and decision making\\n  - *Schmidhuber & Hochreiter* - LSTM\\n  - *Jitendra Malik* - computer vision\\n  - *Andrej Karpathy* - blogger, Tesla AI director\\n- comp neuro\\n  - *Karl Friston* - functional imaging analysis\\n  - *Raymond Dolan* - emotion, pain\\n  - *Terrence J. Sejnowski* - boltzmann machines, ICA\\n  - *david marr* - vision\\n  - *tomaso poggio* - vision\\n  - *Christoph Koch* - head of allen institute\\n  - *Daniel Wolpert* - noise in the nervous system\\n  - *Jonathan Cohen* - theory\\n  - *Larry Abbott* - theoretical neuroscience\\n  - *György Buzsáki* - oscillations\\n  - Peter Dayan\\n  - Haim Sompolinsky \\n  - Stephen Grossberg\\n  - Randall C. O\\'Reilly\\n  - Nancy Kopell\\n  - Chris Eliasmith\\n  - Michael Hasselmo\\n  - David Heeger\\n  - Roger D. Traub\\n  - Bard Ermentrout\\n  - Eugene M. Izhikevich\\n  - Eric L. Schwartz\\n- misc\\n  - *Byron Yu* - bmi\\n  - *James DiCarlo*\\n  - *Liam Paninski* - decoding\\n  - *Jack Gallant* - v4, fmri\\n  - *Bin Yu* - model consistency\\n  - *Sebastian Seung* - connectomes\\n  - *Surya Ganguli* - Stanford\\n  - *David Cox* - MIT/IBM\\n  - *Jascha Sohl-Dickstein* - Google\\n  - [Iain Couzin](https://scholar.google.com/citations?user=dbBW62EAAAAJ&hl=en)\\n  - Haim Sompolinksy\\n  - charlest gilbert - rockefeller - studies spatial distribution of visual cortex\\n  - *Susumu Tonegawa* - memory',\n",
       " \"---\\nlayout: notes\\ntitle: kernels\\ncategory: research\\n---\\n\\n#  kernels\\n\\nAn introduction to kernels and recent research.\\n\\n## kernel basics\\n\\n- basic definition\\n  - continuous\\n  - symmetric\\n  - PSD Gram matrix ($K_n = XX^T$)\\n  \\n- [list of kernels](http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/#spline)\\n\\n- [kernels wiki](https://en.wikipedia.org/wiki/Kernel_method#cite_note-4): kernel memorizes points then uses dists between points to classify\\n\\n- [learning deep kernels](https://arxiv.org/pdf/1811.08357v1.pdf)\\n\\n- [learning data-adaptive kernels](https://arxiv.org/abs/1901.07114)\\n\\n- [kernels that mimic dl](https://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf)\\n\\n- [kernel methods](http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdfs)\\n\\n- [wavelet support vector machines](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.412.362&rep=rep1&type=pdf) - kernels using wavelets\\n\\n  \\n\\n### ch 4 from [support vector machines book](https://link.springer.com/book/10.1007/978-0-387-77242-4)\\n\\n- 4.1 - what is a valid kernel\\n  - in general, most dot-product like things constitute valid kernels\\n  - a function is a kernel iff it is a symmetric, positive definite function\\n    - this refers to the $n$ x $n$ matrix with entries $f(x_{row}-x_{col})$ being a psd matrix\\n  - a given kernel can have many feature spaces (can construct different feature spaces that yield the same inner products)\\n- 4.2 **reproducing kernel hilbert space (RKHS)** of a kernel\\n  - hilbert space - abstract vector space with (1) an inner product and (2) is complete (i.e. enough limits so calculus works)\\n  - the RKHS is the smallest feature space of a kernel, and can serve as a canonical feature space\\n    - RKHS - a $\\\\mathbb K$-hilbert space that consists of *functions* mapping from X to $\\\\mathbb K$\\n    - every RKHS has a unique reproducing kernel\\n    - every kernel has a unique RKHS\\n  - sums/products of kernels also work\\n\\n## kernels in deep learning\\n\\n- [To understand deep learning we need to understand kernel learning](https://arxiv.org/abs/1802.01396) - overfitted kernel classifiers can still fit the data well\\n- original kernels (neal 1994) + (lee et al. 2018) + (matthews et al. 2018)\\n  - infinitely wide nets and only top layer is trained\\n  - corresponds to kernel $\\\\text{ker}(x, x') = \\\\mathbb E_{\\\\theta \\\\sim W}[f(\\\\theta, x) \\\\cdot f(\\\\theta, x')]$, where $W$ is an intialization distr. over $\\\\theta$\\n- [neural tangent kernel](https://arxiv.org/abs/1806.07572) (jacot et al. 2018)\\n  - $\\\\text{ker}(x, x') = \\\\mathbb E_{\\\\theta \\\\sim W}[\\\\left < \\\\frac{f(\\\\theta, x)}{\\\\partial \\\\theta} \\\\cdot \\\\frac{f(\\\\theta, x')}{\\\\partial \\\\theta} \\\\right> ]$ - evolution of weights over time follows this kernel\\n    - with very large width, this kernel is the NTK at initialization\\n    - stays stable during training (since weights don't change much)\\n  - at initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit\\n    - evolution of an ANN during training can also be described by a kernel (kernel gradient descent)\\n  - different types of kernels impose different things on a function (e.g. want more / less low frequencies)\\n    - gradient descent in kernel space can be convex if kernel is PD (even if nonconvex in the parameter space)\\n  - [understanding the neural tangent kernel](https://arxiv.org/pdf/1904.11955.pdf) (arora et al. 2019)\\n\\n    - method to compute the kernel quickly on a gpu\\n- [Scaling description of generalization with number of parameters in deep learning](https://arxiv.org/abs/1901.01608) (geiger et al. 2019)\\n  - number of params = N\\n  - above 0 training err, larger number of params reduces variance but doesn't actually help\\n    - ensembling with smaller N fixes problem\\n  - the improvement of generalization performance with N in this classification task originates from reduced variance of fN when N gets large, as recently observed for mean-square regression\\n- [On the Inductive Bias of Neural Tangent Kernels](https://arxiv.org/abs/1905.12173) (bietti & mairal 2019)\\n- [Kernel and Deep Regimes in Overparametrized Models](https://arxiv.org/abs/1906.05827) (Woodworth...Srebro 2019)\\n  \\n  - transition between *kernel* and *deep regimes*\\n\\n## kernel papers\\n\\n- [data spectroscopy paper](https://arxiv.org/pdf/0807.3719) (shi et al. 2009)\\n  - kernel matrix $K_n$\\n  - Laplacian matrix $L_n = D_n - K_n$\\n    - $D_n$ is diagonal matrix, with entries = column sums\\n  - block-diagonal kernel matrix would imply a cluster\\n    - eigenvalues of these kernel matrices can identify clusters (and are invariant to permutation)\\n    - want to look for data points corresponding to same/similar eigenvectors\\n  - hard to know what kernel to use, how many eigenvectors / groups look at\\n  - here, look at population point of view - realted dependence of spectrum of $K_n$ on the data density function: $K_Pf(x) = \\\\int K(x, y) f(y) dP(y)$\\n\\n## spectral clustering\\n\\n- interested in top eigenvectors of $K_n$ and bottom eigenvectors of $L_n$\\n- scott and longuet-higgins - embed data in space of top eigenvectors, normalize in that space, and investigate block structure\\n- perona and freeman - 2 clusters by thresholding top eigenvector\\n- shi & malik - normalized cut: threshold second smallest generalize eigenvector of $L_n$\\n- similarly we have kernel PCA, spectral dimensionality reduction, and SVMs (which can be viewed as fitting a linear classifier in the eigenspace of $K_n$)\",\n",
       " '---\\nlayout: notes\\ntitle: info retrieval\\ncategory: cs\\n---\\n\\n#  info retrieval\\n\\nSome notes on information retrieval, based on UVA\"s Info Retrieval course.\\n\\n## introduction\\n- building blocks of search engines\\n    - search (user initiates)\\n    - reccomendations - proactive search engine (program initiates e.g. pandora, netflix)\\n    - information retrieval - activity of obtaining info relevant to an information need from a collection of resources\\n    - information overload - too much information to process\\n    - memex - device which stores records so it can be consulted with exceeding speed and flexibility (search engine)\\n- IR pieces\\n    1. Indexed corpus (static)\\n        - crawler and indexer - gathers the info constantly, takes the whole internet as input and outputs some representation of the document\\n            - web crawler - automatic program that systematically browses web\\n        - document analyzer - knows which section has what\\n            -takes in the metadata and outputs the index (condensed), manage content to provide efficient access of web documents\\n    2. User\\n        - query parser - parses the search terms into managed system representation\\n    3. Ranking\\n        - ranking model\\n            -takes in the query representation and the indices, sorts according to relevance, outputs the results\\n        - also need nice display\\n        - query logs - record user\\'s search history\\n        - user modeling - assess user\\'s satisfaction\\n- steps ![](../assets/ir_architecture.png) \\n    1. repository -> document representation \\n    2. query -> query representation\\n    3. ranking is performed between the 2 representations and given to the user\\n    4. evaluation - by users\\n- information retrieval:\\n    1. reccomendation\\n    2. question answering\\n    3. text mining\\n    4. online advertisement\\n    \\n## related fields \\n*they are all getting closer, database approximate search and information extraction converts unstructed data to structured:*\\n\\ndatabase systems        | information retrieval\\n- | \\nstructured data         | unstructured data\\nsemantics are well-defined |  semantics are subjective\\nstructured query languages (ex. SQL) | simple keyword queries\\nexact retrieval         | relevance-drive retrieval\\nemphasis on efficiency  | emphasis on effectiveness\\n\\n- natural language processing - currently the bottleneck\\n    - deep understainding of language\\n    - cognitive approaches vs. statistical\\n    - small scale problems vs. large\\n- developing areas\\n    - currently mobile search is big - needs to use less data, everything needs to be more summarized\\n    - interactive retrieval - like a human being, should collaborate\\n- core concepts\\n    - *information need* - desire to locate and obtain info to satisfy a need\\n    - *query* - a designed representation of user\\'s need\\n    - *document* - representation of info that could satisfy need\\n    - *relevance* - relatedness between documents and need, this is vague\\n        - multiple perspectives: topical, semantic, temporal, spatial (ex. gas stations shouldn\\'t be behind you)\\n- Yahoo used to have system where you browsed based on structure (browsing), but didn\\'t have queries (querying)\\n    - better when user doesn\\'t know keywords, just wants to explore\\n    - push mode - systems push relevant info to users without a query\\n    - pull mode - users pull out info using keywords\\n',\n",
       " '---\\nlayout: notes\\ntitle: python ref\\ncategory: cs\\n---\\n\\n* TOC\\n#  python ref\\n\\n## basic\\n\\n- from math import log, log2, floor, ceil\\n- % is modulus\\n- from random import random\\n  - `random.random() ## [0, 1.0)`\\n- copying: x.copy() for list, dict, numpy\\n  - this is still shallow (won\\'t recursively copy things like a list in a list)\\n    - this does work for basic things though (the copy is not exactly the same as the original) \\n  - for generic objects, need to do from copy import deepcopy\\n\\n## data structures\\n\\n```python\\n- list l (use this for stack as well) ## implemented like an arraylist\\n\\t- l.append(x)\\n    - l.insert(index, element)\\n  - l.pop()\\n  - [\\'x\\'] + [\\'y\\'] = [\\'x\\', \\'y\\']\\n  - [True] * 5\\n- queue: from collections import deque ## implemented as doubly linked list\\n      - q = deque()\\n      - q.append(x)\\n      - q.pop()\\n      - q.popleft()\\n      - q.appendleft(x)\\n      - index like normal\\n      - len(q)\\n- class Node: ## this implements a linkedlist\\n\\t\\tdef __init__(self, val, next):\\n\\t\\t\\tself.val = val\\n\\t\\t\\tself.next = next\\n- set()\\n\\t- add(x)\\n  - remove(x)\\n  - intersection(set2)\\n- dict {\\'key\\': 3}\\n\\t- keys()\\n\\t- values()\\n    - del m[\\'key\\']\\n- PriorityQueue\\n\\t- from queue import PriorityQueue\\n\\t- q = PriorityQueue()\\n    - q.put(x)\\n    - q.get()\\n- from collections import Counter\\n\\t- Counter(Y_train) ## this counts unique values and makes it into a dict of counts\\n```\\n## useful\\n\\n*strings*\\n\\n```python\\n- s = \\'test\\', \\n- s.upper() ## convert to all upper case\\n- s[::-1] ## reverse the str\\n- \"_\".join([s, s]) ## fastest way to join lots of strings (with _ between them)\\n- s.split(\"e\") ## split into a list wherever there is an e\\n- s.replace(\"e\", \"new_str\") ## replaces all instances\\n- s.find(\"t\") ## returns first index, otherwise -1\\n- formatting\\n\\t- \"%05d\"\\t//pad to fill 5 spaces\\n\\t- \"%8.3f\" //max number of digits\\n\\t- \"%-d\"\\t//left justify\\n\\t- \"%,d\" \\t//print commas ex. \"1,000,000\"\\n  - d (int), f (float), s (str)\\n\\t- print(f\"{x:05d}\") ## new in 3.6\\n- int(\"3\") = 3\\n- bin(10) = \\'0b1010\\'\\n- hex(100) = \\'0x64\\'\\n- ord(\\'a\\') = 97\\n- \\'x\\' * 3 = \\'xxx\\'\\n```\\n\\n*sorting*\\n\\n```python\\nl = [\\'abc\\', \\'ccc\\', \\'d\\', \\'bb\\']\\n- sorted(l, reverse=False, key=len) ## decreasing order\\n\\t- key examples: str.lower, func_name\\n    - key = lambda x: x[1]\\n    - def func_name(s):\\n     \\t return s[-1]\\n- l.sort(reverse=False, key=len) ## sorts in place\\n```\\n\\n*exceptions*\\n```python\\ntry:\\n    something...\\nexcept ValueError as e:\\n    print(\\'error!\\', e)\\n\\nraise Exception(\\'spam\\', \\'eggs\\')\\nassert(x == 3)\\n```\\n\\n## higher level\\n\\n- *primitives*: `int, float, bool, str`\\n- only primitive and reference *types*\\n  - when you assign primitives to each other, it\\'s fine\\n  - when you pass in a primitive, its value is copied\\n  - when you pass in an object, its reference is copied\\n    - you can modify the object through the reference, but can\\'t change the object\\'s address\\n\\n## object-oriented\\n\\n```python\\n## example of a class\\nclass Document:\\n    def __init__(self, name):    \\n        self.name = name\\n \\n    def show(self):             \\n        raise NotImplementedError(\"Subclass must implement abstract method\")\\n\\n## example of inheritance\\nclass Pdf(Document):\\n    def show(self):\\n        return \\'Show pdf contents!\\'\\n\\n## example of different types of methods\\nclass MyClass:\\n    def method(self): ## can modify self (and class)\\n        return \\'instance method called\\', self\\n\\n    @classmethod\\n    def classmethod(cls): ## can only modify class\\n        return \\'class method called\\', cls\\n\\n    @staticmethod\\n    def staticmethod(): #can\\'t modify anything\\n        return \\'static method called\\'\\n```\\n\\n## numpy/pandas\\n\\n- loc indexes by val\\n- iloc indexes by index position\\n- .groupby returns a dict\\n- merging\\n  - pd.merge(df1, df2, how=\\'left\\', on=\\'x1\\')',\n",
       " '---\\nlayout: notes\\ntitle: Data Structures\\ncategory: cs\\n---\\n\\n#  data structures\\n\\nSome notes on advanced data structures, based on UVA\\'s \"Program and Data Representation\" course.\\n\\n## lists\\n### arrays and strings\\n- start by checking for null, length 0\\n- *ascii* is 128, extended is 256\\n\\n### queue - linkedlist\\n- has insert at back (enqueue) and remove from front (dequeue)\\n```java\\nclass Node {\\n\\tNode next;\\n\\tint val;\\n\\tpublic Node(int d) { val = d; } \\n}\\n```\\n- finding a loop is tricky, use visited\\n- reverse a linked list\\n  - requires 3 ptrs (one temporary to store next)\\n  - return pointer to new end\\n\\n### stack\\n```java\\nclass Stack { \\n\\tNode top;\\n\\tNode pop() {\\n\\t\\tif (top != null) {\\n\\t\\t\\tObject item = top.data; \\n\\t\\t\\ttop = top.next;\\n\\t\\t\\treturn item;\\n\\t\\t}\\n\\t\\treturn null;\\n\\t}\\n\\tvoid push(Object item) { \\n\\t\\tNode t = new Node(item);\\n\\t\\tt.next = top;\\n\\t\\ttop = t;\\n\\t} \\n}\\n```\\n- sort a stack with 2 stacks\\n  - make a new stack called ans\\n  - pop from old\\n  - while old element is > ans.peek(), old.push(ans.pop())\\n  - then new.push(old element)\\n- stack with min - each el stores min of things below it\\n- queue with 2 stacks - keep popping everything off of one and putting them on the other\\n- sort with 2 stacks\\n\\n## trees\\n- Balanced binary trees are generally logarithmic\\n    - Root: a node with no parent; there can only be one root\\n    - Leaf: a node with no children\\n    - Siblings: two nodes with the same parent\\n    - Height of a node: length of the longest path from that node to a leaf\\n       - Thus, all leaves have height of zero\\n        - Height of a tree: maximum depth of a node in that tree = height of the root\\n    - Depth of a node: length of the path from the root to that node\\n    - Path: sequence of nodes n1, n2, ..., nk such that ni is parent of ni+1 for 1 ≤ i ≤ k\\n    - Length: number of edges in the path\\n    - Internal path length: sum of the depths of all the nodes\\n- Binary Tree - every node has at most 2 children\\n- Binary Search Tree - Each node has a key value that can be compared\\n    - Every node in left subtree has a key whose value is less than the root\\'s key value\\n    - Every node in right subtree has a key whose value is greater than the root\\'s key value\\n\\n```java\\nvoid BST::insert(int x, BinaryNode * & curNode){    //we pass in by reference because we want a change in the method to actually modify the parameter (the parameter is the curNode *)\\n    //left associative so this is a reference to a pointer\\n    if (curNode==NULL)\\n        curNode = new BinaryNode(x,NULL,NULL);\\n    else if(x<curNode->element)\\n        insert(x,curNode->left);\\n    else if(x>curNode->element)\\n        insert(x,curNode->right);\\n}\\n```\\n- BST Remove\\n  - if no children: remove node (reclaiming memory), set parent pointer to null\\n        - one child: Adjust pointer of parent to point at child, and reclaim memory\\n        - two children: successor is min of right subtree\\n            - replace node with successor, then remove successor from tree\\n    - worst-case depth = n-1 (this happens when the data is already sorted)\\n    - maximum number of nodes in tree of height h is 2^(h+1) - 1\\n    - minimum height h ≥ log(n+1)-1\\n    - Perfect Binary tree - impractical because you need the perfect amount of nodes\\n        - all leaves have the same depth\\n        - number of leaves 2^h\\n### AVL Tree\\n- For every node in the tree, the height of the left and right sub-trees differs at most by 1\\n    - guarantees log(n)\\n    - balance factor := The height of the right subtree minus the height of the left subtree\\n    - \"Unbalanced\" trees: A balance factor of -2 or 2\\n    - AVL Insert - needs to update balance factors\\n    - same sign -> single rotation\\n    - -2, -1 -> needs right rotation\\n    - -2, +1 -> needs left then right\\n    - Find: Θ(log n) time: height of tree is always Θ(log n)\\n    - Insert: Θ(log n) time: find() takes Θ(log n), then may have to visit every node on the path back up to root to perform up to 2 single rotations\\n    - Remove: Θ(log n): left as an exercise\\n    - Print: Θ(n): no matter the data structure, it will still take n steps to print n elements\\n\\n### Red-Black Trees\\n- definition\\n    1. A node is either red or black\\n    2. The root is black\\n    3. All leaves are black\\n        The leaves may be the NULL children\\n    4. Both children of every red node are black\\n        Therefore, a black node is the only possible parent for a red node\\n    5. Every simple path from a node to any descendant leaf contains the same number of black nodes\\n- properties\\n    - The height of the right and left subtree can differ by a factor of n\\n    - insert (Assume node is red and try to insert)\\n        1. The new node is the root node\\n        2. The new node\\'s parent is black\\n        3. Both the parent and uncle (aunt?) are red\\n        4. Parent is red, uncle is black, new node is the right child of parent\\n        5. Parent is red, uncle is black, new node is the left child of parent\\n    - Removal\\n        - Do a normal BST remove\\n            - Find next highest/lowest value, put it\\'s value in the node to be deleted, remove that highest/lowest node\\n                - Note that that node won\\'t have 2 children!\\n            - We replace the node to be deleted with it\\'s left child\\n                - This child is N, it\\'s sibling is S, it\\'s parent is P\\n### Splay Trees\\n- A self-balancing tree that keeps \"recently\" used nodes close to the top\\n  - This improves performance in some cases\\n  - Great for caches\\n  - Not good for uniform access\\n- Anytime you find / insert / delete a node, you splay the tree around that node\\n- Perform tree rotations to make that node the new root node\\n- Splaying is Θ(h) where h is the height of the tree\\n    - At worst this is linear time - Θ(n)\\n    - We say it runs in Θ(log n) amortized time - individual operations might take linear time, but other operations take almost constant time - averages out to logarithmic time\\n        - m operations will take m*log(n) time\\n\\n### other trees\\n- to go through *bst (without recursion) in order*, use stacks\\n  - push and go left\\n  - if can\\'t go left, pop\\n    - add new left nodes\\n      - go right\\n- *breadth-first tree*\\n  - recursively print only at a particular level each time\\n  - create pointers to nodes on the right\\n- *balanced tree*  = any 2 nodes differ in height by more than 1\\n  - (maxDepth - minDepth) <=1\\n- *trie* is an infix of the word “retrieval” because the trie can find a single word in a dictionary with only a prefix of the word\\n  - root is empty string\\n  - each node stores a character in the word\\n  - if ends, full word\\n    - need a way to tell if prefix is a word -> each node stores a boolean isWord\\n\\n## heaps\\n- used for *priority queue*\\n- peek(): just look at the root node\\n- add(val): put it at correct spot, percolate up\\n  - percolate - Repeatedly exchange node with its parent if needed\\n  - expected run time: ∑i=1..n 1/2^n∗n=2\\n- pop(): put last leaf at root, percolate down\\n  - Remove root (that is always the min!)\\n  - Put \"last\" leaf node at root\\n  - Repeatedly find smallest child and swap node with smallest child if needed.\\n- Priority Queue - Binary Heap is always used for Priority Queue\\n    1. insert\\n        - inserts with a priority\\n    2. findMin\\n        - finds the minimum element\\n    3. deleteMin\\n        - finds, returns, and removes minimum element\\n- perfect (or complete) binary tree - binary tree with all leaf nodes at the same depth; all internal nodes have 2 children.\\n    - height h, 2h+1-1 nodes, 2h-1 non-leaves, and 2h leaves\\n- Full Binary Tree\\n    - A binary tree in which each node has exactly zero or two children.\\n- Min-heap - parent is min\\n    1. Heap Structure Property: A binary heap is an almost complete binary tree, which is a binary tree that is completely filled, with the possible exception of the bottom level, which is filled left to right.\\n        - in an array - this is faster than pointers \\n            - left child: 2*i\\n            - right child: (2*i)+1\\n            - parent: floor(i/2)\\n            - pointers need more space, are slower\\n            - multiplying, dividing by 2 are very fast\\n    2. Heap ordering property: For every non-root node X, the key in the parent of X is less than (or equal to) the key in X. Thus, the tree is partially ordered.\\n- Heap operations\\n    - findMin: just look at the root node\\n    - insert(val): put it at correct spot, percolate up\\n        - percolate - Repeatedly exchange node with its parent if needed\\n        - expecteed run time: ∑i=1..n 1/2^n∗n=2\\n    - deleteMin: put last leaf at root, percolate down\\n        - Remove root (that is always the min!)\\n        - Put \"last\" leaf node at root\\n        - Repeatedly find smallest child and swap node with smallest child if needed.\\n- Compression\\n    - Lossless compression: X = X\\'\\n    - Lossy compression: X != X\\'\\n        - Information is lost (irreversible)\\n    - Compression ratio: $\\\\vert X\\\\vert /\\\\vert Y\\\\vert $\\n        - Where $\\\\vert X\\\\vert $ is the number of bits (i.e., file size) of X\\n- Huffman coding\\n    - Compression\\n        1. Determine frequencies\\n        2. Build a tree of prefix codes\\n            - no code is a prefix of another code\\n            - start with minheap, then keep putting trees together\\n        3. Write the prefix codes to the output\\n        4. reread source file and write prefix code to the output file\\n    - Decompression\\n        1. read in prefix code - build tree\\n        2. read in one bit at a time and follow tree\\n- ASCII characters - 8 bits, 2^7 = 128 characters\\n    - cost - total number of bits\\n    - \"straight cost\" - bits / character = log2(numDistinctChars)\\n- Priority Queue Example\\n    - insert (x)\\n    - deleteMin()\\n    - findMin()\\n    - isEmpty()\\n    - makeEmpty()\\n    - size()\\n\\n## Hash tables\\n\\n- java: load factor = .75, default init capacity: 16, uses buckets\\n- string hash function: s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1] where n is length mod (table_size)\\n    - Standard set of operations: find, insert, delete\\n    - No ordering property!\\n    - Thus, no findMin or findMax\\n    - Hash tables store key-value pairs\\n    - Each value has a specific key associated with it\\n- fixed size array of some size, usually a prime number\\n- A hash function takes in a \"thing\" )string, int, object, etc._\\n    \\n    - returns hash value - an unsigned integer value which is then mod\\'ed by the size of the hash table to yield a spot within the bounds of the hash table array\\n- Three required properties\\n    1. Must be deterministic\\n        - Meaning it must return the same value each time for the same \"thing\"\\n    2. Must be fast\\n    3. Must be evenly distributed\\n        - implies avoiding collisions\\n    - Technically, only the first is required for correctness, but the other two are required for fast running times\\n- A perfect hash function has:\\n    - No blanks (i.e., no empty cells)\\n    - No collisions\\n- Lookup table is at best logarithmic\\n- We can\\'t just make a very large array - we assume the key space is too large\\n    \\n    - you can\\'t just hash by social security number\\n- hash(s)=(∑k−1i=0si∗37^i) mod table_size\\n    \\n    - you would precompute the powers of 37\\n- collision - putting two things into same spot in hash table\\n    - Two primary ways to resolve collisions:\\n        1. Separate Chaining (make each spot in the table a \\'bucket\\' or a collection)\\n        2. Open Addressing, of which there are 3 types:\\n            1. Linear probing\\n            2. Quadratic probing\\n            3. Double hashing\\n- Separate Chaining\\n    - each bucket contains a data structure (like a linked list)\\n    - analysis of find\\n        - The load factor, λ, of a hash table is the ratio of the number of elements divided by the table size\\n            - For separate chaining, λ is the average number of elements in a bucket\\n                - Average time on unsuccessful find: λ\\n                    - Average length of a list at hash(k)\\n                - Average time on successful find: 1 + (λ/2)\\n                    - One node, plus half the average length of a list (not including the item)\\n            - typical case will be constant time, but worst case is linear because everything hashes to same spot\\n            - λ = 1\\n                - Make hash table be the number of elements expected\\n                - So average bucket size is 1\\n                - Also make it a prime number\\n            - λ = 0.75\\n                - Java\\'s Hashtable but can be set to another value\\n                - Table will always be bigger than the number of elements\\n                - This reduces the chance of a collision!\\n                - Good trade-off between memory use and running time\\n            - λ = 0.5\\n                - Uses more memory, but fewer collisions\\n- Open Addressing: The general idea with all of them is that, if a spot is occupied, to \\'probe\\', or try, other spots in the table to use\\n    - 3 Types:\\n        - General: pi(k) = (hash(k) + f(i)) mod table_size\\n          1.Linear Probing: f(i) = i\\n            - Check spots in this order :\\n                - hash(k)\\n                - hash(k)+1\\n                - hash(k)+2\\n                - hash(k)+3\\n                - These are all mod table_size\\n            - find - keep going until you find an empty cell (or get back)\\n            - problems\\n                - cannot have a load factor > 1, as you get close to 1, you get a lot of collisons\\n                - clustering - large blocks of occupied cells\\n                - \"holes\" when an element is removed\\n                  2.Quadratic:  f(i) = i^2\\n            - hash(k)\\n            - hash(k)+1\\n            - hash(k)+4\\n            - hash(k)+9\\n            - you move out of clusters much quicker\\n              3.Double hashing: i * hash2(k)\\n            - hash2 is another hash function - typically the fastest\\n            - problem where you loop over spots that are filled - hash2 yields a factor of the table size\\n                - solve by making table size prime\\n            - hash(k) + 1 * hash2(k)\\n            - hash(k) + 2 * hash2(k)\\n            - hash(k) + 3 * hash2(k)\\n    - a prime table size helps hash function be more evenly distributed\\n    - problem: when the table gets too full, running time for operations increases\\n    - solution: create a bigger table and hash all the items from the original table into the new table\\n        - position is dependent on table size, which means we have to rehash each value\\n        - this means we have to re-compute the hash value for each element, and insert it into the new table!\\n        - When to rehash?\\n            - When half full (λ = 0.5)\\n            - When mostly full (λ = 0.75)\\n                - Java\\'s hashtable does this by default\\n            - When an insertion fails\\n            - Some other threshold\\n        - Cost of rehashing\\n            - Let\\'s assume that the hash function computation is constant\\n            - We have to do n inserts, and if each key hashes to the same spot, then it will be a Θ(n2) operation!\\n            - Although it is not likely to ever run that slow\\n    - Removing\\n        - You could rehash on delete\\n        - You could put in a \\'placeholder\\' or \\'sentinel\\' value\\n            - gets filled with these quickly\\n            - perhaps rehash after a certain number of deletes\\n- has functions    \\n    - MD5 is a good hash function (given a string or file contents)\\n        - generates 128 bit hash\\n        - when you download something, you download the MD5, your computer computes the MD5 and they are compared to make sure it downloaded correctly\\n        - not reversible because when a file has more than 128 bits, won\\'t be 1-1 mapping\\n        - you can lookup a MD5 hash in a rainbow table - gives you what the password probably is based on the MD5 hash\\n    - SHA (secure Hash algorithm) is much more secure\\n        - generates hash up to 512 bits',\n",
       " '---\\nlayout: notes\\ntitle: software engineering\\ncategory: cs\\n---\\n\\n#  software engineering\\n\\nSome notes on software engineering, based on two courses at UVA.\\n\\n## ch 2 - software engineering\\n- software engineering - the application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; that is, the application of engineering to software\\n- SWEBOK guide\\n- Polya\\n\\t1.\\tUnderstand the problem (communication and analysis).\\n\\t2.\\tPlan a solution (modeling and software design).\\n\\t3.\\tCarry out the plan (code generation).\\n\\t4.\\tExamine the result for accuracy (testing and quality assurance).\\n\\n1. *tools* - provide automated or semi-automated support\\n2. *methods* - provide technical how-tos\\n\\t- ex: communication, requirements analysis, design modeling, program construction, testing and support\\n3. *process model* - framework for delivering technology\\n\\t- framework activities\\n\\t\\t- communication, planning, modeling, construction, deployment\\n\\t- umbrella activities\\n\\t\\t- software project tracking and control, risk management,\\nsoftware quality assurance, technical reviews, measurement, software configuration management, reusability management, work product preparation and production\\n4. a *quality focus* - bedrock that supports software engineering\\n- every software has some business need\\n\\n## ch 3 - software process structure\\n- *task set* - defines the actual work to be done to accomplish the objectives of a software engineering action\\n- *process pattern* - template - describes process-related problem, suggests solutions\\n\\t- *stage patterns* - defines problems associated with a framework activity\\n\\t- *task patterns*\\n\\t- *phase patterns* - define sequence of framework activities\\n- process assessment and improvement\\n\\t- SCAMPI, CBAIPI, SPICE, ISO 9001:2000 for software\\n\\n## ch 4 - process models\\n- *prescriptive models* - advocated orderly approach\\n\\t- *waterfall model*\\n\\t\\t- everything is in order\\n\\t\\t- communication, planning, modeling, construction, deployment\\n\\t- *V model*\\n\\t\\t- move down the model and then test each step on the way up\\n\\t- *incremental model*\\n\\t\\t- increments (like versions are produced)\\n- *evolutionary models* - use prototyping, users may think prototype is actual model\\n\\t- *spiral* - continually do each step of waterfall\\n\\t- *concurrent* - different parts have different interations\\n- others\\n\\t- *component based* - when using outside code\\n\\t\\t- the process to apply when reuse is a development objective\\n\\t- *formal* - when safety is needed\\n\\t- *aspect-oriented* - construct aspects - new paradigm\\n\\t- *unified process* - use-case driven, works well with UML\\n- personal and team software processes\\n\\n## ch 5 - agile development\\n- driven by customer descriptions\\n- adapts as changes occur\\n- delivers multiple software increments\\n- self-organizing teams\\n- at regular intervals, the team reflects on how to become more effective\\n- *agile unified process* - each AUP iteration addresses several activies\\n\\t- modeling, implementation, testing, deployment,...\\n- *agile modeling* - set of modeling principles\\n\\t- model with a purpose\\n\\t- use multiple models\\n\\t- adapt locally\\n1. *extreme programming (xp)* - most widely used agile process\\n\\t- begins with *user stories*\\n\\t- each one gets a cost\\n\\t- grouped together for deliverable increment\\n\\t- CRC - class responsibility collaborator\\n\\t- commitment is made on delivery date\\n\\t- project velocity defines subsequent delivery dates\\n\\t- encourages unit tests, pair programming\\n\\t- acceptance tests defined by the user\\n2. *industrial xp* - greater inclusion of management, customers\\n\\t- readiness assesment, project community, project chartering, test driven management, retrospectives, continuous learning\\n3. *scrum* - development work is partition into \"packets\"\\n\\t- testing and documentation are on-going\\n\\t- work occurs in *sprints* and derived from a *backlog*\\n\\t- meetings are short\\n\\t- demos are delivered to the customer\\n4. *dynamic systems development method* - similar to xp\\n\\n## ch 6 - human aspects\\n- roles\\n\\t- ambassador – represents team to outside constituencies\\n\\t- scout – crosses team boundaries to collect information\\n\\t- guard – protects access to team work products\\n\\t- sentry – controls information sent by stakeholders\\n\\t- coordinator – communicates across the team and organization \\n- teams\\n\\t- sense of purpose, involvement, trust, improvement\\n\\t- diversity of skill sets\\n- *organizational paradigms*\\n\\t- closed paradigm — structure by authority\\n\\t- random paradigm - loose, depends on individual initiatives\\n\\t- open paradigm - mix of closed and random\\n\\t- synchronous paradigm - compartmentalizes problem\\n- some collaboration tools\\n\\t- slack\\n\\t- gforge\\n\\t- onedesk\\n\\t- rational team concert\\n- collaboration, coordination, communication\\n\\t\\n## ch 7 - principles\\n- process principles\\n\\t- be agile, effective team, communicate, assess risk, quality focus, \\n- practice principles\\n\\t- divide and conquer, transfer of information, abstraction, patterns, maintenance\\n- communication principles\\n\\t- gather customer requirements\\n- planning principles\\n\\t- understand scope, be iterative\\n- modeling principles\\n\\t- *UML diagrams*\\n\\t- *requirements models* - *analysis models* - represent customer requirements in information domain, functional domain, behavioral domain\\n\\t- *design models* - represent characteristics of software that helps construct it effectively - ex. like an architect\\'s plans for a house\\n- preparation, construction, testing, deployment\\n- The deployment activity encompasses 3 actions:\\n\\t- Delivery\\n\\t- Support\\n\\t- Feedback \\n\\n## ch 8 - understanding requirements\\n1. inception - first questions\\n2. elicitation - requirements understood\\n3. elaboration \\n4. negotiation\\n5. specification - maybe in models\\n6. validation - products are assessed\\n7. requirements managing\\n- quality function deployment - determines the customer satisfaction of each requirement\\n- requirement types - functional  describes what software should do, while non-functional place constraints on how\\n\\t- Functional Requirement:  A system must send an email whenever a certain condition is met (e.g. an order is placed, a customer registers, etc.\\n\\t- Nonfunctional Requirement: Emails should be sent with a latency of no greater than 12 hours from such an activity\\n- use-cases - collection of user scenarios that describe the thread of usage of a system\\n\\t- each described from point-of-view of an actor\\n- analysis model - provides description for computer-based system\\n\\t- scenario-based elements - functional and use-cases\\n\\t- class-based elements - implied by scenarios\\n\\t- behavioral elements - state diagram\\n\\t- flow-oriented elements - data flow diagram\\n- state diagram - represents the behavior of a system by depicting its states and the events that cause the system to change state\\n- *UML diagrams*\\n\\t- case diagrams\\n\\t- activity diagrams\\n\\t- class diagrams\\n\\t\\n## ch 9 - requirements modeling  - scenario-based\\n- analysis modeling - focuses on what not how\\n- requirements modeling must achieve 3 objectives\\n\\t1. to describe what the customer requires\\n\\t2. basis for creation of a software design\\n\\t3. define a set of requirements that can be validated\\n- domain analysis - define the domain to be investigated\\n\\t- analyze applications in the domain\\n- scenario-based modeling\\n\\t- *use-case* - a scenario that describes a thread of usage\\n\\t\\t- *actors* - represent roles people or devices play as the system functions\\n\\t\\t- *users* - can play a number of different roles for a given scenario\\n\\t- exceptions - situations that cause the system to exhibit unusual behavior\\n- uml models\\n\\t- *activity diagrams* – supplements the use case by providing a graphical representation of the flow of interaction within a specific scenario.\\n\\t- *swimlane diagrams* - is a useful variation of the activity diagram and allows you to represent the flow of activities described by the use case at the same time indicate which actor or analysis class has responsibility for the action described by an activity rectangle\\n\\n## ch 10 - requirements modeling - class-based methods\\n1. *structured analysis* - considers data and the processes that transform the data as separate entities\\n2. *object-oriented analysis* - focuses on definition of classes\\n- class-based modeling - *objects, operations, relationships, collaborations*  \\n- processing narrative - describes overall description of the function to be developed - nouns underlined\\n- *attributes* - describe a class that has been selected for inclusion in the analysis model\\n- operations - define the behavior of an object\\n- *class-responsibility-collaborator (CRC) modeling* - provides simple means for making classes that are relevant to requirements\\n\\t- like index cards with class, responsibilities, collaborators on each card\\n- class types\\n\\t- entity classes - extracted directly from problem statement\\n\\t- boundary classes - used to create the interface\\n\\t- controller classes - manage a unit of work from start to finish\\n- collaborations - three different generic relationships\\n\\t- associations\\n\\t\\t- is-part-of\\n\\t\\t- has-knowledge-of\\n\\t- dependencies\\n\\t\\t- depends-upon\\n- plus - public visible, minus - private\\n\\n## ch 11 - requirements modeling - behavior models\\n- behavioral modeling - depicts the system states and its classes\\n\\t- indicates how software will respond to external events or stimuli\\n- state representations\\n\\t- passive state - current status of attributes\\n\\t- active state - current status of object as it undergoes continuing transformation\\n\\t- event - occurrence that cuases the system to exhibit some behavoir\\n\\t- action - process that occurs as a consequence of making a transaction\\n- semantic analysis pattern - describes set of use cases for application\\n1. *content Analysis* - The full spectrum of content to be provided by the WebApp is identified,  including text, graphics and images, video, and audio data. Data modeling can be used to identify and describe each of the data objects. \\n2. *interaction model*\\n\\t- use-cases\\n\\t- sequence diagrams\\n\\t- state diagrams\\n\\t- user interface prototypes\\n3. *functional model* - describes operations that will be applied to manipulate content and describes other processing functions that are independent of content but necessary to the end user\\n4. *configuration model* - environment and infrastructure in which the app resides\\n\\t- server-side and client-side\\n5. *navigation modeling* - defines overall navigation strategy for the app',\n",
       " \"---\\nlayout: notes\\ntitle: quantum\\ncategory: cs\\ntypora-copy-images-to: ../assets\\n---\\n\\n#  quantum\\n\\nSome *very limited* notes on quantum computing\\n\\n\\n- what does physics tell us about the limits of computers?\\n- NP - can check soln in polynomial time\\n- NP-hard - if solved, solves every NP\\n- NP-complete - NP hard and in NP\\n- *church-turing thesis* $\\\\implies$ turing machine polynomial time should be best possible in the universe\\n  - physics could allow us to do better than a turing machine\\n- examples\\n  - glass plates with soapy water - forms minimum steiner tree\\n  - can get stuck in local optimum\\n  - ex. protein folding\\n  - ex. relativity computer\\n    - leave computer on earth, travel at speed of light for a while, come back and should be done\\n    - if you want exponential speedup, need to get exponentially close to speed of light (requires exponential energy)\\n  - ex. zeno's computer - run clock faster (exponentially more cooling = energy)\\n\\n## basics\\n- An n-bit computer has 2^n states and is in one of them with probability 1.  You can think of it as having 2^n coefficients, one of which is 0 and the rest of which are 1.  Operations on it are multiplying these coefficients by stochastic matrices.  Only produces n bits of info.\\n- an n-qubit quantum computer is described by 2^n complex coefficients.  The sum of their squares sums to 1.  It’s 2^n complex coefficients must be multiplied by unitary matrices (they preserve that the sum of the squares add up to 1.)\\n- Problem: **Decoherence** – results from interaction with the outside world\\n- Properties: \\t\\n\\t- Superposition – an object is in more than one state at once\\n\\t\\t- Has a percentage of being in both states\\n\\t- Entanglement – 2 particles behave exactly the opposite – instantly\\n\\n## storing qubits\\n- Fullerenes – naturally found in Precambrian rock, reasonable for storing qubits – can store \\n\\t- not developed, but some experiments have shown ability to store qubits for milliseconds\\n\\n\\n## intro\\n\\n- probability with minus signs\\n- *amplitudes* - used to calculate probabilites, but can be negative / complex\\n\\n![](../assets/double_slit.png)\\n\\n- applications\\n  - quantum simulation\\n  - also could factor integers in polynomial time (shor 1994)\\n  - scaling up is hard because of *decoherence*= interaction between cubits and outside world\\n  - error-correcting codes can make it so we can still work with some decoherence\\n- algorithms\\n  - paths that lead to wrong answer - quantum amplitudes cancel each other out\\n  - for right answer, quantum amplitudes in phase (all positive or all negative)\\n  - prime factorization is NP but not NP complete\\n  - unclear that quantum can solve all NP problems\\n  - *Grover's algorithm* - with quantum computers, something like you can only use sqrt of number of steps\\n  - *adiabatic optimization* - like quantum simulated annealing, maybe can solve NP-complete problems\\n- dwave - company made ~2000 cubit machine\\n  - don't maintain coherence well\\n  - algorithms for NP-complete problems may not work\\n  - hope: *quantum tunneling* can get past local maximum in polynomial time maybe\\n    - empircally unclear if this is true\\n- quantum supremacy - getting quantum speedup for something, maybe not something useful\\n\\n## maxwell's demon\\n\\n- second law of thermodynamics: entropy is always increasing\\n- hot things transfer heat to cold things\\n  - temperature is avg kinetic energy - particles follow a diistribution of temperature\\n- separate 2 samples (one hot, one cold) with insulator\\n  - **idea**: demon makes all fast particles go to hot side, all slow particles go to slow side - **this is against entropy**\\n  - demon controls door between the samples\\n  - ![](../assets/demon.png)\\n    - demon opens door whenever high temperature particle comes from cold sample, then closes\\n    - demon opens door for slow particles from hot sample, then closes\\n- problem: demon has to track all the particles (which would generate a lot of heat)\\n\\n## quantum probability\\n\\n- based on this [blog post](https://www.math3ma.com/blog/a-first-look-at-quantum-probability-part-1)\\n\\n  - marginal prob. loses information but we don't need to\\n- ![Screen Shot 2019-08-17 at 10.36.26 AM](../assets/matrix_prob.png)\\n  \\n  \",\n",
       " '---\\nlayout: notes\\ntitle: Algorithms\\ncategory: ai\\n---\\n#  algorithms\\n\\nSome notes on algorithms following the book [Introduction to algorithms](https://en.wikipedia.org/wiki/Introduction_to_Algorithms) and based on UVA\\'s course.\\n\\n## asymptotics\\n\\n- Big-O\\n    - big-oh: O(g): functions that grow no faster than g - upper bound, runs in time less than g\\n        - $f(n) \\\\leq c\\\\cdot g(n)\\u200b$ for some c, large n\\n        - set of functions s.t. there exists c,k>0, 0 ≤ f(n) ≤ c*g(n), for all n > k\\n    - big-theta: Θ(g): functions that grow at the same rate as g\\n        - big-oh(g) and big-theta(g) - asymptotic tight bound\\n    - big-omega: Ω(g): functions that grow at least as fast as g\\n        - f(n)≥c*g(n) for some c, large n\\n    - Example: f = 57n+3\\n        - O(n^2) - or anything bigger\\n        - Θ(n)\\n        - Ω(n^.5) - or anything smaller\\n        - input must be positive\\n        - We always analyze the worst case run-time\\n    - little-omega: omega(g) - functions that grow faster than g\\n    - little-o: o(g) - functions that grow slower than g\\n        - we write f(n) ∈ O(g(n)), not f(n) = O(g(n))\\n    - They are all reflexive and transitive, but only Θ is symmetric.\\n        - Θ defines an equivalence relation.\\n- add 2 functions, growth rate will be $O(max(g_1(n)+g_2(n))$ (same for sequential code)\\n- recurrence thm:$ f(n) = O(n^c) => T(n) = 0(n^c)$\\n  - $T(n) = a*T(n/b) + f(n)$\\n  - $c = log_b(a)$\\n- over bounded number of elements, almost everything is constant time\\n\\n## recursion\\n\\n- moving down/right on an NxN grid - each path has length (N-1)+(N-1)\\n  - we must move right N-1 times\\n  - ans = (N-1+N-1 choose N-1)\\n  - for recursion, if a list is declared outside static recursive method, it shouldn\\'t be static\\n- *generate permutations* - recursive, add char at each spot\\n- think hard about the base case before starting \\n  - look for lengths that you know\\n  - look for symmetry\\n- n-queens - one array of length n, go row by row\\n\\n## dynamic programming\\n```java\\n//returns max value for knapsack of capacity W, weights wt, vals val\\nint knapSack(int W, int wt[], int val[])\\nint n = wt.length;\\nint K[n+1][W+1];\\n//build table K[][] in bottom up manner\\nfor (int i = 0; i <= n; i++)\\n   for (int w = 0; w <= W; w++)\\n\\t   if $(i==0 \\\\vert \\\\vert  w==0)$ // base case\\n\\t\\t   K[i][w] = 0;\\n\\t   else if (wt[i-1] <= w) //max of including weight, not including\\n\\t\\t   K[i][w] = max(val[i-1] + K[i-1][w-wt[i-1]], K[i-1][w]);\\n\\t   else //weight too large\\n\\t\\t   K[i][w] = K[i-1][w];\\nreturn K[n][W];\\n```\\n\\n## min-cut / max-cut\\n\\n## hungarian\\n\\n- assign N things to N targets, each with an associated cost\\n\\n## max-flow\\n\\n- A list of pipes is given, with different flow-capacities. These pipes are connected at their endpoints. What is the maximum amount of water that you can route from a given starting point to a given ending point?\\n\\n## sorting\\n- you can assume w.l.o.g. all input numbers are unique\\n- sorting requires Ω(nlog n) (proof w/ tree)\\n  - considerations: worst case, average, in practice, input distribution, stability (order coming in is preserved for things with same keys), in-situ (in-place), stack depth, having to read/write to disk (disk is much slower), parallelizable, online (more data coming in)\\n- adaptive - changes its behavior based on input (ex. bubble sort will stop)\\n\\n### comparised-based\\n\\n#### bubble sort\\n\\n- keep swapping adjacent pairs\\n\\n```java\\nfor i=1:n-1\\n\\tif a[i+1]<a[i]\\n\\t\\tswap(a,i,i+1)\\n```\\n- have a flag that tells if you did no swaps - done\\n- number of passes ~ how far elements are from their final positions\\n- O(n^2)\\n\\n#### odd-even sort\\n- swap even pairs\\n- then swap odd pairs\\n- parallelizable\\n\\n#### selection sort\\n- move largest to current position\\n   for i=1:n-1\\n   \\t\\tfor j=1:n\\n   \\t\\t\\tx = max(x,a[j])\\n   \\t\\t\\tjmax = j\\n   \\t\\tswap(a,i,j)\\n- 0(n^2)\\n\\n#### insertion sort\\n- insert each item into lists\\n   for i=2:n\\n   \\t\\tinsert a[i] into a[1..(i-1)]\\n   \\t\\tshift\\n- O(n^2), O(nk) where k is max dist from final position\\n- best when alsmost sorted\\n\\n#### heap sort\\n- insert everything into heap\\n- kepp remove max\\n- can do in place by storing everything in array\\n- can use any height-balanced tree instead of heap\\n  - traverse tree to get order\\n  - ex. B-tree: multi-rotations occur infrequently, average O(log n) height\\n- 0(n log n)\\n\\n#### smooth sort\\n- adaptive heapsort\\n- collection of heaps (each one is a factor larger than the one before)\\n- can add and remove in essentially constant time if data is in order\\n\\n#### merge sort\\n- split into smaller arrays, sort, merge\\n- T(n) = 2T(n/2) + n = 0(n log n) \\n- stable, parallelizable (if parallel, not in place)\\n\\n#### quicksort\\n- split on pivot, put smaller elements on left, larger on right\\n- O(n log n) average, O(n^2) worst\\n- O(log n) space\\n\\n#### shell sort\\n- generalize insertion sort\\n- insertion-sort all items i apart where i starts big and then becomes small\\n  - sorted after last pass (i=1)\\n- O(n^2), O(n^(3/2), ... unsure what complexity is\\n  - no matter what must be more than n log n\\n- not used much in practice\\n\\n### not comparison-based\\n\\n#### counting sort \\n- use values as array indices in new sort\\n- keep count of number of times at each index\\n- for specialized data only, need small values\\n- 0(n) time, 0(k) space\\n\\n#### bucket sort\\n- spread data into buckets based on value\\n- sort the buckets\\n- O(n+k) time\\n- buckets could be trees\\n\\n#### radix sort\\n- sort each digit in turn\\n- stable sort on each digit\\n  - like bucket sort d times\\n- 0(d*n time), 0(k+n) space\\n\\n#### meta sort\\n- like quicksort, but 0(nlogn) worst case\\n- run quicksort, mergesort in parallel\\n  - stop when one stops\\n- there is an overhead but doesn\\'t affect big-oh analysis\\n- ave, worst-cast = O(n log n)\\n\\n### sorting overview\\n- in exceptional cases insertion-sort or radix-sort are much better than the generic QuickSort / MergeSort / HeapSort answers.\\n- merge a and b sorted - start from the back\\n\\n## searching\\n- binary sort can\\'t do better than linear if there are duplicates\\n- if data is too large, we need to do external sort (sort parts of it and write them back to file)\\n- write binary search recursively\\n    - use low<= val and high >=val so you get correct bounds\\n    - binary search with empty strings - make sure that there is an element at the end of it\\n- \"a\".compareTo(\"b\") is -1 \\n- we always round up for these\\n- finding minimum is Ω(n)\\n  - pf: assume an element was ignored, that element could have been minimum\\n  - simple algorithm - keep track of best so far\\n  - thm: n/2 comparisons are necessary because each comparison involves 2 elements\\n  - thm: n-1 comparisons are necessary - need to keep track of knowledge gained\\n    - every non-min element must win atleast once (move from unkown to known)\\n- find min and max\\n  - naive solution has 2n-2 comparison\\n  - pairwise compare all elements, array of maxes, array of mins = n/2 comparisons\\n    - check min array, max array = 2* (n/2-1)\\n  - 3n/2-2 comparisons are sufficient (and necessary)\\n    - pf: 4 categories (not tested, only won, only lost, both)\\n    - not tested-> w or l =n/2 comparisons\\n    - w or l -> both = n/2-1\\n    - therefore 3n/2-2 comparisons necessary\\n- find max and next-to-max\\n  - thm: n-2 + log(n) comparisons are sufficient\\n  - consider elimination tournament, pairwise compare elements repeatedly\\n    - 2nd best must have played best at some point - look for it in log(n)\\n- selection - find ith largest integer\\n  - repeatedly finding median finds ith largest\\n  - finding median linear yields ith largest linear\\n    - T(n) = T(n/2) + M(n) where M(n) is time to find median\\n  - quickselect - partition around pivot and recur\\n    - average time linear, worst case O(n^2)\\n- median in linear time - quickly eliminate a constant fraction and repeat\\n  - partition into n/5 groups of 5 \\n    - sort each group high to low\\n    - find median of each group\\n    - compute median of medians recursively\\n    - move groups with larger medians to right\\n      - move groups with smaller medians to left\\n    - now we know 3/10 of elements larger than median of medians\\n      - 3/10 of elements smaller than median of medians\\n    - partition all elements around median of medians \\n      - recur like quickselect\\n      - guarantees each partition contains at most 7n/10 elements\\n    - T(n) = T(n/5) + T(7n/10) + O(n) -> f(x+y)≥f(x)+f(y)\\n    - T(n) ≤ T(9n/10) + O(n) -> this had to be less than T(n)\\n\\n## computational geometry\\n- range queries\\n  - input = n points (vectors) with preprocessing\\n  - output - number of points within any query rectangle\\n  - 1D \\n    - range query is a pair of binary searches\\n    - O(log n) time per query\\n    - O(n) space, O(n log n) preprocessing time\\n  - 2D\\n    - subtract out rectangles you don\\'t want\\n    - add back things you double subtracted\\n    - we want rectangles anchored at origin\\n  - nD\\n    - make regions by making a grid that includes all points\\n    - precompute southwest counts for all regions - different ways to do this - tradeoffs between space and time\\n    - O(log n) time per query (after precomputing) - binary search x,y\\n- polygon-point intersection\\n  - polygon - a closed sequence of segments\\n  - simple polygon - has no intersections\\n  - thm (Jordan) - a simple polygon partitions the plane into 3 regions: interior, exterior, boundary\\n  - convex polygon - intersection of half-planes\\n  - polytope - higher-dimensional polygon\\n  - raycasting \\n    - intersections = odd - interior, even - exterior\\n    - check for tangent lines, intersecting corners\\n    - O(n) time per query, O(1) space and time\\n  - convex case\\n    - preprocessing \\n      - find an interior point p (pick a vertext or average the vertices)\\n      - partition into wedges (slicing through vertices) w.r.t. p\\n      - sort wedges by polar angle\\n    - query\\n      - find containing wedge (look up by angle)\\n      - test interior / exterior\\n        - check triangle - cast ray from p to point, see if it crosses edge\\n    - O(log n) time per query (we binary search the wedges)\\n    - O(n) space and O(n log n) preprocessing time\\n  - non-convex case\\n    - preprocessing\\n      - sort vertices by x\\n      - find vertical slices\\n      - partition into trapezoids (triangle is trapezoid)\\n      - sort slice trapezoids by y\\n    - query\\n      - find containing slice\\n      - find trapezoid in slice\\n      - report interior/ exterior\\n    - O(log n) time per query (two binary searches)\\n    - O(n^2) space and O(n^2) preprocessing time\\n- convex hull\\n  - input: set of n points\\n  - output: smallest containing convex polygon\\n  - simple solution 1 - Jarvis\\'s march \\n  - simple solution 2 - Graham\\'s scan\\n  - mergehull\\n    - partition into two sets - computer MergeHull of each set\\n    - merge the two resulting CHS\\n      - pick point p with least x\\n      - form angle-monotone chains w.r.t p\\n      - merge chains into angle-sorted list\\n      - run Graham\\'s scan to form CH\\n    - T(n) = 2T(n/2) + n = 0(n log n)\\n    - generalizes to higher dimensions\\n    - parallelizes\\n  - quickhull (like quicksort)\\n    - find right and left-most points\\n      - partition points along this line\\n      - find points farthest from line - make quadrilateral\\n        - eliminate all internal points\\n      - recurse on 4 remaining regions\\n      - concatenate resulting CHs\\n    - O(n log n) expected time\\n    - O(n^2) worst-case time - ex. circle\\n    - generalizes to higher dim, parallelizes\\n  - lower bound - CH requires Ω(n log n) comparisons\\n    - pf - reduce sorting to convex hull\\n    - consider arbitrary set of x_i to be sorted\\n    - raise the x_i to the parabola (x_i,x_i^2) - could be any concave function\\n    - compute convex hull of the parabola - all connected and line on top\\n    - from convex hull we can get sorted x_i => convex hull did sorting so at least n log n comparisons\\n    - corollary - Graham\\'s scan is optimal\\n  - Chan\\'s convex hull algorithm\\n    - assume we know CH size m=h\\n    - partitoin points into n/m sets of m each\\n- convex polygon diameter\\n- Voronoi diagrams - input n points - takes O(nlogn) time to compute\\n  - problems that are solved\\n    - Voronoi cell - the set of points closer to any given point than all others form a convex polygon\\n    - generalizes to other metrics (not just Euclidean distance)\\n    - a Voronoi cell is unbounded if and only if it\\'s point is on the convex hull\\n      - corollary - convex hull can be computed in linear time\\n    - Voronoi diagram has at most 2n-5 vertices and 3n-6 edges\\n    - every nearest neighbor of a point defines an edge of the Voronoi diagram\\n      - corollary - all nearest neighbors can be computed from the Voronoi diagram in linear time\\n      - corollary - nearest neighbor search in O(log n) time using planar subdivision search (binary search in 2D)\\n    - connection points of neighboring Voronoi diagram cells form a triangulation (Delanuay triangulation)\\n    - a Delanuay triangulation maximizes the minimum angle over all triangulations - no long slivery triangles\\n      - Euclidean minimum spanning tree is a subset of the Delanuay triangulation (can be computed easily)\\n  - calculating Voronoi diagram\\n    - discrete case / bitmap - expand breadth-first waves from all points \\n      - time is O(bitmap size)\\n      - time is independent of #points\\n    - intersecting half planes\\n      - Voronoi cell of a point is intersection of all half-planes induced by the perpendicular bisectors w.r.t all other points\\n      - use intersection of convex polygons to intersect half-planes (nlogn time per cell)\\n    - can be computed in nlogn total time\\n      1. idea divide and conquer\\n      - merging is complex\\n      2. sweep line using parabolas ',\n",
       " '---\\nlayout: notes\\ntitle: Graphs\\ncategory: cs\\n---\\n\\n#  graphs\\n\\nSome notes on computer science graphs.\\n\\n- Edges are of the form (v1, v2)\\n    - Can be ordered pair or unordered pair\\n- Definitions\\n    - A *weight* or cost can be associated with each edge - this is determined by the application\\n    - w is adjacent to v iff (v, w) $\\\\in$ E\\n    - path: sequence of vertices w1, w2, w3, ..., wn such that (wi, wi+1) ∈ E for 1 ≤ i < n\\n    - length of a path: number of edges in the path\\n    - simple path: all vertices are distinct\\n    - cycle:\\n        - directed graph: path of length $\\\\geq$ 1 such that w1 = wn\\n        - undirected graph: same, except all edges are distinct\\n    - connected: there is a path from every vertex to every other vertex\\n    - loop: (v, v) $\\\\in$ E\\n    - complete graph: there is an edge between every pair of vertices\\n- digraph\\n    - directed acyclic graph: no cycles; often called a \"DAG\"\\n    - strongly connected: there is a path from every vertex to every other vertex\\n    - weakly connected: the underlying undirected graph is connected\\n- For Google Maps, an adjacency matrix would be infeasible - almost all zeros (sparse)\\n    - an adjacency list would work much better\\n    - an adjacency matrix would work for airline routes\\n- detect cycle\\n  - dfs from every vertex and keep track of visited, if repeat then cycle\\n- Topological Sort\\n    - Given a directed acyclic graph, construct an ordering of the vertices such that if there is a path from vi to vj, then vj appears after vi in the ordering\\n    - The result is a linear list of vertices\\n    - indegree of v: number of edges (u, v) -- meaning the number of incoming edges\\n    - Algorithm\\n        - start with something of indegree 0\\n        - take it out, and take out the edges that start from it\\n        - keep doing this as we take out more and more edges\\n    - can have multiple possible topological sorts\\n- Shortest Path\\n    - single-source - start somewhere, get shortest path to everywhere\\n    - unweighted shortest path - breadth first search\\n    - Weighted Shortest Path\\n        - We assume no negative weight edges\\n        - Djikstra\\'s algorithm: uses similar ideas as the unweighted case\\n        - Greedy algorithms: do what seems to be best at every decision point\\n        - Djikstra: v^2\\n            - Initialize each vertex\\'s distance as infinity\\n            - Start at a given vertex s\\n                - Update s\\'s distance to be 0\\n            - Repeat\\n                - Pick the next unknown vertex with the shortest distance to be the next v\\n                - If no more vertices are unknown, terminate loop\\n            - Mark v as known\\n            - For each edge from v to adjacent unknown vertices w\\n                - If the total distance to w is less than the current distance to w\\n                - Update w\\'s distance and the path to w\\n            - It picks the unvisited vertex with the lowest-distance, calculates the distance through it to each unvisited neighbor, and updates the neighbor\\'s distance if smaller. Mark visited (set to red) when done with neighbors.\\n    - Shortest path from a start node to a finish node\\n        - 1. We can just run Djikstra until we get to the finish node\\n        - 2. Have different kinds of nodes\\n            - Assume you are starting on a \"side road\"\\n            - Transition to a \"main road\"\\n            - Transition to a \"highway\"\\n            - Get as close as you can to your destination via the highway system\\n            - Transition to a \"main road\", and get as close as you can to your destination\\n            - Transition to a \"side road\", and go to your destination\\n- Traveling Salesman\\n    - Given a number of cities and the costs of traveling from any city to any other city, what is the least-cost round-trip route that visits each city exactly once and then returns to the starting city?\\n    - Hamiltonian path: a path in a connected graph that visits each vertex exactly once\\n    - Hamiltonian cycle: a Hamiltonian path that ends where it started\\n    - The traveling salesperson problem is thus to find the least weight Hamiltonian path (cycle) in a connected, weighted graph\\n- Minimum Spanning Tree\\n    - Want fully connected\\n    - Want to minimize number of links used\\n        - We won\\'t have cycles\\n    - Any solution is a tree\\n    - Slow algorithm: Construct a spanning tree:\\n        - Start with the graph\\n        - Remove an edge from each cycle\\n        - What remains has the same set of vertices but is a tree\\n        - Spanning Trees\\n- Minimal-weight spanning tree: spanning tree with the minimal total weight\\n    - Generic Minimum Spanning Tree Algorithm\\n        - KnownVertices <- {}\\n        - while KnownVertices does not form a spanning tree, loop:\\n            - find edge (u,v) that is \"safe\" for KnownVertices\\n            - KnownVertices <- KnownVertices U {(u,v)}\\n        - end loop\\n    - Prim\\'s algorithm\\n        - Idea: Grow a tree by adding an edge to the \"known\" vertices from the \"unknown\" vertices. Pick the edge with the smallest weight.\\n        - Pick one node as the root,\\n        - Incrementally add edges that connect a \"new\" vertex to the tree.\\n        - Pick the edge (u,v) where:\\n        - u is in the tree, v is not, AND\\n        - where the edge weight is the smallest of all edges (where u is in the tree and v is not)\\n        - Running time: Same as Dijkstra\\'s: Θ(e log v)\\n    - Kruskal\\'s algorithm\\n        - Idea: Grow a forest out of edges that do not create a cycle. Pick an edge with the smallest weight.\\n        - When optimized, it has the same running time as Prim\\'s and Dijkstra\\'s: Θ(e log v)\\n        - unoptomized: v^2',\n",
       " '```r\\nx %%2 ## modulus\\nx <- 3 ## assignment\\nclass(x) checks the class of x\\nrm(list=ls())\\n```\\n- *vectors*\\n\\t- numeric_vector <- c(1, 2, 3)\\n\\t- poker_vector <- c(140, -50, 20, -120, 240)\\n\\t- names(poker_vector) <- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\\n\\t\\t- assigns names to elements of poker vector\\n- *matrices*\\n\\t- matrix(1:9, byrow = TRUE, nrow = 3)\\n\\t- can name the rows / cols\\n\\t- 1-indexed\\n\\t- has slicing\\n\\t- dim(m) prints dimensions\\n- *factor* - data type for storing categorical variable\\n- *data frame* - when you want different types of data\\n\\t- columns are variables, rows are observations\\n- *lists* - ordered, can hold any data type\\n\\t- length(list)',\n",
       " '---\\nlayout: notes\\ntitle: os\\ncategory: cs\\n---\\n* TOC\\n#  os\\n\\n## 1 - introduction\\n### 1.1 what operating systems do\\n- *computer system* - hierarchical approach = layered approach\\n\\t1. hardware\\n\\t2. operating system\\n\\t3. application programs\\n\\t4. users\\n- views\\n\\t1. user view - OS maximizes work user is performing\\n\\t2. system view\\n\\t\\t- os allocates resources - CPU time, memory, file-storage, I/O\\n\\t\\t- os is a *control program* - manages other programs to prevent errors\\n- program types\\n\\t1. os is the *kernel* - one program always running on the computer\\n\\t\\t- only kernel can access resources provided by hardware\\n\\t2. system programs - associated with OS but not in kernel\\n\\t3. application programs\\n- *middleware* - set of software frameworks that provide additional services to application developers\\n\\n### 1.2 computer-system organization\\n- when computer is booted, needs *bootstrap program*\\n\\t- initializes things then loads OS\\n\\t- also launches *system processes*\\n\\t\\t- ex. Unix launches \"init\"\\n- events\\n\\t- hardware signals with *interrupt*\\n\\t- software signals with *system call*\\n\\t- *interrupt vector* holds addresses for all types of interrupts\\n\\t- have to save address of interrupted instruction\\n- memory\\n\\t- *von Neumman architecture* - uses instruction register\\n\\t- main memory is *RAM*\\n\\t\\t- *volatile* - lost when power off\\t\\n\\t- *secondary storage* is non-volatile (ex. hard disk)\\n\\t- *ROM* is unwriteable so static programs like bootstrap are ROM\\n\\t- access\\n\\t\\t1. uniform memory access (*UMA*)\\n\\t\\t2. non-uniform memory access (*NUMA*)\\n- I/O\\n\\t- *device driver* for I/O devices\\n\\t- *direct memory access (DMA)* - transfers entire blocks of data w/out CPU intervention\\n\\t\\t- otherwise device controller must move data to its local buffer and return pointer to that\\n- multiprocessor systems\\n\\t1. increased throughput\\n\\t2. economies of scale (costwise)\\n\\t3. increased reliability (fault tolerant)\\n\\n### 1.3 computer-system architecture \\n1. single-processor system - one main cpu\\n\\t- usually have special-purpose processors (e.g. keyboard controller)\\n2. *multi-processor system* / *multicore system*\\n\\t- *multicore* means multi-processor on same chip\\n\\t\\t- multicore is generally faster\\n\\t- multiple processors in close communication\\n\\t- advantages\\n\\t\\t- increased throughput\\n\\t\\t- economy of scale\\n\\t\\t- increased reliability = *graceful degradation* = *fault tolerant*\\n\\t- types\\n\\t\\t1. *asymmetric multiprocessing* - boss processor controls the system\\n\\t\\t2. *symmetric multiproccesing (SMP)* - each processor performs all tasks\\n\\t\\t\\t- more common\\n\\t\\t3. *blade server* - multiple independence multiprocessor systems in same chassis\\n3. *clustered system* - multiple loosely coupled cpus\\n\\t- types\\n\\t\\t1. *asymmetric clustering* - one machine runs while other monitors it (*hot-standby mode*)\\n\\t\\t2. *symmetric clustering* - both run something\\n\\t- parallel clusters\\n\\t\\t- require *disributed lock manager* to stop conflicting parallel operations\\n\\t\\t- can share same data via *storage-area-networks*\\n\\t- *beowulf cluster* - use ordinary PCs to make cluster\\n\\n### 1.4 operating-system structure\\n- *multiprogramming* - increases CPU utilization so CPU is always doing something\\n\\t- keeps *job pool* ready on disk\\n\\t- *time sharing / multitasking* - multiple jobs switch so fast that both can be interacted with\\n\\t\\t- requires an *interactive* computer system\\n\\t- *process* - program loaded into memory\\n\\t- scheduling\\n\\t\\t- *job scheduling* - picking jobs from job pool (disk -> memory)\\n\\t\\t- *CPU scheduling* - what to run first (memory -> cpu)\\n\\t- memory\\n\\t\\t- processes are *swapped* from main memory to disk\\n\\t\\t- *virtual memory* allows for execution of process not in memory\\n\\n### 1.5 operating-system operations\\n- *trap / exception* - software-generated *interrupt*\\n- *user-mode* and *kernel mode* (also called system mode)\\n\\t- when in kernel mode, *mode bit* is 0\\n\\t- separate mode for virtual machine manager (VMM)\\n\\t- this is built into hardware\\n- kernel can use a timer to getting stuck in user mode\\n\\n### 1.6 process management\\n- program is passive, process is active\\n- process needs resources\\n\\t- process is unit of work\\n- single-threaded process has one *program counter*\\n\\n### 1.7 memory management\\n- cpu can only directly read from main memory\\n- computers must keep several programs in memory\\n\\t- hardware design is impmortant\\n\\n### 1.8 storage management\\n- defines *file* as logical storage unit\\n- most programs stored on disk until loaded\\n- in addition to secondary storage, there is *tertiary storage* (like DVDs)\\n- *caching* - save frequent items on faster things\\n\\t- *cache coherency* - make sure cache coherency is properly updated with parallel processes\\n\\n### 1.9 protection & security\\n- process can execute only within its address space\\n- *protection* - controlling access to resources\\n- *security* - defends a system from attacks\\n- maintain list of *user IDs* and *group IDs*\\n\\t- can temporarily *escalate priveleges* to an *effective UID* - *setuid* command\\n\\n### 1.10 basic data structures\\n- *bitmap* - string of n binary digits\\n\\n### 1.11 computing environments\\n- *network computers* - are essentially terminals that understand web-based computing\\n- *distributed system* - shares resources among separate computer systems\\n\\t- *network* - communication path between two or more computers\\n\\t- *TCP/IP* is most common network protocol\\n- networks\\n\\t- *PAN* - personal-area network (like bluetooth)\\n\\t- *LAN* - local-area network connects computers within a room, building, or campus\\n\\t- *WAN* - wide-area network\\n\\t- *MAN* - metropolitan-area network\\n\\t- *network OS* provides features like file sharing across the network\\n\\t- *distributed OS* provides less autonomy - makes it feel like one OS controls entire network\\n- *client-server* computing\\n\\t1. *compute-server* - performs actions for user\\n\\t2. *file-server* - stores files\\n- *peer-to-peer* computing\\n\\t1. all clients w/ central lookup service, ex. Napster\\n\\t2. no centralized lookup service\\n\\t\\t- uses *discovery protocol* - puts out request and other peer must respond\\n- *virtualization* - allows OS to run within another OS\\n\\t- *interpretation* - run programs as non-native code (ex. java runs on JVM)\\n\\t- BASIC can be compiled or interpreted\\n- *cloud-computing* - computing, storage, and applications as a service accross a network\\n\\t- public cloud\\n\\t- private cloud\\n\\t- hybrid cloud\\n\\t- software as a service (SAAS)\\n\\t- platform as a service (PAAS)\\n\\t- infrastructure as a service (IAAS)\\n\\t- cloud is behind a firewall, can only make requests to it\\n- *embedded systems* - like microwaves / robots \\n\\t- specific tasks\\n\\t - have *real-time OS* - fixed time constraints\\n\\n## 2 - OS Structures\\n### 2.1 os services\\n- for the user\\n\\t- *user interface* - command-line interface and graphical user interface\\n\\t- *program execution* - load a program and run it\\n\\t- *I/O operations* - file or device\\n\\t- *File-system manipulation*\\n\\t- *communications* - between processes / computer systems\\n\\t- *error detection* \\n- for system operation\\n\\t- *resource allocation*\\n\\t- *accounting* - keeping stats on users / processes\\n\\t- *protection / security* \\n\\n### 2.2 user and os interface\\n1. *command interpreter* = *shell* - gets and executes next user-specified command\\n\\t- could contain the code to execute the command\\n\\t- command interpreter could have code to execute commands\\n\\t- more often, executes *system programs*, such as \"rm\", that are executed\\n2. *GUI*\\n\\n### 2.3 system calls\\n- *system calls* - provide an interface to os services\\n- API usually wraps system calls (ex. java)\\n\\t- *libc* - provided by Linux/Mac OS for C\\n\\t- *system-call interface* links API calls to system calls\\n- passing parameters\\n\\t1. pass parameters in registers\\n\\t2. parameters stored in block of memory and address passed in register\\n\\t3. parameters pushed onto stack\\n\\n### 2.4 system call types\\n1. process control - halting, ending\\n\\t- *lock* shared data - no other process can access until released\\n2. file manipulation\\n3. device manipulation\\n\\t- similar to file manipulation\\n4. information maintenance - time, date, dump()\\n\\t- *single step* is CPU mode which throws trap for CPU after every instruction for a debugger\\n5. communications\\n\\t1. *message-passing model*\\n\\t\\t- each computer has *host name* and *network identifier* (IP address)\\n\\t\\t- each process has *process name*\\n\\t\\t- *daemons* - system programs for receiving connections (like servers waiting for a client)\\n\\t2. *shared-memory model*\\n6. protection\\n\\n### 2.5 system programs\\n- *system programs* = *system utilities* \\n- some provide interfaces for system calls\\n- other uses\\n\\t1. file management\\n\\t2. status info\\n\\t3. file modification\\n\\t4. programming-language support\\n\\t5. program loading and execution\\n\\t6. communications\\n\\t7. background services\\n\\n### 2.6 os design and implementation\\n- *mechanism* - how to do something\\n\\t- want this to be general so only certain parameters change\\n- *policy* - what will be done\\n- os mostly in C, low-level kernel in assembly\\n\\t- high-level is easier to port but slower\\n\\n### 2.7 os structure\\n- want modules but current models aren\\'t very modularized\\n\\t- *monolithic* system has performance advantages - very little overhead\\n\\t- in practice everything is a hybrid\\n- system can be modularized with a *layered approach*\\n\\t- layers: hardware, ..., user interface\\n\\t- easy to construct and debug\\n\\t- hard to define layers, less efficient\\n- *microkernel approach* - used in os *Mach* \\n\\t- move nonessential kernel components to system / user-level\\n\\t- smaller kernel, everything communicates with *message passing*\\n\\t- makes extending os easier, but slower functions due to system overhead\\n- *loadable kernel modules*\\n\\t- more flexible - kernel modules can change\\n- examples (see pics)\\n\\n### 2.8 os debugging\\n- errors are written to *log file* and *core dump* (memory snapshot) is written to file\\n- if kernel crashes, must save its dump to s special area\\n- *performance tuning* - removing *bottlenecks*\\n\\t- monitor *trace listings* - log if interesting events with times / parameters\\n- SolarisDTrace is a tool to debug and tune the os\\n- *profiling* - periodically samples instruction pointer to determine which code is being executed\\n\\n### 2.9 generation\\n- *system generation* - configuring os on a computer\\n\\t- usually on a CD-ROM\\n\\t- lots of things must be determined (like what CPU to use)\\n\\n### 2.10 system boot\\n- bootstrap program\\n\\n## 3 - processes\\n### 3.1 process concept\\n- *process* - program in execution\\n\\t- batch system executes *jobs* = *processes*\\n\\t- time-shared system has user programs or *tasks*\\n\\t- program is passive while process is active\\n- parts\\n\\t- program code - *text section*\\n\\t- program counter\\n\\t- registers\\n\\t- stack\\n\\t- data section\\n\\t- heap\\n- same program can have many processes\\n- process can be execution environment for other code (ex. JVM)\\n- *process state*\\n\\t- new\\n\\t- running\\n\\t- waiting\\n\\t- ready\\n\\t- terminated\\n- *process control block (PCB)* = *task control block* - repository for any info that varies process to process\\n\\t- process state\\n\\t- program counter\\n\\t- CPU registers\\n\\t- CPU-scheduling information\\n\\t- memory-management information\\n\\t- accounting information\\n\\t- I/O status information\\n\\t- could include information for each thread\\n- *parent* - process that created another process\\n\\n### 3.2 process scheduling\\n- *process scheduler* - selects available process for multi-tasking\\n\\t- processes begin in *job queue*\\n\\t- processes that are ready and waiting are in the *ready queue* until they are *dispatched* - usually stored as a linked list\\n\\t- lots of things can happen here (fig 3_6)\\n\\t\\t- ex. make I/O request and go to I/O queue\\n\\t- *I/O-bound process* - spends more time doing I/O\\n\\t- *CPU-bound process* - spends more time doing computations\\n\\t- each device has a list of process waiting in its *device queue*\\n- *scheduler* - selects processes from queues\\n\\t- *long-term scheduler* - selects from processes on disk to load into memory\\n\\t\\t- controls the *degree of multiprogramming* = number of processes in memory\\n\\t\\t- has much more time than short-term scheduler\\n\\t\\t- want good mix of *I/O-bound* and *CPU-bound* processes\\n\\t\\t- sometimes this doesn\\'t exist\\n\\t- *short-term / CPU scheduler* - selects from processes ready to execute and allocates CPU to one of them\\n\\t- sometimes *medium-term scheduler*\\n\\t\\t- does *swapping* - remove a process from memory and later reintroduce it\\n- *context switch* - occurs when switching processes\\n\\t- when interrupt occurs, kernel saves *context* of old process and loads saved context of new process\\n\\t- context is in the PCB\\n\\t- might be more or less work depending on hardware\\n\\n### 3.3 operations on processes\\n- usually each process has unique process identifier (*pid*)\\n- linux everything starts with init process (pid=1)\\n- restricting a child process to a subset of the parent\\'s resources prevents system overload\\n\\t- parent may pass along initialization data\\n- after creating new process\\n\\t1. parent continues to execute concurrently with children\\n\\t2. parent waits until some or all of its children terminate\\n- two address-space possibilities for the new process:\\n\\t1. child is duplicate of parent (it has the same program and data as the parent).\\n\\t2. child loads new program\\n- forking\\n\\t- when call *fork()* continue operation but returns 0 for parent process and nonzero for child\\n\\t\\t- child is a copy of the parent\\n\\t- after fork, usually one process calls *exec()* to load binary file into memory\\n\\t\\t- overrides program, doesn\\'t return unless error occurs\\n\\t- parent can call *wait()* until child finishes (moves itself off ready queue until child finishes)\\n- on Windows, uses *CreateProcess()* which requires loading a new program rather than sharing address space\\n\\t- STARTUPINFO - \\n\\t- PROCESSINFORMATION - \\n- process termination\\n\\t- *exit()* kills process (return in main calls exit)\\n\\t- process can return status value\\n\\t- parent can terminate child if it knows its pid\\n\\t- *cascading termination* - if parent dies, its children die\\n\\t- *zombie process* - terminated but parent hasn\\'t called wait() yet\\n\\t\\t- remains because parent wants to know what exit status was\\n\\t\\t- if parent terminates without wait(), *orphan* child is assigned *init* as new parent (init periodically invokes wait())\\n\\t\\n### 3.4 interprocess communication\\n- process *cooperation*\\n\\t- information sharing\\n\\t- computation speedup\\n\\t- modularity\\n\\t- convenience\\n- *interprocess communication (IPC)* - allows exchange of data and info\\n\\t1. *shared memory* - shared region of memory is established\\n\\t\\t- one process establishes region\\n\\t\\t- other process must attach to it (OS must allow this)\\n\\t\\t- less overhead (no system calls)\\n\\t\\t- suffers from cache coherency\\n\\t\\t- ex. producer consumer\\n\\t\\t\\t- producer fills buffer and consumer empties it\\n\\t\\t\\t- *unbounded buffer* - producer can keep producing indefinitely\\n\\t\\t\\t- *bounded buffer* - consumer waits if empty, producer waits if full\\n\\t\\t\\t- in points to next free position\\n\\t\\t\\t- out points to first full position\\n\\t2. *message passing* - messages between coordinating processes\\n\\t\\t- useful for smaller data\\n\\t\\t- easier in a distributed system\\n\\t\\t1. *direct or indirect communication*\\n\\t\\t\\t- direct requires knowing the id of process to send / receive\\n\\t\\t\\t\\t- can be *asymmetrical* - need to know id of process to send to, but not receive from\\n\\t\\t\\t\\t- results in hard-coding\\n\\t\\t\\t- indirect - messages are sent / received from mailboxes\\n\\t\\t\\t\\t- more flexible, can send message to whoever shares mailbox\\n\\t\\t\\t\\t- mailbox owned by process - owner receives those messages\\n\\t\\t\\t\\t- mailbox owned by os - unclear\\n\\t\\t2. *synchronous or asynchronous* communication\\n\\t\\t\\t- synchronous = blocking\\n\\t\\t\\t- when both send and recieve are blocking = *rendezvous*\\n\\t\\t3. *automatic or explicit* buffering\\n\\t\\t\\t- queue for messages can have 3 implementations\\n\\t\\t\\t\\t1. *zero capacity (must be blocking)*\\n\\t\\t\\t\\t2. *bounded capacity*\\n\\t\\t\\t\\t3. *unbounded capacity*\\n\\n### 3.5 examples of IPC systems\\n1. POSIX - shared memory\\n2. Mach - message passing\\n3. Windows - shared memory for message passing\\n\\n### 3.6 communication in client-server systems\\n1. *sockets* - endpoint for communication\\n\\t- IP address + port number\\n\\t- connecting\\n\\t\\t1. server listens on a port\\n\\t\\t2. client creates socket and requests connection to server\\'s port\\n\\t\\t3. server accepts connection (then usually writes data to socket)\\n\\t- all ports below 1024 are well known\\n\\t- *connection-oriented*=*TCP*\\n\\t- *connectionless* = *UDP*\\n\\t- special IP address 127.0.0.1 - *loopback* - refers to itself\\n\\t- sockets are *low-level* - can only send unstructured bytes\\n2. *remote procedure calls (RPCs)* - remote message-based communication\\n\\t- like IPC, but between different computers\\n\\t- message addressed to an RPC daemon listening to a port\\n\\t- messages are well-structured\\n\\t- specifies a *port* - a number included at the start of a message packet\\n\\t\\t- system has many ports to differentiate different services\\n\\t- uses *stubs* to hide details\\n\\t\\t- they *marshal* the parameters\\n\\t\\t- might have to convert data into *external data representation (XDR)* (to avoid issues like big-endian vs. little-endian)\\n\\t- must make sure each message is acted on *exactly once*\\n\\t- client must know port\\n\\t\\t1. binding info (port numbers) may be predetermined and unchangeable\\n\\t\\t2. binding can be dynamic with rendezvous deaemon (*matchmaker*) on a fixed RPC port\\n3. *pipes* - conduit allowing 2 processes to communicate\\n\\t- four issues\\n\\t\\t1. bidirectional?\\n\\t\\t2. full duplex (data can travel in both directions at same time?) or half duplex (only one way)?\\n\\t\\t3. parent-child relationship?\\n\\t\\t4. communicate over a network?\\n\\t- *ordinary pipe* - write at one end, read at the other\\n\\t\\t- unix function `pipe(int fd[])`\\n\\t\\t\\t- fd[0] is read-end and fd[1] is write-end\\n\\t\\t- only exists while a child and parent process are communicating\\n\\t\\t\\t- therefore only on same machine\\n\\t\\t- parent and child should both close unused ends of the pipe\\n\\t\\t- on windows, called *anonymous pipes*\\n\\t\\t\\t- requires security attributes\\n\\t- *named pipe* - can be bidirectional\\n\\t\\t- called *FIFOs* in Unix\\n\\t\\t\\t- only half-duplex, requires same machine\\n\\t\\t- Windows - fulll-duplex and can be different machines\\n\\t\\t- many processes can use them\\n\\t\\t\\n## 4 - threads\\n- *thread* - basic unit of CPU utilization\\n\\t1. program counter\\n\\t2. register set\\n\\t3. stack\\n- making a thread is quicker and less resource-intensive than making a process\\n- used in RPC and kernels\\n- benefits\\n\\t1. responsiveness\\n\\t2. resource sharing\\n\\t3. economy\\n\\t4. scalability\\n\\n### 4.2 - multicore programming (skipped)\\n- *amdahl\\'s law*: $speedup \\\\leq \\\\frac{1}{S+(1-S)/N_{cores}}$\\n\\t- S is serial portion\\n- parallelism\\n\\t- *data parallelism* - distributing subsets of data across cores and performing same operation on each core\\n\\t- *task parallelism* - distribution tasks across cores\\n\\n### 4.3 - multithreading models\\n- need relationship between *user threads* and *kernel threads*\\n\\t1. *many-to-one model* - maps user-level threads to one kernel thread\\n\\t\\t- can\\'t be parallel on multicore systems\\n\\t\\t- ex. used by *Green threads*\\n\\t2. *one-to-one model*\\n\\t\\t- small overhead for creating each thread\\n\\t\\t- used by Linux and Windows\\n\\t3. *many-to-many model*\\n\\t\\t- less than or equal number of kernel threads\\n\\t\\t- *two-level model* mixes a one-to-one model and a many-to-many model\\n\\t\\t\\n### 4.4 - thread libraries\\n- *thread library* - provides programmer with an API for creating/managing threads\\n- *asynchronous* v. *synchronous* threading\\n\\n1 - POSIX Pthreads\\n\\n```\\n/* get the default attributes */\\npthread attr init(&attr);\\n/* create the thread */\\npthread create(&tid,&attr,runner,argv[1]);  // runner is a func to call\\n/* wait for the thread to exit */\\npthread join(tid,NULL);\\n```\\n- shared data is declared globally\\n\\n2 - Windows\\n\\n3 - Java\\n\\t- uses Runnable interface\\n\\n### 4.5 - implicit threading (skipped)\\n- *implicit threading* - handle threading in run-time libraries and compilers\\n\\t1. *thread pool* - number of threads at startup that sit in a pool and wait for work\\n\\t2. *OpenMP* - set of compiler directives / API for parallel programming\\n\\t\\t- identifies *parallel regions*\\n\\t\\t- uses #pragma\\n\\t3. *Grand central dispatch* - extends C\\n\\t\\t- uses *dispatch queue*\\n\\n### 4.6 - threading issues\\n- fork/exec need to know if should fork all threads / when to replace program\\n- *signal* notifies a process that a particular event has occurred\\n\\t1. has a default signal handler\\n\\t2. user-defined signal handler\\n\\t- delivering a signal to a process: `kill(pid_t pid, int signal)`\\n\\t- delivering a signal to a thread: `pthread_kill(pthread_t tid, int signal)`\\n- *thread cancellation* - terminating *target thread* before it has completed\\n\\t1. *asynchronous cancellation* - one thread immediately terminates target thread\\n\\t2. *deferred cancellation* - target thread periodically checks whether it should terminate\\n\\t- pthread_cancel(tid)\\n\\t\\t- uses deferred cancellation\\n\\t\\t- cancellation occurs only when thread reaches *cancellation point*\\n- *thread-local storage* - when threads need separate copies of data\\n- *lightweight process* = *LWP* - between user thread and kernel thread\\n- *scheduler activation* - kernel provides application with LWPs\\n\\t- *upcall* - kernel informs application about certain events\\n\\t\\n### 4.7 - linux (skipped)\\n- linux process / thread are same = task\\n- uses clone() system call\\n\\n## 5 - process synchronization\\n- *cooperating process* can effect or be affected by other executing processes\\n- ex. consumer/producer\\n\\t- if counter++ and counter-- execute concurrently, don\\'t know what will happen\\n\\t- this is a *race condition*\\n\\t\\n### 5.2 - critical-section problem\\n- each process has *critical section* where it updates common variables\\n- 3 requirements\\n\\t1. *mutual exclusion* -\\t2 processes can\\'t concurrently do critical section\\n\\t2. *progress* - things should be in critical selection\\n\\t3. *bounded waiting* - every process should eventually get to critical selection\\n- kernels\\n\\t1. *preemptive kernels*\\n\\t\\t- more responsive\\n\\t2. *nonpreemptive kernels*\\n\\t\\t- no race conditions\\n\\n### 5.3 - peterson\\'s solution\\n- *peterson\\'s solution*\\n  - not guaranteed to work\\n\\n### 5.4  - synchronization hardware\\n- *locking* - protecting critical regions using locks\\n- single-processor solution\\n\\t- prevent interrupts while shared variable is being modified\\n\\t- ex. `test_and_set()`\\n- instructions do things like swapping *atomically* - as one uninterruptable unit\\n\\t- these are basically locked instructions\\n\\t- ex. `compare_and_swap()`\\n\\t\\n### 5.5 - mutex locks\\n- *mutex*\\n\\t- simplest synchronization tool\\n\\t- this type of mutex lock is called *spinlock* because requires *busy waiting* - processes not in critical section are continuously looping\\n\\t- good when locks are short\\n\\t\\n#### 5.6 - semaphores\\n- *semaphore* S - integer variable accessed through *wait()* (like trying to execute) and *signal()* (like releasing)\\n\\t- *counting semaphore* - unrestricted domain\\n\\t- *binary sempahore* - 0 and 1\\n\\t\\n```c\\nwait(S) {\\n\\twhile(S<=0)\\n\\t\\t// busy wait\\n\\tS--;\\n}\\nsignal(S) {\\n\\tS++;\\n}\\n```\\n\\n- to improve performace, replace busy wait by process blocking itself\\n\\t- places itself into a waiting queue\\n\\t- restarted when other process executes a signal() operation\\n\\t\\n```c\\ntypedef struct{ \\n\\tint value;\\n\\tstruct process *list;\\n} semaphore;\\nwait(semaphore *S) { \\n\\tS->value--;\\n\\tif (S->value < 0)\\n\\t\\tadd this process to S->list;\\n}\\nsignal(semaphore *S) { \\n\\tS->value++;\\n\\tif (S->value <= 0){\\n\\t\\tremove a process P from S->list; \\n\\t\\twakeup(P); // resumes execution\\n\\t}\\n}\\n```\\n\\n- *deadlocked* - 2 processes are in waiting queues, can\\'t wakeup unless other process signals them\\n- *indefinite blocking=starving* - could happen if we remove processes from waiting queue in LIFO order\\n\\t- bottom never gets out\\n- *priority inversion*\\n\\t- only occurs when processes have > 2 priorities\\n\\t- usually solved with a *priority-inheritance protocol*\\n\\t\\t- when a process accesses resources needed by a higher-priority process, it inherits the higher priority until they are finished with the resources in question\\n\\t\\t\\n### 5.7 - classic synchronization problems\\n1. bounded-buffer problem\\n2. readers-writers problem\\n\\t- writers must have exclusive access\\n\\t- readers can read concurrently\\n3. dining-philosophers problem\\n\\n#### 5.8 - monitors\\n- *monitor* - highl-level synchronization construct\\n\\t- only 1 process can run at a time\\n\\t- *abstract data type* which includes a set of programmer-defined operations with mutual exlusion\\n\\t- has *condition* variables\\n\\t\\t- these can only call wait() or signal()\\n\\t\\t- when a signal is encountered, 2 options\\n\\t\\t\\t1. signal and wait\\n\\t\\t\\t2. signal and continue\\n- can implement with a semaphore\\n\\t- 1st semaphore: `mutex` - process must wait before entering and signal after leaving the monitor\\n\\t- 2nd semaphore: `next` - signaling processes use next to suspend themselves\\n\\t- 3rd semaphore: `next_count` = number of suspended processes\\n\\t\\n```\\n wait(mutex);\\n// body of F\\n\\nif (next count > 0) \\n\\tsignal(next);\\nelse\\n\\tsignal(mutex);\\n```\\n\\n- *conditional-wait* construct can help with resuming\\n\\t- `x.wait(c);`\\n\\t- *priority number* c stored with name of process that is suspended\\n\\t- when `x.signal()` is executed, process with smallest priority number is resumed next\\n\\t\\n### 5.9.4 - pthreads synchronization\\n```\\n#include <pthread.h> \\npthread mutex t mutex;\\n\\n/* create the mutex lock */ \\npthread mutex init(&mutex,NULL) // null specifies default attributes\\n\\npthread mutex lock(&mutex); // acquire the mutex lock\\n/* critical section */\\npthread mutex unlock(&mutex); // release the mutex lock\\n```\\n- these functions return 0 w/ correct operation otherwise error code\\n- POSIX specifies *named* and *unnamed* semaphores\\n\\t- name has name and can be shared by different processes\\n\\n```\\n#include <semaphore.h> sem t sem;\\n/* Create the semaphore and initialize it to 1 */ sem init(&sem, 0, 1);\\n\\n/* acquire the semaphore */ \\nsem wait(&sem);\\n\\n/* critical section */\\n\\n/* release the semaphore */ \\nsem post(&sem);\\n```\\n\\n### 5.10 - alternative approaches (skip)\\n\\n### 5.11 - deadlocks\\n- resource utilization\\n\\t1. request\\n\\t2. use\\n\\t3. release\\n- deadlock requires 4 simultaneous conditions\\n\\t1. mutual exclusion\\n\\t2. hold and wait\\n\\t3. no preemption\\n\\t4. circular wait\\n- deadlocks can be described by *system resource-allocation graph*\\n\\t- *request edge* - directed edge from process P to resource R means P has requested instance of resource type R\\n\\t- *assignment edge* - R-> P\\n\\t- if the graph has no cycles, not deadlocked\\n\\t- if cycle, possible deadlock\\n- three ways to handle\\n\\t1. use protocol to never enter deadlock\\n\\t2. enter, detect, recover\\n\\t3. ignore the problem\\n\\t\\t- developers must write code that avoids deadlocks\\n\\n\\t\\n## 7 - main memory\\n### 7.1 - background\\n- CPU can only directly access main memory and registers\\n- accessing memory is slower than registers\\n\\t\\n\\t- processor must *stall* or use *cache*\\n- processes need separate memory spaces\\n\\t1. *base register* - holds smallest usable address\\n\\t2. *limit register* - specifies size of range\\n\\t- os / hardware check these, throw a trap if there was error\\n- *input queue* holds processes waiting to be be brought into memory\\n- compiler *binds* symbolic addresses to relocatable addresses\\n\\t\\n\\t- linkage editor binds relocatable addresses to absolute addresses\\n- CPU uses *virtual address*=logical address\\n\\t- *memory-management unit (MMU)* maps from virtual to *physical address*\\n\\t\\t- simple ex. add virtual address to a process\\'s base register = *relocation register*\\n- *dynamic loading* - don\\'t load whole process, only load things when called\\n- *dynamically linked libraries* - system libraries linked to user programs when the programs are run\\n\\t\\n\\t- *stub* - tells how to load / locate library routine\\n- *shared libraries* - all use same library\\n\\n### 7.2 (skipped)\\n\\n### 7.3 - contiguous memory allocation\\n- *contiguous memory allocation* - each process has a section\\n\\t- put OS in low memory and process memory in higher\\n- *transient OS code* - not often used\\n\\t- ex. drivers\\n\\t- can remove this and change OS memory usage by decreasing val in OS limit register\\n- split mem into *partitions*\\n\\t- each partition can only have 1 process\\n\\t- *multiple-partition method* - free partitions take a new process\\n\\t- *variable-partition scheme* - OS keeps table of free mem\\n\\t\\t- all available mem = *hole*\\n\\t\\t- holes are divided between processes\\n\\t\\t\\t1. *first-fit* - allocate first hole big enough\\n\\t\\t\\t2. *best-fit* - allocate smallest hole that is big enough\\n\\t\\t\\t3. *worst-fit* - allocate largest hole (largest leftover hole)\\n\\t\\t\\t\\t- worst\\n- *external fragmentation* - there is enough free mem, but it isn\\'t contiguous\\n\\t- *50-percent rule* - 1/3 of mem is unusable\\n\\t- solved with *compaction* - shuffle mem to put free mem together\\n\\t\\t- can be expensive to move mem around\\n- *internal fragmentation* - extra mem a proc is allocated but not using (because given in block sizes)\\n- 2 types of non-contiguous solutions\\n\\t1. segmentation\\n\\t2. paging\\n\\n### 7.4 - segmentation (skip)\\n- *segments* make up logical address space\\n\\t- name (or number)\\n\\t- length\\n- logical address is a tuple\\n\\t- (segment-number, offset)\\n- *segment table*\\n\\t- each entry has *segment base* and *segment limit*\\n- doesn\\'t avoid external fragmentation\\n\\n### 7.5 - paging (skip)\\n- break physical mem into fixed-size *frames* and logical mem into corresponding *pages*\\n- CPU address = [*page number*|*page offset*]\\n\\t- *page table* contains base address of each page in physical mem\\n\\t- usually, each process gets a page table\\n- *frame table* keeps track of which frames are available / who owns them\\n- paging is prevalent\\n- avoids external fragmentation, but has internal fragmentation\\n- small page tables can be stored in registers\\n\\t- usually *page-table base register* points to page table in mem\\n\\t- has *translation look-aside buffer* - stores some page-table entries\\n\\t\\t- some entries are *wired down* - cannot be removed from TLB\\n\\t\\t- some TLBS store *address-space identifiers* (ADIDs)\\n\\t\\t\\t- identify a process\\n\\t\\t\\t- otherwise hard to contain entries for several processes\\n\\t\\t- want high *hit ratio*\\n- page-table often stores a bit for read-write or read-only\\n\\t- *valid-invalid* bit sets whether page is in a process\\'s logical address space\\n\\t- OR *page-table length register* - says how long page table is\\n- can share *reentrant code* = *pure code*\\n\\t- non-self-modifying code\\n\\t\\t\\n### 7.6 - page table structure (skip)\\n- page tables can get quite large (total mem / page size)\\n1. *hierarchical paging* - ex. two-level page table\\n  - also called *forward-mapped page table*\\n  - unused things aren\\'t filled in\\n  - for 64-bit, would generally require too many levels\\n2. *hashed page tables*\\n\\t- virtual page number is hash key -> physical page number\\n\\t- *clustered page tables* - each entry stores everal pages, can be faster\\n3. *inverted page tables*\\n\\t- only one page table in system\\n\\t- one entry for each page/frame of memory\\n\\t- takes more time to lookup\\n\\t\\t- hash table can speed this up\\n\\t- difficulty with shared memory\\n\\t\\n### 7.7-9 (skipped)\\n\\n## 6 - cpu scheduling\\n- *preemptive* - can stop and switch a process that is currently running\\n\\n#### 6.3 - algorithms\\n1. first-come, first-served\\n2. shortest-job-first\\n\\t- can be preemptive or non preemptive\\n3. priority-scheduling\\n\\t- indefinite blocking / starvation\\n4. round-robin\\n\\t- every process gets some time\\n5. multilevel queue scheduling\\n\\t- ex. foreground and background\\n6. multilevel feedback queues\\n\\t- allows processes to move between queues\\n\\t\\n#### 6.4 - thread scheduling\\n- *process contention scope* - competition for CPU takes place among threads belonging to same process\\n\\t- PTHREAD_SCOPE_PROCESS - user-level threads onto available LWPs\\n\\t- PTHREAD_SCOPE_SYSTEM - binds LWP for each user-level thread\\n\\t\\n#### 6.5 - multiple-processor scheduling\\n- asymmetric vs. symmetric\\n\\t- almost everything is symmetric (SMP)\\n\\t- *processor affinity* - try to not switch too much\\n\\t- *load balancing* - try to make sure all processes share work\\n\\t- *multithreading*\\n\\t\\t1. coarse-grained - thread executes until long-latency event, such as memory stall\\n\\t\\t2. fine-grained - switches between instruction cycle\\n\\t\\t\\n#### 6.6 - real-time systems\\n- *event latency* - amount of time that elapses from when an event occurs to when it is serviced\\n1. *interrupt latency* - period of time from the arrival of an interrupt at the CPU to the start of the routine that services the interrupt\\n2. *dispatch latency*\\n\\t1. Preemption of any process running in the kernel\\n\\t2. Release by low-priority processes of resources needed by a high-priority process\\n- *rate-monotonic* scheduling - schedules periodic tasks using a static priority policy with preemption\\n\\n### 6.7 - SKIP\\n\\n## 8 - virtual memory\\n### 8.1 - background\\n- lots of code is seldom used\\n- virtual mem allows the execution of processes that are not completely in \\n- benefits\\n\\t- programs can be larger than physical mem\\n\\t- more processes in mem at same time\\n\\t- less swapping programs into mem\\n- *sparse* address space - virtual address spaces with hole (betwen heap and stack)\\n\\n### 8.2 - demand paging\\n- *demand paging* - load pages only when they are needed\\n\\t- *lazy pager* - only swaps a page into memory when it is needed\\n\\t- can use valid-indvalid bit in page table to signal whether a page is in memory\\n- *memory resident* - residing in memory\\n- accessing page marked invalid causes *page fault*\\n  - must restart after fetching page\\n  \\t1. don\\'t let anything change while fetching\\n  \\t2. use registers to store state before fetching\\n- *pure demand paging* - never bring a page into memory until it is required\\n\\t\\n\\t- programs tend to have *locality of reference*, so we bring in chunks at a time\\n- extra time when there is a page fault\\n\\t1. service the page-fault interrupt\\n\\t2. read in the page\\n\\t3. restart the process\\n\\t- effective access time is directly proportional to *page-fault rate*\\n- *anonymous memory* - pages not associated with a file\\n\\n### 8.3 - copy-on-write\\n- *copy-on-write* - allows parent and child processes intially to share the same pages\\n\\t- if either process writes, copy of shared page is created\\n\\t- new pages can come from a set *pool*\\n- *zero-fill-on-demand* - zeroed out before being allocated\\n- *virtual memory fork* - not copy-on-write\\n\\t- child uses adress space of parent\\n\\t- parent suspended\\n\\t- meant for when child calls exec() immediately\\n\\n### 8.4 - page replacement - select which frames to replace\\n- multiprogramming might *over-allocate* memory \\n\\t\\n\\t- all programs might need all their mem at once\\n- buffers for I/O also use a bunch of mem\\n- when over-allocated, 3 options\\n\\t1. terminate user process\\n\\t2. swap out a process\\n\\t3. page replacement\\n- want lowest page-fault rate\\n- test with *reference string*, which is just a list of memory references\\n- if no frame is free, find one not being used and free it\\n- write its contents to swap space\\n- *modify bit*=*dirty bit* reduces overhead\\n\\t\\n\\t- if hasn\\'t been modified then don\\'t have to rewrite it to disk\\n- page replacement examples\\n\\t1. FIFO\\n\\t\\t\\n\\t\\t- *Belady\\'s anomaly* - for some algorithms, page-fault rate may increase as number of allocate frames increases\\n\\t2. optimal (OPT / MIN)\\n\\t\\t\\n\\t\\t- replace the page that will not be used for the longest period of time\\n\\t3. LRU - least recently used (last used)\\n\\t\\t1. implement with counters since each use\\n\\t\\t2. stack of page numbers (whenever something is used, put it on top)\\n\\t\\t- *stack algorithms* - set of pages in memory for n frames is always a subset of the set of pages that would be in memory with n + 1 frames \\n\\t\\t\\t- don\\'t suffer from Belady\\'s anomaly\\n\\t4. LRU-approximation\\n\\t\\t- *reference bit* - set whenever a page is used\\n\\t\\t- can keep *additional reference bits* by recording reference bits at regular intervals1\\n\\t\\t- *second-chance* algorithm - FIFO, but if ref bit is 1, set ref bit to 0 and move on to next FIFO page\\n\\t\\t- can have clock algorithm\\n\\t\\t- *enhanced second-chance* - uses reference bit and modify bit\\n\\t\\t\\t- give preference to pages that have been modified\\n\\t5. counting-based - count and implement LFU (least frequently used) or MFU (most frequently used)\\n- page-buffering algorithms\\n\\t- pool of free frames - makes things faster\\n\\t- list of modified pages - written to disk whenever paging device is idle\\n\\t- som algorithms, like databases perform better when they get their own memory capability called *raw disk* instead of being managed by OS\\n\\t\\n### 8.5 *frame-allocation algorithms* - how many frames to allocate to teach process in memory (skipped)\\n\\n### 8.6 - thrashing\\n- if low-priority process gets too few frames, swap it out\\n\\t- *thrashing* - process spends more time paging than executing\\n\\t\\t- CPU utilization stops increasing\\n- *local replacement algorithm* = *priority replacement algorithm* - if one process starts thrashing, cannot steal frames from another\\n\\t- *locality model* - each locality is a set of pages actively used together\\n\\t- give process enough for its current locality\\n- *working-set model* - still based on locality\\n\\t- defines *working-set window* $\\\\delta$\\n\\t- defines *working set* as pages in most recent $\\\\delta$ refs\\n\\t- OS adds / suspends processes according to working set sizes\\n\\t- approximate with fixed-interval timer\\n- *page-fault frequency* - add / decrease pages based on targe page-fault rate\\n\\n### 8.8.1 - buddy system\\n- memory allocated with *power-of-2 allocator* - requests are given powers of 2\\n\\t- each page is split into 2 *buddies* and each of those splits again recursively\\n\\t- *coalescing* - buddies can be combined quickly\\n\\t\\n\\n## 9 - mass-storage structure\\n\\n### 9.4 - disk scheduling\\n- *bandwidth* - total number of bytes transferred, divided by time\\n- first-come first-served\\n- shortest-seek-time-frist\\n- *SCAN* algorithm - disk swings side to side servicing requests on the way\\n\\t- also called elevator algorithm\\n\\t- also has circular-scan\\n\\n### 9.5 - disk management\\n- *low-level formatting* - dividing disk into sectors that *controller* can read/write\\n\\t- blocks have header / trailer with error-correcting codes\\n- *bad blocks* are corrupted - need to replace them with others = *sector sparing* = *forwarding*\\n\\t- *sector slipping* - just renumbers to not index bad blocks\\n\\n## 10 - file-system interface\\n### 10.1\\n- os maintains *open-file table*\\n- might require file locking\\n- must support different file types\\n\\n### 10.2 - access methods\\n- simplest - *sequential*\\n- *direct access* = *relative access*\\n\\t- uses relative block numbers\\n\\t\\n### 10.3\\n- disk can be partitioned\\n- two-level directory\\n\\t- users are first level\\n\\t- directory is 2nd level\\n- extend this into a tree\\n\\t- acyclic makes it faster to search\\n\\t- cycles require very slow *garbage collection*\\n- *link* - pointer to another thing\\n\\n## 11 - file-system implementation\\n\\n### 11.1\\n- *file-control block (FCB)* contains info about file ownership, etc.\\n\\n### 11.4\\n- contiguous allocation\\n- linked allocation\\n\\t- FAT\\n- indexed allocation - all the pointers in 1 block\\n\\n## 11.5\\t\\n\\n- keep track of *free-space list*\\n\\t- implemented as bit map\\n- keep track of linked list of free space\\n- *grouping* - block stores n-1 free blocks and 1 pointer to next block\\n- *counting* - keep track of ptr to next block and the number of free blocks after that\\n\\n## 12 - i/o systems\\n- *bus* - shared set of wires\\t\\n- registers\\n\\t- data-in - read by the host\\n\\t- data-out\\n\\t- status\\n\\t- control\\n- *interrupt chaining* - each element in the interrupt vector points to the had of a list of interrupt handlers\\n- system calls use software interrupt\\n- *direct memory access* - read large chunks instead of one byte at a time\\n- *device-status table*\\n- *spool* - buffer for device (ex. printer) that can\\'t hold interleaved data',\n",
       " '---\\nlayout: notes\\ntitle: Architecture\\ncategory: cs\\n---\\n\\n* TOC\\n#  architecture\\n\\n## units\\n- we will use only the i versions (don\\'t have to write i):\\n\\t- K - 10^3: Ki - 1024\\n\\t- M - 10^6: Mi - 1024^2\\n\\t- G - 10^9: Gi - 1024^3\\n- convert to these: 2^27 = 128M\\n- log(8K)=13\\n- hardware is parallel by default\\n- amdahl\\'s law: tells you how much of a speedup you get\\n    - S = 1 / (1-a+a/k)\\n    - a-portion optimized, k-level of parallelization, S-total speedup\\n\\t\\tif you really want performance increase in java, allocate a very large array, then keep track of it on your own\\t\\n\\t\\n## numbers\\n- 0x means hexadecimal\\n- 0 means octal\\n- bit - stores 0 or 1\\n- byte - 8 bits - 2 hex digits\\n- integer - almost always 32 bits\\n- \"Big-endian\": most significant first (lowest address) - how we thnk\\n  \\n  - 1000 0000 0000 0000 = 2^5 = 32768\\n- \"Little-endian\": most significant last (highest address) - this is what computers do\\n  - 1000 0000 0000 0000 = 2^0 = 1\\n  - Note that although all the bits are reversed, usually it is displayed with just the bytes reversed\\n- Consider 0xdeadbeef\\n  - On a big-endian machine, that\\'s 0xdeadbeef\\n  - On a little-endian machine, that\\'s 0xefbeadde\\n  - 0xdeadbeef is used as a memory allocation pattern by some OSes\\n- Representing integers\\n  - Sign and magnitude - first digit specifies sign\\n  - One\\'s complement - encode using n-1 bits, flip if negative\\\\\\n  - Two\\'s complement - encode using n-1 bits, flip if negative, add 1\\n    - only one representation for 0\\n      - maximum: 2^(n-1) - 1\\n      - minimum: - 2^(n-1)\\n        - flip everything to the left of the rightmost 1\\n- Floating point - like scientific notation\\n  - 3.24 * 10 ^ -6\\n  - Mantissa - 3.24 - between 1 and the base (10)\\n    - For binary, the mantissa must be between 1 and 2 \\n    - we assume the base is 2\\n  - 32 bits are split as follows:\\n    - bit 1: sign bit, 1 means negative (1 bit)\\n    - bits 2-9: exponent (8 bits)\\n    - Exponent values:\\n      - 0: zeros\\n      - 1-254: exponent-127\\n      - 255: infinities, overflow, underflow, NaN\\n      - bits 10-32: mantissa (23 bits)\\n      - mantissa=1.0+∑(i=1:23)(b^i/2^i) //we don\\'t encode the 1. because it has to be there\\n            value=(1−2∗sign)∗(1+mantissa)∗2^(exponent−127)\\n    - The largest float has:\\n      - 0 as the sign bit (it\\'s positive)\\n        - 254 as the exponent (1111 1110)\\n        - 255 is reserved for infinities and overflows\\n        - That exponent is 254-127 = 127\\n        - All 1\\'s for the mantissa\\n          - Which yields almost 2\\n                    2 * 2^127 = 2^128 = 3.402823 * 10^38    //actually a little bit lower\\n                Minimum positive:\\n                    1 * 2^-126 = 2^-126 = 1.175494 x 10^-38 //this is exact\\n                Floating point numbers are not spatially uniform\\n                    Depending on the exponent, the difference between two successive numbers is not the same\\n            union class - converts from one data type to another //when you write one field, it overrides the other\\n                \\n\\n    union foo {  //this converts a float to hex\\n      float f;\\n      int *x;\\n    } bar;\\n    int main() {\\n        bar.f = 42.125;\\n        cout << bar.x << endl; // this outputs as 0x42288000 (it is now converted to hex)\\n    }                          // if you were to dereference it, bad things would happen\\nNever compare floating point numbers - even if you print them out, they might be stored internally\\nAny fraction that doesn\\'t have a power of 2 as the denominator will be repeating\\n                       // C++ (need to #include <math.h> and compile with -lm)\\ndefine EPSILON 0.000001\\nbool foo = fabs (a-b) < EPSILON;\\nYou could use a rational or use more digits\\n64-bit: 11 exponent bits\\n    offset=1023\\nCowlishaw encoding: use 3 bits to store a binary digit - inefficient\\n\\t\\n\\n\\u200b\\t\\n## x86\\nAssembly Language - assembler translates text into machine code\\nx86 is the type of chip\\n8 registers, although you can\\'t use 2 (stack pointer and base pointer)\\n    they are all 32 bit registers\\n1 byte = 8 bits\\nDeclare variables with 3 things:\\n    identifier, how big it is, and value\\n    doesn\\'t give you a type\\n    ? means uninitialized\\n    x\\tDD\\t \\t1, 2, 3  //declares arr with 3, 4-byte integers\\n    y\\tTIMES 8 DB\\t0    //declares 8 bytes all with value 0\\nnasm assumes you are using 32 bits\\nmov <dest>, <src>\\n    more like copy\\n    Where dest and src can be:\\n        A register\\n        A constant\\n        Variable name\\n        Pointer: [ebx]\\n    always put square brackets around variable\\n    you can ADD up to two registers, add one constant, and premultipy ONE register by 2,4,or 8\\nThe destination cannot be a constant (would overwrite the constant)\\nYou cannot access memory twice in one instruction\\n    not enough time to do that at clock speed\\nStack starts at the end of memory and goes backwards\\n    when you push onto it, it\\'s actually at a lower index\\n    ESP points to most recently pushed item\\n    push\\n        First decrements ESP (stack pointer) by 4 (stack grows down)\\n        push (mov) operand onto stack (4 bytes - we make this assumption, not always true)\\n    pop\\n        Pop top element of stack to memory or register, then increment stack pointer (ESP) by 4\\n        Value is written to the parameter\\nCommands\\n    0fH - H at end specifies hex number\\n    lea is like & (get the address of)\\n        Load effective address\\n        Place address of second parameter into the first parameter \\n        this is faster than arithmetic because you can do things as a a single command\\n    add, sub\\n        a += b\\n    inc, dec\\n        a++ \\n    imul\\n        a *= b\\n    idiv - use shift if possible\\n        have to load a,b into one 64-bit integer\\n    and, xor - bitwise\\n    cmp - compare two things\\n        je - jump when equal - specify where you are going to jump to\\n        Others: jne, jz, jg, jge, jl, jle, js\\n    call <label> - subroutine call\\n        pushes address of next instruction onto stack\\n        then jumps to label\\n    ret - returns from subroutine\\nCalling conventions\\n    A set of rules/expectations between functions\\n    Using a stack for calling convention is implemented on most processors. Why? - Recursion\\n        Parameters: pushed on the stack\\n        Registers: saved on the stack\\n            eax,ecx,edx can be modified\\n            ebx,edi,esi shouldn\\'t be \\n        call - places return address on stack\\n        Local variables: placed in memory on the stack\\n        Return value: eax\\n    Callee: the function which is called by another function\\nRegister Usage\\n    Three registers may be modified by the callee: eax, ecx, edx\\n    If the caller wants to keep those values, they need to be saved by pushing them onto the stack\\n        Return value: eax register\\nVarying number of parameters\\n    Method overloading\\n        Foo::bar(int) and Foo::bar(int, float) - this just creates two methods\\n    Default parameters\\n        Foo::bar (int x = 3) - value defaults to three, method always gets one parameter\\n    Variable number of parameters\\n        #include <cstdarg>\\n        #include <iostream>\\n        using namespace std;\\n        double average (int num, ...) { //num is the number of arguments following\\n          va_list arguments;\\n          double sum = 0;\\n          va_start (arguments, num);\\n          for ( int x = 0; x < num; x++ )\\n            sum += va_arg (arguments, double);\\n          va_end (arguments);\\n          return sum / num;\\n        }\\n        int main() {\\n          cout << average(3, 12.2, 22.3, 4.5) << endl;\\n          cout << average(5, 3.3, 2.2, 1.1, 5.5, 3.3) << endl;\\n        }\\nCaller: the function which calls another function\\n    Prologue\\n        Tasks to take care of BEFORE calling a subroutine\\n        Call the subroutine with the call opcode\\n    Epilogue\\n        Tasks to complete AFTER subroutine call returns\\n        (It is not really called this, but I use this to parallel the equivalent components in the callee convention)\\n    Before calling the function (the prologue)\\n        Save registers that might be needed after the call (eax, ecx, edx)\\n        Push parameters on the stack\\n    Call the function\\n        call instruction places return address in stack\\n    After the called function returns (the epilogue)\\n        Remove parameters from stack\\n        Restore saved registers\\npop saves the value and the increments esp by 4\\nif we don\\'t need to save the values (like with arguments we passed in), we can just increment esp    \\nCallee\\n    parameters are above ebp\\n    local variables are under ebp\\n    sub esp,4 - doing this at the beginning makes space for local variables\\n    ...\\n    return value is placed into eax //we won\\'t focus on things that don\\'t return 4 bytes, like returning a double\\n    mov esp,ebp - this undoes the above command.  We could do add esp,4 but if it were more complicated this would still work.\\n\\n    subroutine may not know how many parameters are passed to it - thus, 1st arg must be at ebp+8 and the rest are pushed above it.\\n    Every subroutine call puts return address and ebp backup on the stack\\n\\nActivation Records\\n    Every time a sub-routine is called, a number of things are pushed onto the stack:\\n        Registers\\n        Parameters\\n        Old base/stack pointers\\n        Local variables\\n        Return address\\n    The callee also pushes caller-saved registers\\n    Typically stack stops around 100-200 Megabytes, although this can be changed\\n        \\nMemory - There are two types of memory that need to be handled:\\n    Dynamic memory (via new, malloc(), etc.)\\n        This is stored on the heap\\n    Static memory (on the stack)\\n        This is where the activation records are kept\\n\\n    The binary program starts at the beginning of the 2^32 = 4 Gb of memory\\n    The heap starts right after this\\n    The stack starts at the end of this 4 Gb of memory, and grows backward\\n    If they meet, you run out of memory\\n\\nBuffer Overflow\\n    void security_hole() {\\n        char buffer[12];\\n        scanf (\"%s\", buffer); // how C handles input\\n    }\\n    The stack looks like (with sizes in parenthesis):\\n\\n     esi (4) \\t edi (4) \\t buffer (12) \\t ebp (4) \\t ret addr (4) \\n    \\n    Addresses increase to the right (the stack grows to the left)\\n    What happens if the value stored into buffer is 13 bytes long?\\n         We overwrite one byte of ebp\\n    What happens if the value stored into buffer is 16 bytes long?\\n         We completely overwrite ebp\\n    What if it is exactly 20 bytes long?\\n         We overwrite the return address!\\n    Buffer Overflow Attack\\n         When you read in a string (etc.) that goes beyond the size of the buffer\\n         You can then overwrite the return address\\n         And set it to your own code\\n         For example, code that is included later on in the string - overwrite ebp, overwrite ret addr with beginning of malicious code\\n\\nWe are using nasm as our assembler for the x86 labs\\n    looks different when you use the compiler\\nin C, you can only have one method with the same name\\n    C translates more cleanly into assembly\\noptimization rearranges stuff to lessen memory access\\n_Z3maxii:\\n    ii is the parameter list (two integers)\\n         \\nIn little-Endian, the entire 32-bit word and the 8-bit least significant byte have the same address\\n    this makes casting very easy\\nRISC\\n    Reduced instruction set computer\\n    Fewer and simpler instructions (maybe 50 or so)\\n    Less chip complexity means they can run fast\\nCISC\\n    Complex instruction set computer\\n    More and more complex instructions (300-400 or so)\\n    More chip complexity means harder to make run fast\\n\\nCaller\\n  Parameters: pushed on the stack\\n        Registers: saved on the stack\\n            eax,ecx,edx can be modified\\n            ebx,edi,esi shouldn\\'t be \\n        call - places return address on stack\\n        Local variables: placed in memory on the stack\\n        Return value: eax\\nCallee: the function which is called by another function\\n    push ebp\\n    mov ebp, esp\\n    sub esp, 4 //allocate local variables\\n    push ebx   //you don\\'t have to back these up\\n    mov ebp-4, 1 //load 1 into local variable\\n\\n    add esp, 4  //deallocate local var\\n    pop ebx\\n    pop ebp\\n    ret\\n\\n## intro to C - we use ANSI standard\\n```java\\nall C is valid C++\\n// doesn\\'t work\\nalways use +=1 not ++\\ncompile with gcc -ansi -pedantic -Wall -Werror program.c\\n\\t-Werror will stop the program from compiling\\nall variables have to be declared at the top\\n\\tint main(int argc, char*argv[]){\\n\\t\\tint x = 1;\\n\\t\\tint y = 34;\\n\\t\\tint z = y*y/x;\\n\\t\\tx = 13;\\n\\t\\tint w = 1; <- this will not work\\n\\tlabel_name:\\n\\t\\tprintf(\"omg!\");\\n\\t\\tgoto label_name; /*this goes to the label_name line - don\\'t do this, but assembly only has this*/\\n\\t\\treturn 0;\\n\\t}\\nprintf(const char *format, ...)\\n\\tprintf(\"%d %f %g %s\\\\n\",); /* int, double (these must be explicitly doubles), double (as small as possible), string */\\n```\\n## compile steps\\nsource (text) -> pre-processor -> modified source (text) -> compiler -> assembly (text) -> assembler -> binary program -> linker -> executable\\n\\n1. pre-processing - deals with hashtags - sets up line numbers for errors, includes external definitions, normal defines (.i)\\n2. compile - turns it into assembly (.s)\\n\\t- this assembly has commands with destination, src\\n3. assemble - turns assembly into binary with a lot of wrappers (.o)\\n4. link - makes the file executable, gets the code from includes together (a.out)\\n\\n## strings\\n- char - number that fits in 1 byte\\n- string is an array of chars: char*\\n- all strings end with null character \\\\0, bad security\\n- length of string doesn\\'t include null character\\n\\nh|e|l|l|o|\\\\0\\n-|\\n10|.|.|.|.|15\\n\\n## memory in C\\n- byte is smallest accessible memory unit - 2 hex digits (ex: 0x3a)\\n\\nBits | Name | Bits | Name\\n- | \\n1 | bit | 16 | word\\n4 | nyble | 32 | double word, long word\\n8 | byte | 64 | quad word\\n\\ntheoretically would work:\\n\\n```java\\nVoid * p = 3 (address 3)\\n*p = 0x3a (value 3a)\\np[0] = 0x3a (value 3a)\\np[3] is same as *(p+3) - can even use negative addresses, (at end wraps around - overflows)\\n```\\nin practice:\\n\\n```java\\nint* p\\nsizeof(int) == 4, but all pointers are just one byte - points to location of four consecutive bytes that are int\\nindexing this pointer will tell you how much to offset memory by\\naddress must be 4-bytes aligned (means it will be a multiple of 4)\\n```\\n- little-endian - least significant byte first\\n- big-endian - most significant byte first (normal) - networks use this\\n- we will use little-endian, this is what most computers use\\n- low addresses are unused - will throw error if accessed\\n- top has the os - will throw error if accessed\\n- contains heap, stack, code globals, shared stuff\\n\\n## call stack\\n1. return address\\n2. local variables\\n3. backups\\n4. top pointer\\n5. base pointer\\n6. next pointer\\n7. parameters (sometimes)\\n8. return values (sometimes)\\n\\none frame - between the lines is everything the current method has - largest addresses at top, grows downwards\\n\\n- parameters (backwards)\\n- ret address\\n\\n---\\n- base pointer\\n- previous stack base\\n- saved stuff\\n- locals\\n- top pointer (actually at the bottom)\\n- return value\\n\\n---\\n- in practice, most parameters and return values are put in registers\\n\\n## types\\n- 2\\'s complement: positive normal, negative subtract 1 more than biggest number you can do\\n\\t- flip everything to the left of the rightmost one\\n\\t- math is exactly the same, discard extra bits\\n\\t\\ntype | signed? | bits\\n- | \\nchar | ? | 8\\nshort | signed | 16 (usually)\\nint | . | 16 or 32\\nlong | . | ≤16 or ≥ int\\nlong long | signed | 64\\n- everything can have an unsigned / signed in front of the type\\n- C will cast things without throwing error\\n\\t\\n## boolean operators\\n- 9 = 1001 = 1.001 x 2^3\\n- x && y -> {0,1}\\n- x & y -> {0,1,...,2^32} (bit operations)\\n- ^ is xor\\n- !x - 0 -> 1, anything else -> 0\\n- and is often called masking because only the things with 1s go through\\n- shifting will pad with 0s\\n\\t (1 << 3 )-1 \\t\\t\\tgives us 3 1s\\n\\t ~((1 << 3 )-1)\\t\\tgives us 111...111000\\n\\t x & ~((1 << 3 )-1)\\tgives us x1x2.....000\\n- >> copies the msb\\n- then we can or it in order to change those last 3 bits\\n- trinary operator - a≥b:c means if(a) b; else c \\n- a&b | ~a&c\\n\\t- only works for 2-bit numbers if a=00 or a=11\\n\\t-!x 1 if x=0\\n\\t-!!x 1 if x!=0 so we want a=-!!x\\n                                 \\n## ATT x86 assembly\\n- there are 2 hardwares\\n\\t- x86-64 (desktop market)\\n\\t- Arm7 (mobile market) - all instructions are like cmov\\n- think -> not = ex. mov $3, %rax is 3 into rax\\n- prefixes\\n\\t- $34 - $ is an immediate (literal) value\\n\\t- %rax - the contents of register rax\\n\\t- main - label (no prefix) - assembler turns this into an immediate\\n\\t- 3330(%rax,%rdi,8) - memory at (3330+rax+8*rdi) - in x86 but not y86\\n\\t\\t- you could do 23(%rax)\\n- gcc -S will give you .S file w/ assembly\\n\\t- what would actually be compiled\\n- gcc -c will give you object file\\n- then, objdump -d will dissassemble object file and print assembly\\n\\t- leaves out suffixes that tell you sizes\\n\\t- can\\'t be compiled, but is informative\\n- gcc -O3 will be fast, default is -O0, -OS optimizes for size\\n- we call the part of x86 we use y86\\n- registers\\n\\t- general purpose registers (program registers)\\n\\t- PC - program counter - cannot directly change this - what line to run next\\n\\t- CC - condition codes - sign of last math op result\\n\\t\\t- remembers whether answer was 0,-,+\\n- cmp - basically subtraction (works backwards, 2nd-1st), but only sets the condition codes\\n- in assembly, arithmetic works as +=\\n\\t- ex. imul %rdi, %rax multiplies and stores result in rdi\\n- doesn\\'t really matter: eax, rax are same register but eax is bottom half of rax\\n\\t- on some hardwares eax is faster than rax\\n- call example\\n\\t\\t- PC=0x78 callq 0x56\\n\\t- PC=0x7d next command (because callq is 5 bytes long, it could be different)\\n\\t\\t- puts 7d on stack to return to at address 0x100\\n\\t\\t- this address (0x100) is subtracted by number of bytes in address (8)\\n\\t\\t- this value (0x0f8) is put into rsp(in C this is always on the stack)\\n\\t\\t\\t- rsp stores address of the top of the stack\\n\\t\\t- PC becomes 56\\n- call\\n\\t- movq (next PC), (%rsp) ~PC can\\'t actually be changed\\n\\t- addq $-8, %rsp\\n\\t- jmp $0x56\\n- ret\\n\\t- addq $8, %rsp\\n\\t- movq (%rsp), (PC)  ==same as== jmp (%rsp)\\n- push\\n\\t- mov _, (%rsp)\\n\\t- sub $8, %rsp\\n- pop does the opposite\\n\\t- add $8, %rsp\\n\\t\\t movq (%rsp), _\\t\\ncmp\\n\\t- cmovle %5, (%rax) - move only if we are in this state\\n\\t\\n## y86 - all we use\\n1. halt - stops the chip\\n2. nop - do nothing\\n3. op_q\\n\\t- addq, subq, andq, xorq\\n\\t- takes 2 registers, stores values in second\\n\\t\\t- sub is 2nd-1st\\n4. jxx\\n\\t- jl, jle, jg, je, jne, jmp\\n\\t- takes immediate\\n5. movq longer PC increment because it stores offset, register always comes first (is rA)\\n\\t- rrmovq (register-register, same as cmov where condition is always)\\n\\t- irmovq (immediate-register)\\n\\t- rmmovq\\n\\t- mrmovq (memory)\\n6. cmovxx (register-register)\\n7. call\\n\\t- takes immediate\\n\\t- pushes the return address on the stack and jumps to the destination address.\\n8. ret\\n\\t- pops a value and jumps to it\\n9. pushq\\n\\t- one register\\n\\t- pushes then decrements\\n10. popq\\n\\t- one register\\n\\n- programmer-visible state\\n\\t- registers\\n\\t\\t- program\\n\\t\\t\\t- rax-r14 (8 registers x86 has 15)\\n\\t\\t\\t- rsp is the special one\\n\\t\\t\\t- 64 bit integer (or pointer) - there is no floating point, in x86 floating point is stored in other registers\\n\\t\\t- other\\n\\t\\t\\t- program counter (PC), instruction pointer\\n\\t\\t\\t\\t- 64-bit pointer\\n\\t\\t\\t- condition codes (CC) - not important, tell us <, =, > on last operation\\n\\t\\t\\t\\t- only set by the op_q commands\\n\\t- memory - all one byte array\\n\\t\\t- instruction\\n\\t\\t- data\\n- encoding (assembly -> bytes)\\n\\t- 1st byte -> high-order nybble | low-order nybble\\n\\t\\t- higher order is opcode (add, sub, ...)\\n\\t\\t- lower-order is either instruction function or flag(le, g, ...) - usually 0\\n\\t- remaining bytes \\n\\t\\t- argument in little-endian order\\n\\t- examples\\n\\t\\t- call $0x123\\t\\t\\t-> 80 23 01 00 00 00 00 00 00\\n\\t\\t\\t ret \\t\\t\\t\\t\\t-> 90\\n\\t\\t\\t subq %rcx, %r11\\t\\t-> 81 1b (there are 15 registers, specify register with one nybble)\\n\\t\\t\\t irmov $0x3330, %rdi\\t-> 30 f7 30 33 00 00 00 00 00 00 (register-first always, f means no source, but destination of register 7)\\n\\t- compact binary (variable-length encoding) vs. simple binary (fixed-length encoding)\\n\\t\\t- x86 vs. ARM\\n\\t\\t- people can\\'t decide\\n\\t\\t- compact binary - complex instruction set computer (cisc) - emphasizes programmer\\n\\t\\t- simple binary - reduced instruction set computer (risc) - emphasizes hardware\\n\\t\\t\\t- have more complex compilers\\n\\t\\t\\t- fixed width instructions\\n\\t\\t\\t- lots of registers\\n\\t\\t\\t- few memory addressing modes - no memory operands (only mrmov, rmmov)\\n\\t\\t\\t- few opcodes\\n\\t\\t\\t- passes parameters in registers, not the stack (usually)\\n\\t\\t\\t- no condition codes (uses condition operands)\\n\\t\\t- in general, computers use compact and tablets/phones use simple\\n\\t\\t- if we can get away from x86 backwards compatibility, we will probably meet in the middle\\n\\t\\t- study RISC vs. CISC\\n\\t\\t\\n## hardware\\n- flows when control is high\\n- power - everything loses power by creating heat (every gate consumes power)\\n\\t- changing a transistor takes more power than leaving it\\n- voltage - threshold above which transistor is open\\n- register - on rising clock edge store input\\n- overclock computer - could work, or logic takes too long to get back - things break\\n\\t- could be fixed with colder, more power\\n\\t\\tchips are small because of how fast they are\\t\\n- mux - selectors pick which input goes through\\n- out = [\\n\\tguard:value;\\n\\t\\t...\\n];\\n- this is a language called HCL written by the book\\'s authors\\n- out = g?input: g2:input2: ...:0\\n\\t- if first is true return that, otherwise keep going otherwise return 0\\n\\n## executing instructions\\n- we have wires\\n1. register file\\n2. data memory\\n3. instruction memory\\n4. status output - 3-bits\\n- ex. popq, %rbx\\n  - todo: get icode, check if it was pop, read mem at rsp, put value in rbx, inc rsp by 8\\n  - getting icode\\n  \\t- instruction in instruction memory: B0 3F\\n  \\t- register pP (p is inputs), (P is outputs)\\n  \\t\\t- pP { pc:64 = 0;} - stores the next pc\\n  \\t- pc <- P_pc - the fixed functionality will create i10 bytes\\n  \\t- textbook: icode:ifun = M_1[PC] - gets one byte from PC\\n  \\t- HCL (HCL uses =): \\n  \\t\\twire icode:4;\\n  \\t\\ticode = i10bytes[4..8] - little endian values, one byte at a time - this grabs B from B0 3F\\n  - assume icode was b (in reality, this must be picked with a mux)\\n  ```java\\n  \\tvalA \\t<- R[4] \\t\\t// gets %rsp - rsp is 4th register\\n  \\trA:rB\\t<- M_1[PC+1]\\t// book\\'s notation - splits up a byte into 2 halves, 1 byte in rA, 1 byte in rB, PC+1 because we want second byte\\n  \\t\\t\\t\\t\\t\\t\\t// 3 is loaded into rA, F is loaded into rB\\n  \\tvalE \\t<- valE+8\\t\\t// inc rsp by 8\\n  \\tvalM \\t<- M_8[valA] \\t// send %rsp to \\n  \\tR[rA] \\t<- valM\\t\\t\\t// writes to %rbx \\n  \\tR[4]\\t<- valE\\t\\t\\t// writes to %rsp\\n  \\tp_pc\\t=  P_pc+2\\t\\t// increment PC\\tby 2 because popq is 2-byte instruction\\t\\n  ```\\n```\\n- steps\\n\\u200b```java\\n\\t1. fetch - what is wanted\\n\\t2. decode - find what to do it to - read prog registers\\n\\t3. execute and/or memory - do it\\n\\t4. write back - tell you result\\n\\n020 10\\t\\t\\n\\tnop\\t\\t\\t\\t\\t\\tfetch\\t\\tchange pc to 021\\n021 6303\\t\\n\\txorq \\t%rax,%rbx\\t\\tfetch \\t\\tpc <- 0x023\\n\\t\\t\\t\\t\\t\\t\\tdecode \\t\\tread reg. file at 0,3 to get 17 and 11\\n\\t\\t\\t\\t\\t\\t\\texecute\\t\\t17^11 = 10001^01011 = 11010 = 26, also sets CC to >0\\n\\t\\t\\t\\t\\t\\t\\twrite back\\twrite 26 to regFile[3]\\n023 50 23 1000000000000000 \\tthe immediate 16 is in little-endian but is 8 bytes - 1st in memory last in little-endian\\n\\tmrmovq  16(%rbx),%rcx\\tfetch \\t\\tread bytes, understand, PC <- 0x02d\\n\\t\\t\\t\\t\\t\\t\\tdecode\\t\\tread regFile to get (26),13\\n\\t\\t\\t\\t\\t\\t\\texecute\\t\\t16+26=42 (to be new address)\\n\\t\\t\\t\\t\\t\\t\\tmemory\\t\\task RAM for address 42, it says 0x0000000071000000 - little-endian\\n\\t\\t\\t\\t\\t\\t\\twrite back\\tput 0x71000000 into regFile[2]\\n02d 71 0000000032651131\\t\\t\\t\\t\\n    jle 0x3111653200000000 \\tfetch \\t\\tvalP <- 0x036\\n\\t\\t\\t\\t\\t\\t\\tdecode cc > 0, not jump, PC <- valP\\n036\\t00 \\n\\thalt - set STAT to HALT and the computer shuts off, STAT is always going on in the background\\npush\\n```\\n\\t- reads source register\\n\\t- reads rsp\\n\\t- dec rsp by 8\\n\\t- writes read value to new rsp address\\n\\n\\n## hardware wires - opq example \\n```java\\n## fetch\\n## 1. set pc\\n## 1.a. make a register to store the next PC\\nregister qS {\\n\\tpc : 64 = 0;\\n\\tlt : 1 = 0;\\n\\teq : 1 = 0;\\n\\tgt : 1 = 0;\\n}\\n\\n## 2. read i10bytes\\npc = S_pc;\\n\\n## 3. parse out pieces of i10bytes\\nwire icode:4, ifun:4, rA:4, rB:4;\\nicode = i10bytes[4..8]; ## 1st byte: 0..8  high-order nibble 4..8\\nifun = i10bytes[0..4]; \\n\\nconst OPQ = 6;\\nconst NO_REGISTER = 0xf;\\n\\nrA = [\\n\\ticode == OPQ : i10bytes[12..16];\\n\\t1: NO_REGISTER;\\n];\\n\\nrB = [\\n\\ticode == OPQ : i10bytes[8..12];\\n\\t1: NO_REGISTER;\\n];\\n\\nwire valP : 64;\\n\\nvalP = [\\n\\ticode == OPQ : S_pc + 2;\\n\\t1 : S_pc + 1; ## picked at random\\n];\\n\\n\\nStat = STAT_HLT; ## fix this\\n\\n## decode\\n\\n## 1. set srcA and srcB\\n\\nsrcA = rA;\\nsrcB = rB;\\n\\ndstE = [\\n\\ticode == OPQ : rB;\\n\\t1 : NO_REGISTER;\\n];\\n\\n## execute\\n\\nwire valE : 64;\\n\\nvalE = [\\n\\ticode == OPQ && ifun == 0 : rvalA + rvalB;\\n\\t1 : 0xdeadbeef;\\n];\\n\\nq_lt = [\\n\\ticode == OPQ : valE < 0;\\n\\t## ...\\n];\\n\\n## memory\\n\\n## writeback\\n\\nwvalE = [\\n\\ticode == OPQ : valE;\\n\\t1: 0x1234567890abcdef;\\n];\\n\\n\\n## PC update\\nq_pc = valP; \\n```\\n\\n## pipelining \\n- nonuniform partitioning - stages don\\'t all take same amount of time\\n- register - changes on the clock\\n\\t- normal - output your input\\n\\t- bubble - puts \"nothing\" in registers - register outputs nop\\n\\t- stall - put output into input (same output)\\n- see notes on transitions\\n- stalling a stage (usually because we are waiting for some earlier instruction to complete)\\n\\t- stall every stage before you\\n\\t- bubble stage after you so nothing is done with incomplete work - this will propagate\\n\\t- stages after that are normal\\n- bubbling a stage - the work being performed should be thrown away\\n\\t- bubble stage after it\\n\\t- basically send a nop - use values NO_REGISTER and 0s\\n\\t\\t- often there are some fields that don\\'t matter\\n- stalling a pipeline = stalling a stage\\n- bubbling a pipeline - bubble all stages\\n\\n```java\\nirmovq\\t \\t$31, %rax\\naddq \\t\\t$rax,%rax\\njle\\n```\\n\\n- the stages are offset - everything always working\\n- when you get a jmp, you have to wait for the thing before you to writeback.  2 possible solns\\n\\t- stall decode \\n\\t- forward value from the stage it is currently in\\n- look at online notes - save everything in register that needs to be used later\\n\\n#### problems\\n1. dependencies - outcome of one instruction depends on the outcome of another - in the software\\n\\t1. data - data needed before advancing - destination of one thing is used as source of another\\n\\t\\t- load/use\\n\\t2. control - which instruction to run depends on what was done before\\n2. hazards - potential for dependency to mess up the **pipeline** - in the hardware design\\n\\t- hardware may or may not have a hazard for a dependency\\n\\t- can detect them by comparing the wire that reads / writes to regfile (rA,rB / dstE) - they shouldn\\'t be the same because you shouldn\\'t be reading/writing to the same register (except when all NO_REGISTER)\\n\\n#### solutions\\n- P is PC, then FDEMW\\n1. stall until it finishes if there\\'s a problem\\n\\tstall_P = 1;\\t//stall the fetch/decode stage\\n\\tbubble_E = 1;   //completes and then starts a nop, gives it time to write values\\n\\t. forward values (find what will be written somewhere)\\t\\n\\t- in a 2-stage system, we have dstE and we use it to check if there\\'s a problem\\n\\t- usually if we can check that there\\'s a problem, we have the right answer\\n\\t- if we have the answer, put value where it should be\\n\\t- this is difficult, but doesn\\'t slow down hardware\\n\\t- we decide whether we can forward based on a pipeline diagram - boxes with stages (time is y axis, but also things are staggered)\\n\\t\\t- we need to look at when we get the value and when we need it\\n\\t\\t- we can pipeline if we need the value after we get it \\n\\t\\t- if we don\\'t, we need to stall\\n```java\\nsubq %rax,%rbx\\njge bazzle\\n```\\n- we could stall until CC is set in execute of subq to fetch for jge - this is slow\\n1. speculation execution - we make a guess and sometimes we\\'ll be right (modern processors are right ~90%)\\n\\t- branch prediction - process of picking branch\\n\\t- jumps to a smaller address are taken more often than not - this algorithm and more things make it complicated\\n\\t- if we were wrong, we need to correct our work\\n- Example: \\n```java\\n1\\tsubq\\n2\\tjge 4\\n3\\tir\\n4\\trm\\n```\\n- the stage performing ir is wrong - this stage needs to be bubbled\\n- in a 5-stage pipleine like this, we only need to bubble decode\\n- ret - doesn\\'t know the next address to fetch until after memory stage, also can\\'t be predicted well\\n\\t- returns are slow, we just wait\\n\\t- some large processors will guess, can still make mistakes and will have to correct\\n\\t\\n#### real processors\\n1. memory is slow and unpredictably slow (10-100 cycles is reasonable)\\n2. pipelines are generally 10-15 stages (Pentium 4 had 30 stages)\\n3. multiple functional units\\n\\t- alu could take different number of cycles for addition/division\\n\\t- multiple functional units lets one thing wait while another continues sending information down the pipeline\\n4. out-of-order execution\\n\\t- compilers look at whether instructions can be done out of order\\n\\t- it might start them out of order so one can compute in functional unit while another goes through\\n\\t- can swap operations if 2nd operation doesn\\'t depend on 1sts\\n5. (x86) turn the assembly into another language\\n\\t- micro ops\\n\\t- makes things specific to chips\\n- profiler - software that times software - should be used to see what is taking time in code\\n- hardware vs. software\\n\\t- software: .c -> compiler -> assembler -> linker -> executes\\n\\t\\t- compiler - lexes, parses, O_1, O_2, ...\\n\\t- hardware: register transfer languages (.hcl) -> ... -> circuit-level descriptions -> layout (veryify this) -> mask -> add silicon in oven -> processor\\n\\t\\n## memory\\n- we want highest speed at lowest cost (higher speeds correspond to higher costs)\\n- fastest to slowest\\n\\t- register - processor - about 1K\\n\\t- SRAM - memory (RAM) - about 1M\\n\\t- DRAM - memory (RAM) - 4-16 GB\\n\\t- SSD (solid-state drives) - mobile devices\\n\\t- Disk/Harddrive - filesystem - 500GB\\n- the first three are volatile - if you turn off power you lose them\\n- the last three are nonvolatile\\n- things have gotten a lot faster\\n- where we put things affects speed \\n\\t- registers near ALU\\n\\t- SRAM very close to CPU\\n\\t- CPU also covered with heat sinks\\n\\t- DRAM is large - pretty far away (some other things between them)\\n- locality\\n\\t- temporal - close in time - the characteristic of code that repeatedly uses the same address \\n\\t- spatial - close in space - the characteristic of code that uses addresses that are numerically close\\n- real-time performance is not big-O - a tree can be faster than hash because of locality\\n- caching - to keep a copy for the purpose of speed\\n\\t- if we need to read a value, we want to read from SRAM (we call SRAM the cache)\\n\\t- if we must read from DRAM (we call DRAM the main memory), we usually want to store the value in SRAM\\n\\t- cache - what they wanted most recently (good guess because of temporal locality)\\n\\t- slower caches are still bigger\\n\\t\\t- cache nearby bytes to recent acesses (good guess because of spatial locality)\\n- simplified cache\\n\\t- 4 GB RAM -> 32-bit address\\n\\t- 1MB cache\\n\\t- 64-bit words\\n\\t- ex. addr: 0x12345678\\n\\t\\t- simple - use entire cache as one block\\n\\t\\t- chop off the bottom log(1MB)=20 bits\\n\\t\\t- send addr = 0x12300000\\n\\t\\t- fill entire cache with 1MB starting at that address\\n\\t\\t- tag cache with 0x123\\n\\t\\t- send value from cache at address offset=0x45678\\n\\t\\t- if the tag of the address is the same as the tag of the cache, then return from cache\\n\\t\\t\\t- otherwise redo everything\\n- slightly better cache\\n\\t- beginning of address is tag\\n\\t- middle of address might give you index of block in table = log(num_blocks_in_cache)\\n\\t- end of address is block offset (offset size=log(block_size))\\n\\t- a (tag,block) pair is called a line\\n\\t1. Fully-associative cache set of (tag,block) pairs\\n\\t\\t- table with first column being tags, second column being blocks\\n\\t\\t- to read, check tag against every tag in cache\\n\\t\\t\\t- if found, read from that block\\n\\t\\t\\t- else, pick a line to evict (this is discussed in operating systems)\\n\\t\\t\\t\\t- read DRAM into that line, read data from that line\\'s block\\n\\t\\t- long sets are slow! - typicaly 2 or 3 lines would be common\\n\\t2. Direct-mapped cache - array of (tag,block) pairs\\n\\t\\t- table with 1st column tags, 2nd column blocks\\n\\t\\t- to read, check the block at the index given in the address\\n\\t\\t\\t- if found, read from block\\n\\t\\t\\t- else, load into that line\\n\\t\\t- good spatial locality would make the tags adjacent (you read one full block, then the next full block)\\n\\t\\t- this is faster (like a mux) instead of many = comparisons, typically big (1K-1M lines)\\n\\t3. Set-associative cache - hybrid - array of sets\\n\\t\\t- indices each link to a set, each set with multiple elements\\n\\t\\t- we search through the tags of this set\\n\\t- look at examples in book, know how to tell if we have hit or miss\\n\\n#### writing\\n- assume write-back, write-allocate cache\\n1. load block into cache (if not already) - write-allocate cache\\n2. change it in cache\\n3. two optionsm\\n\\t1. write back - wait until remove the line to update RAM\\n\\t\\t- line needs to have tag, block, and dirty bit\\n\\t\\t- dirty = wrote but did not update RAM\\n\\t2. write through - update RAM now\\n- no-write-allocate bypasses cache and writes straight to memory if block not already in cache (typically goes with write-through cache)\\n- valid bits - whether or not the line contains meaningful information\\n- kinds of misses\\n\\t1. cold miss - never looked at that line before\\n\\t\\t- valid bit is 0 or we\\'ve only loaded other lines into the cache\\n\\t\\t- associated with tasks that need a lot of memory only once, not much you can do\\n\\t2. capacity miss - we have n lines in the cache and have read ≥ n other lines since we last read this one\\n\\t\\t- typically associated with a fully-associative cache\\n\\t\\t- code has bad temporal locality\\n\\t3. conflict miss - recently read line with same index but different tag\\n\\t\\t- typically asssociated with a direct-mapped cache\\n\\t\\t- characteristic of the cache more than the code\\n\\n#### cache anatomy\\n- i-cache - holds instructions\\n\\t- typically read only\\n- d-cache - holds program data\\n- unified cache - holds both\\n- associativity - number of cache lines per set\\n\\n## optimization\\n- only need to worry about locality for accessing memory\\n\\t- things that are in registers / immediates don\\'t matter\\n- compilers are often cautious\\n- some things can\\'t be optimized because you could pass in the same pointer twice as an argument\\n- loop unrolling - often dependencies accross iterations\\n```java\\nfor(int i=0;i<n;i++){ //this line is bookkeeping overhead - want to reduce this\\n\\ta[i]+=1; //has temporal locality because you add and then store back to same address, obviously spatial locality\\n}\\n// unrolled\\nfor(int i=0;i<n-2;i+=3){ //reduced number of comparisons, can also get benefits from vectorization (complicated)\\n\\ta[i]+=1;\\n\\ta[i+1]+=1;\\n\\ta[i+2]+=1;\\n}\\nif(n%3>=1) a[n-1]+=1;\\nif(n%2>=2) a[n-2]+=1;\\n```\\n\\n- n%4 = n&3\\n- n%8 = n&7s\\n```java //less error prone\\n\\tfor(int i=0;i<n;i+=1){ //this can be slower, data dependency between lines might not allow full parallelism (more stalling)\\n\\t\\ta[i]+=1;\\n\\t\\ti+=1;\\n\\t\\ta[i]+=1;\\n\\t\\ti+=1;\\n\\t\\ta[i]+=1;\\n\\t\\ti+=1;\\n\\t}\\n```\\n- loop order\\n- the order of loops can make this way faster\\n```java\\nfor(i...)\\n\\tfor(j...)\\n\\t\\ta[i][j]+=1; //two memory accesses\\n```\\n- flatten arrays //this is faster, especially if width is a factor of 2, end of one row and beginning of next row are spatially local\\n\\t- row0 then row1 then row2 ....\\n\\t- float[height*width], access with array[row*width+column]\\n- problem - loop can\\'t be picked\\n```java\\n\\tfor(i..)\\n\\t\\tfor(j...)\\n\\t\\t\\ta[i][j]=a[j][i]; \\n```\\n- solution - blocking\\n\\t- pick two chunks \\n\\t- one we read by rows and is happy - only needs one cache line\\n\\t- one we read by columns - needs blocksize cache lines (by the time we get to the second column, we want all rows to be present in cache)\\n```java\\n\\tint bs=8;\\n\\tfor (bx=0;bx<N;bx+=bs) // for each block\\n\\t\\tfor(by=0;by<N;by+=bs)\\n\\t\\t\\tfor(x=bx;x<bx+bs;x+=1) // for each element of block\\n\\t\\t\\t\\tfor(y=by;y<by+bs;y+=1)\\n\\t\\t\\t\\t\\tswap(a[x][y],a[y][x]); // do stuff\\n```\\n- conditions for blocking\\n\\t1. the whole thing doesn\\'t fit in cache\\n\\t2. there is no spatially local loop order\\n\\t- block size must be able to fit in cach\\n- reassociation optimization - compiler will do this for integers, but not floats (because of round-off errors)\\n\\t- a+b+c+d+e -> this is slower (addition is sequential by default, the sum of the first two is then added to the third number)\\n\\t- ((a+b)+(c+d))+e -> this can do things in parallel, we have multiple adders (we can see logarithmic performace if the chip can be fully parallell)\\n- using methods can increase your instruction cache hit rate\\n\\n\\n## exceptions\\n- processor is connected to I/O Bridge which is connected to Memory Bus and I/O Bus\\n\\t- these things are called the mother board\\n\\t- we don\\'t want to wait for I/O Bus\\n\\t\\t1. Polling - CPU periodically checks if ready\\n\\t\\t2. Interrupts - CPU asks device to tell the CPU when the device is ready\\n\\t\\t\\t- this is what is usually done\\n- CPU has an interrupt pin (interrupt sent by I/O Bridge)\\n- steps for an interrupt\\n\\t1. pause my work - save where I can get back to it\\n\\t2. decide what to do next\\n\\t3. do the right thing\\n\\t4. resume my suspended work\\n- jump table - array of code addresses\\n\\t- each address points to handler code\\n\\t- CPU must pause, get index from bus, jump to exception_table[index]\\n\\t- need the exception table, exception code in memory, need register that tells where the exception table is\\n\\t- the user\\'s code should not be able to use exception memory\\n- memory\\n\\t- mode register (1-bit): 2 modes\\n\\t\\t- kernel mode (operating system) - allows all instructions\\n\\t\\t- user mode - most code, blocks some instructions, blocks some memory\\n\\t\\t\\t- cant set mode register\\n\\t\\t\\t- can\\'t talk to the I/O bus\\n\\t- largest addresses are kernel only\\n\\t\\t- some of this holds exception table, exception handlers\\n\\t- between is user thhings\\n\\t- smallest addresses are unused - they are null (people often try to dereference them - we want this to throw an error)\\n- exceptions\\n\\t1. interrupts, \\tindex: bus, \\t\\t\\twho causes it: I/O Bridge\\n\\t\\t- i1,i2,i3,i4 -> interrupt during i3 instruction\\n\\t\\t- let i3 finish (maybe)\\n\\t\\t- handle interrupt\\n\\t\\t- resume i4 (or rest of i3)\\n\\t\\t\\t trap, \\t\\t%al (user)\\t\\t\\t\\tint assembly instruction (user code)\\n\\t\\t- trap is between instructions, simple\\n\\t\\t\\t fault,\\t\\tbased on what failed\\tfailing user-mode instruction\\n\\t\\t- fault during i3\\n\\t\\t- suspend i3\\n\\t\\t- handle fault\\n\\t\\t- rerun i3 (assuming we corrected fault - ex. allocating new memory) otherwise abort\\n\\t- abort - reaction to an exception (usually to a fault) - quits instead of resuming\\n- suspending\\n\\t\\n\\t- save PC, program register, condition codes (put them in a struct in kernel memory)\\n- on an exception\\n\\t1. switch to kernel mode\\n\\t2. suspend program\\n\\t3. jump to exception handler \\n\\t4. execute exception handler\\n\\t5. resume in user mode\\n\\n## processes\\n- (user) read file -> (kernel) send request to disk, wait, clean up -> (user) resume\\n\\t\\n\\t- this has lots of waiting so we run another program while we wait (see pic)\\n- process - code with an address space\\n\\t- CPU has a register that maps user addresses to physical addresses (memory pointers to each process)\\n\\t- general we don\\'t call the kernel a process\\n\\t- also had pc, prog. registers, cc, etc.\\n\\t- each process has a pointer to the kernel memory\\n\\t- also has more (will learn in OS)...\\n- context switch - changing from one process to another\\n\\t- generally each core of a computer is running one process\\n\\t1. freeze one process\\n\\t2. let OS do some bookkeeping\\n\\t3. resume another process\\n\\t- takes time because of bookkeeping and cache misses on the resume\\n\\t- you can time context switches \\n\\t\\twhile(true) \\n\\t\\t\\tgetCurrentTime()\\n\\t\\t\\t\\tif(increased a lot) contextSwitches++\\n\\n## threads\\n- threads are like processes that user code manages, not the kernel\\n\\t- within one address space, I have 2 stacks\\n\\t- save/restores registers and stack\\n\\t- hardware usually has some thread support\\n\\t\\t- save/restore instructions\\n\\t\\t- a way to run concurrent threads in parallel\\n\\t\\t\\tpython threads don\\'t run in parallel\\t\\n\\n## system calls\\n- how user code asks the kernel to do stuff\\n\\t- exception table - there are 30ish, some free spots for OS to use\\n- system call - Linux uses exception 128 for almost all user -> kernel requests\\n\\t- uses rax to decide what you are asking //used in a jump table inside the 128 exception handler\\n\\t- most return 0 on success\\n\\t- non-zero on error where the ## is errno\\n- you can write assembly in C code\\n\\n## software exceptions\\n- you can throw an exception and then you want to return to something several method calls before you\\n- nonlocal jump\\n\\t- change PC\\n\\t- and registers (all of them)\\n- try{} - freezes what we need for catch\\n- catch{} - what is frozen\\n- throw{} - resume\\n- hardware exception can freeze state\\n\\n## signals, setjmps\\n- exceptions - caused by hardware (mostly), handled by kernel\\n- signal - caused by kernel, handled by user code (or kernel)\\n\\t- mimics exception (usually a fault)\\n\\t- user-defined signal handler know to the OS\\n\\t- various signals (identified by number)\\n\\t\\t- implemented with a jump table\\n\\t- we can mask (ignore) some signals\\n\\t- turns hardware fault into software exception (ex. divide by 0, access memory that doesn\\'t exist), this way the user can handle it\\n\\t- SIGINT (ctrl-c) - usually cancels, can be blocked\\n\\t\\t- ctrl-c -> interrupt -> handler -> terminal (user code) -> trap (SIGINT is action) -> handler -> sends signal\\n\\t- SIGTER - cancels, can\\'t be blocked\\n\\t- SIGSEG - seg fault\\n- setjmp/longjmp - caused by user code, handled by user code\\n\\t- functions in standard C library in setjmp.h\\n\\t- jumps somewhere where pointer is something that stores current state\\n\\t- setjmp - succeeds first time (returns 0)\\n\\t- longjmp - never returns - calls setjmp with a different return value\\n\\t- you usually use if(setjmp) else {handle error} - basically try-catch\\n\\n## virtual memory\\n- 2 address spaces \\n\\t1. virtual address space (addressable space)\\n\\t\\t- used by code\\n\\t\\t- fixed by ISA designer \\n\\t2. memory management unit (MMU) - takes in a virtual address and spits out physical address\\n\\t\\t- page fault - MMU says this virtual address does not have a physical address\\n\\t\\t\\t- when there\\'s a page fault, go to exception handler in kernel\\n\\t\\t\\t- usually we go to disk\\n\\t3. physical address space (cannot be discovered by code)\\n\\t\\t- used by memory chips\\n\\t\\t- constrained by size of RAM\\n- assume all virtual addresses have a physical address in RAM (this is not true, will come back to this)\\n\\t- each process has code, globals, heap, shared functions, stack\\n\\t- lots of unused at bottom, top because few programs use 2^64 bytes\\n\\t- RAM - we\\'ll say this includes all caches\\n\\t- virtual memory is usually mostly empty\\n\\t\\t\\n\\t\\t- allocated in a few blocks / regions\\n\\t- MMU\\n\\t\\t1. bad idea 1: could be a mapping from every virtual address to every physical address, but this wastes a lot\\n\\t\\t- instead, we split memory into pages (page is continuous block of addresses ~ usually 4k)\\n\\t\\t\\t- bigger = fewer things to map, more likely to include unused addresses\\n\\t\\t\\t- address = low-order bits: page offset, high-order bits: page number\\n\\t\\t\\t\\t- page offset takes log_2(page_size)\\n\\t\\t2. bad idea 2: page table - map from virtual page number -> physical page number\\n\\t\\t\\t- we put the map in RAM, we have a register (called the PTBR) that tells us where it is\\n\\t\\t\\t- we change the PTBR for each process\\n\\t\\t\\t- CPU sends MMU a virtual address\\n\\t\\t\\t- MMU splits it into a virtual page number and page offset\\n\\t\\t\\t- takes 2 separate accesses to memory\\n\\t\\t\\t\\t1. uses register to read out page number from page table\\n\\t\\t\\t\\t\\t- page table - array of physical page numbers, 2^numbits(virtual page numbers)\\n\\t\\t\\t\\t\\t\\t- page table actually stores page table entries (PTEs)\\n\\t\\t\\t\\t\\t\\t- PTE = PPN, read-only?, code or data?, user allowed to see it? \\n\\t\\t\\t\\t\\t\\t- MMU will check this and fault on error\\n\\t\\t\\t\\t2. then it sends page number and page offset and gets back data\\n\\t\\t\\t\\t\\t- lookup address PTBR + VPN*numbytes(PPN)\\n\\t\\t\\t- consider 32-bit VA, 16k page (too large)\\n\\t\\t\\t\\t- page offset is 14 bits\\n\\t\\t\\t\\t- 2^18 PTEs = 256k PTEs\\n\\t\\t\\t\\t- each PTE could be 4 bytes so the page table takes about 1 Megabyte\\n\\t\\t\\t- 64-bit VA, 4k page\\n\\t\\t\\t\\t- 2^52 PTE -> the page table is too big to store\\n\\t\\t3. good idea: multi-level page table\\n\\t\\t\\t- virtual address: page offset, multiple virtual page numbers $VPN_0,VPN_1,VPN_2,VPN_3$ (could have different number of\\tthese)\\n\\t\\t\\t1. start by reading highest VPN: PTBR[VPN_3] -> PTE_3\\n\\t\\t\\t2. read PPN[VPN_2] -> PTE_2\\n\\t\\t\\t3. read PPN_2[VPN_1] -> PTE_1\\n\\t\\t\\t4. read PPN_1[VPN_0] -> PTE_0\\n\\t\\t\\t5. read PPN_0[VPN] -> PTE_ans\\n\\t\\t\\t- check at each level if valid, if unallocated/kernel memory/not usable then fault and stop looking\\n\\t\\t\\t- highest level VP_n is highest bits of address, likely that it is unused\\n\\t\\t\\t\\t- therefore we don\\'t have to check the other addresses\\n\\t\\t\\t\\t- they don\\'t exist so we save space, only create page tables when we need them - OS does this\\n\\t\\t\\t- look at these in the textbook\\n\\t\\t\\t- virtual memory ends up looking like a tree\\n\\t\\t\\t\\t- top table points to several tables which each point to more tables\\n- TLB - maps from virtual page numbers to physical page numbers\\n- TLB vs L1, L2, etc:\\n- Similarities\\n\\t\\n\\t- They are all caches- i.e., they have an index and a tag and a valid bit (and sets)\\n- Differences\\n\\t- TLB has a 0-bit BO (i.e., 1 entry per block; lg(1) = 0)\\n\\t- TLB is not writable (hence not write-back or write-through, no dirty bit)\\n\\t- TLB entries are PPN, L* entries are bytes\\n\\t- TLB does VPN â†’ PPN; the L* do PA â†’ data\\n\\n## overview\\n- CPU -> creates virtual address\\n- virtual address: 36 bits (VPN), 12 bits (PO) //other bits are disregarded\\n- VPN broken into 32 bits (Tag), 4 bits (set index)\\n\\t- set index tells us which set in the Translation Lookaside Buffer to look at\\n\\t\\t- there are 2^4 sets in the TLB\\n\\t\\t- currently there are 4 entries per set ~ this could be different\\n\\t\\t\\t- each entry has a valid bit\\n\\t\\t\\t- a tag - same length as VP Tag\\n\\t\\t\\t- value - normally called block - but here only contains one Physical page number - PPN = 40 bits ~ this could be different\\n\\t- when you go into kernel mode, you reload the TLB\\n\\t\\t\\n## segments\\n- memory block\\n\\t- kernel at top\\n\\t- stack (grows down)\\n\\t- shared code\\n\\t- heap (grows up)\\n\\t- empty at bottom\\n\\n\\n\\nbase: 0x60\\n\\nread: 0xb6\\nval at 0x6b -> 0x3d\\nval at 0xd6 -> ans\\n\\nread: 0xa4\\nval at 0x6a -> 0x53 \\nval at 0x34 -> ans\\n\\n0xb3a6\\n\\nread: 0xb3\\nval at 0x6b -> 0x3d\\nval at 0xd3 -> 0x0f\\nval at 0xfa -> 0x6b\\nval at 0xb6\\n\\n\\n[TOC]\\n\\n## quiz rvw\\n- commands\\n- floats\\n- labs\\n- In method main you declare an int variable named x. The compiler might place that variable in a register, or it could be in which region of memory? - Stack\\n- round to even is default\\n- Which Y86-64 command moves the program counter to a runtime-computed address? - ret\\n- [] mux defaults to 0\\n- caller-save register - caller must save them to preserve them\\n- callee-saved registers - callee must save them to edit them\\n- in the sequential y86 architecture valA<-eax\\n- valM is read out of memory - used in ret, mrmovl\\n- labels are turned into addresses when we assemble files\\n- accessing memory is slow\\n- most negative binary number: 100000\\n- floats can represent less numbers than unsigned ints\\n\\t- 0s and -0s are same\\n\\t- NaN doesn\\'t count\\n- push/pop - sub/add to %rsp, put value into (%rsp)\\n- opl is 32-bit, opq is 64-bit\\n- fetch determines what the next PC will be\\n- fetch reads rA,rB,icode,ifun - decode reads values from these\\n\\n## labs\\n#### strlen\\n```java\\nunsigned int strlen( const char * s ){\\n\\tunsigned int i = 0; \\n\\twhile(s[i]) \\n\\t\\ti++; \\n\\treturn i;\\n}\\n```\\n#### strsep\\n```java\\nchar *strsep( char **stringp, char delim ){\\n\\tchar *ans = *stringp;\\n\\tif (*stringp == 0) \\n\\t\\treturn 0;\\n\\twhile (**stringp != delim && **stringp != 0) /* don\\'t need this 0 check, 0 is same as \\'\\\\0\\' */\\n\\t\\t*stringp += 1;\\n\\tif (**stringp == delim){ \\n\\t\\t**stringp = 0;\\n\\t\\t*stringp += 1; \\n\\t}\\n\\telse \\n\\t\\t*stringp = 0; \\n\\treturn ans;\\n}\\n```\\n###lists\\n- always test after malloc\\n- singly-linked list: node* { TYPE payload, struct node *next }\\n\\t- length: while(list) list = (*list).next\\n\\t- allocate: malloc(sizeof(node)*length)\\n\\t\\t\\t\\thead[i].next=(i >= length) ? 0 : (head+i+1) \\n\\t- access: (*list).payload or list[i].payload (for accessing)\\n- array: TYPE*\\n\\t- length: while(list[i] != sentinel)\\n\\t- allocate: malloc(sizeof(TYPE) * (length+1));\\n\\t- access: list[i]\\n- range: { unsigned int length, TYPE *ptr }\\n\\t- length: list.length\\n\\t- allocate: list.ptr = malloc(sizeof(TYPE) * length);\\n\\t\\t\\t\\tans.length = length;\\n\\t- access: list.ptr[i]\\n\\t\\n#### bit puzzles\\n```java\\n// leastBitPos - return a mask that marks the position of the least significant 1 bit\\nint leastBitPos(int x) {\\n    return x & (~x+1);\\n}\\nint bitMask(int highbit, int lowbit) {\\n    int zeros = ~1 << highbit; /* 1100 0000 */\\n    int ones = ~0 << lowbit;   /* 1111 1000 */\\n    return ~zeros & ones;      /* 0011 1000 */\\n}\\n/* satAdd - adds two numbers but when positive overflow occurs, returns maximum possible value, and when negative overflow occurs, it returns minimum positive value. */\\n// soln - overflow when operands have same sign and sum and operands have different sign\\nint satAdd(int x, int y) {\\n\\tint x_is_neg = x >> 31;\\n\\tint y_is_neg = y >> 31;\\n\\tint sum = x + y;\\n\\tint same_sign = (x_is_neg & y_is_neg  |  ~x_is_neg & ~y_is_neg);\\n\\tint overflow = same_sign & (x_is_neg ^ (sum >> 31));\\n\\tint pos_overflow = overflow & ~x_is_neg;\\n\\tint neg = 0x1 << 31;\\n\\tint ans = ~overflow&sum | overflow & (pos_overflow&~neg | ~pos_overflow&neg);\\n\\treturn ans;\\n}\\n```\\n\\n## reading\\n#### ch 1 (1.7, 1.9)\\n- files are stored as bytes, most in ascii\\n- all files are either text files or binary files\\n- i/o devices are connected to the bus by a controller or adapter\\n- processor holds PC, main memory holds program\\n- os-layer between hardware and applications - protects hardware and unites different types of hardware\\n- concurrent - instructions of one process are interleaved with another\\n    - does a context switch to switch between processes\\n    - concurrency - general concept of multiple simultaneous activities\\n    - parallelism - use of concurrency to make a system faster\\n- virtual memory-abstraction that provides each process with illusion of full main memory\\n\\t- memory - code-data-heap-shared libraries-stack\\n- threads allow us to have multiple control flows at the same time - switching\\n- multicore processor: either has multicore or is hyperthreaded (one CPU, repeated parts)\\n- processors can do several instructions per clock cycle\\n- Single-Instruction, Multiple-Data (SIMD) - ex. add four floats\\n\\n#### ch 2 (2.1, 2.4.2, 2.4.4)\\n- floating points (float, double)\\n\\t- sign bit (1)\\n\\t- exponent-field (8, 11)\\n\\t\\t- bias = 2^(k-1)-1 ex. 127\\n\\t\\t- normalized\\n\\t\\t\\texponent = exp-Bias, mantissa = 1.mantissa\\n\\t\\t- denormalized: exp - all 0s\\n\\t\\t\\t- exponent = 1-Bias, mantissa without 1 - 0 and very small values\\n\\t\\t- exp: all 1s\\n\\t\\t\\t- infinity (if mantissa 0) \\n\\t\\t\\t- NaN otherwise\\n\\t- mantissa \\n- rounding\\n\\t1. round-to-even - if halfway go to closest even number - avoides statistical bias\\n\\t2. round-toward-zero\\n\\t3. round-down\\n\\t4. round-up\\n- leading 0 specifies octal\\n- leading 0x specifies hex\\n- leading 0b specifies binary\\n\\n#### ch 3 (3.6, 3.7)\\n- computers execute machine code\\n- intel processors are all back-compatible\\n- ISA - instruction set architecture\\n- control - condition codes are set after every instruction (1-bit registers)\\n\\t1. Zero Flag - recent operation yielded 0\\n\\t2. Carry Flag - yielded carry\\n\\t3. Sign Flag - yielded negative\\n\\t4. Overflow Flag - had overflow (pos or neg)\\n- guarded do can check if a loop is infinite\\n- instruction src, destination\\n- parentheses dereference a point\\n- there is a different add command for 16-bit operands than for 64-bit operands\\n- all instructions change the program counter\\n- call instruction only changes the stack pointer\\n\\n#### 4.1,4.2\\n- eight registers\\n\\t- esp is stack pointer\\n- CC and PC\\n- 4-byte values are little-endian\\n- status code State\\n\\t- 1 AOK\\n\\t- 2 HLT\\n\\t- 3 ADR - seg fault\\n\\t- 4 INS - invalid instruction code\\n- lines starting with \".\" are assembler directives\\n- assembly code is assembled resulting in just addresses and instruction codes\\n- pushl %esp - this doesn\\'t change esp\\n- pop %esp - pops the value in esp\\n- high voltage = 1\\n- digital system components\\n\\t1. logic\\n\\t2. memory elements\\n\\t3. clock signals\\n- mux - picks a value and lets it through\\n- int Out = [\\n\\t\\ts: A;\\n\\t\\t1: B; \\n    ];\\n\\t- B is the default\\n- combinatorial circuit - many bits as input simultaneously\\n- ALU - three inputs, A, B, func\\n- clocked registers store individual bits or words\\n- RAM stores several words and uses address to retrieve them\\n\\t- stored in register file\\n\\n#### 4.3.1-4\\n- SEQ - sequential processor\\n- stages\\n\\t- fetch\\n\\t\\t- read icode,ifun <- byte 1\\n\\t\\t- maybe read rA, rB <- byte 2\\n\\t\\t- maybe read valC <- 8 bytes\\n\\t- decode\\n\\t\\t- read operands usually from rA, rB - sometimes from %esp\\n\\t\\t- call these valA, valB\\n\\t- execute\\n\\t\\t- adds something, called valE\\n\\t\\t- for jmp tests condition codes\\n\\t- memory\\n\\t\\t- reads something from memory called valM or writes to memory\\n\\t- write back\\n\\t\\t- writes up to two results to regfile\\n\\t- PC update\\n- popl reads two copies so that it can increment before updating the stack pointer\\n\\t components: combinational logic, clocked registers (the program counter and condition code register), and random-access memories\\t\\t\\n\\t- reading from RAM is fast\\n\\t- only have to consider PC, CC, writing to data memory, regfile\\n- processor never needs to read back the state updated by an instruction in order to complete the processing of this instruction.\\n- based on icode, we can compute three 1-bit signals :\\n\\t1. instr_valid: Does this byte correspond to a legal Y86 instruction? This signal is used to detect an illegal instruction.\\n\\t2. need_regids: Does this instruction include a register specifier byte? \\n\\t3. need_valC: Does this instruction include a constant word?\\n\\t\\n#### 4.4 pipelining\\n- the task to be performed is divided into a series of discrete stages\\n- increases the throughput - ## customers served per unit time\\n- might increase latency - time required to service an individual customer.\\n- when pipelining, have to add time for each stage to write to register\\n- time is limited by slowest stage\\n- more stages has diminishing returns for throughput because there is constant time for saving into registers\\n\\t- latency increases with stages\\n\\t- throughput approaches 1/(register time)\\n- we need to deal with dependencies between the stages\\n\\n#### 4.5.3, 4.5.8\\n- several copies of values such as valC, srcA\\n- registers dD, eD, mM, wW - lowercase letter is input, uppercase is output\\n- we try to keep all the info of one instruction within a stage\\n- merge signals for valP in call and valP in jmp as valA\\n- load/use hazard - (try using before loaded) one instruction reads a value from memory while the next instruction needs this value as a source operand\\n- we can stop this by stalling and forwarding (the use of a stall here is called a load interlock)\\n\\n#### 5 - optimization\\n- eliminate unnecessary calls, tests, memory references\\n- instruction-level parallelism\\n- profilers - tools that measure the performance of different parts of the program\\n- critical paths - chains of data dependencies that form during repeated executions of a loop\\n- compilers can only apply safe operations\\n- watch out for memory aliasing - two pointers desginating same memory location\\n- functions can have side effects - calling them multiple times can have different results\\n- small boost from replacing function call by body of function (although this can be optimized in compiler sometimes)\\n- measure performance with CPE - cycles per element\\n- reduce procedure calls (ex. length in for loop check)\\n- loop unrolling - increase number of elements computed on each iteration\\n- enhance parallelism \\n\\t- multiple accumulators\\n- limiting factors\\n\\t- register spilling - when we run out of registers, values stored on stack\\n\\t- branch prediction - has misprediction penalties, but these are uncommon\\n\\t\\t- trinary operator could make things faster\\n- understand memory performance\\n- using macros lets compiler optimizem more, lessens bookkeeping\\n\\n#### 6.1.1, 6.2, 6.3\\n- SRAM is bistable as long as power is on - will fall into one of 2 positions\\n- DRAM loses its value ~10-100 ms\\n\\t- memory controller sends row,col (i,j) to DRAM and DRAM sends back contents\\n\\t- matrix organization reduces number of inputs, but slower because must use 2 steps to load row then column\\n- memory modules\\n- enhanced DRAMS\\n- nonvolatile memory \\n\\t- ROM - read-only memories - firmwared\\n- accessing main memory\\n\\t- buses - collection of parallel wires that carry address, data, control\\n- accessing main memory\\n- locality\\n\\t- locality of references to program data\\n\\t\\t- visiting things sequentially (like looping through array) - stride-1 reference pattern or sequential reference pattern\\n\\t- locality of instruction fetches\\n\\t\\t- like in a loop, the same instructions are repeated\\n- memory hierarchy\\n\\t- block-sizes for caching can differ between different levels\\n\\t- when accessing memory from cache, we either get cache hit or cache miss\\n\\t- if we miss we replace or evict a block\\n\\t\\t- can use random replacement or least-recently used\\n\\t- cold cache - cold misses / compulsory misses - when cache is empty\\n\\t\\t- need a placement policy for level k+1 -> k (could be something like put block i into i mod 4)\\n\\t- conflict miss - miss because placement policy gets rid of block you need - ex. block 0 then 8 then 0 with above placement policy\\n\\t\\t capacity misses - the cache just can\\'t hold enough\\t\\n\\n#### 6.4, 6.5 - cache memories & writing cache-friendly code\\n- Miss rate. The fraction of memory references during the execution of a program, or a part of a program, that miss. It is computed as #misses/#references.\\n- Hit rate. The fraction of memory references that hit. It is computed as 1 − miss rate.\\n- Hit time. The time to deliver a word in the cache to the CPU, including the time for set selection, line identification, and word selection. Hit time is on the order of several clock cycles for L1 caches.\\n- Miss penalty. Any additional time required because of a miss. The penalty for L1 misses served from L2 is on the order of 10 cycles; from L3, 40 cycles; and from main memory, 100 cycles.\\n- Traditionally, high-performance systems that pushed the clock rates would opt for smaller associativity for L1 caches (where the miss penalty is only a few cycles) and a higher degree of associativity for the lower levels, where the miss penalty is higher\\n- In general, caches further down the hierarchy are more likely to use write-back than write-through\\n\\n#### 8.1 Exceptions\\n- exceptions - partly hardware, partly OS\\n- when an event occurs, indirect procedure call (the exception) through a jump table called exception table to OS subroutine (exception handler).\\n- three possibilities\\n\\t- returns to I_curr\\n\\t- returns to I_next\\n\\t- program aborts\\n- exception table - entry k contains address for handler code for exception k\\n- processor pushes address, some additional state\\n- four classes\\n\\t1. interrupts (the faulting instruction)\\n\\t\\t- signal from I/O device, Async, return next instruction\\n\\t2. traps\\n\\t\\t- intentional exception (interface for making system calls), Sync, return next\\n\\t3. faults\\n\\t\\t- potentially recoverable error (ex. page fault exception), Sync, might return curr\\n\\t4. aborts\\n\\t\\t- nonrecoverable error, Sync, never returns\\n- examples\\n\\t- general protection fault - seg fault\\n\\t- machine check - fatal hardware error\\n\\t\\n#### 8.2 Processes\\n- process - instance of program in execution\\n\\t- every program runs in the context of some process (context has code, data stack, pc, etc.)\\n1. logic control flow - like we have exclusive use of processor\\n\\t- processes execute partially and then are preempted (temporarily suspended)\\n\\t- concurrency/multitasking/time slicing - if things trade off\\n\\t- parallel - concurrent and on separate things\\n\\t- kernel uses context switches \\n2. private address space - like we have exclusive use of memory\\n\\t- each process has stack, shared libraries, heap, executable\\n\\n#### 8.3 System Call Error Handling\\n- system level calls return -1, set the global integer variable errno\\n\\t- this should be checked for\\n\\t\\n#### 9-9.5 Virtual Memory\\n- address translation - converts virtual to physical address\\n\\t- translated by the MMU\\n- VM partitions virtual memory into fixed-size blocks called virtual pages partitioned into three sets\\n\\t1. unallocated\\n\\t2. cached\\n\\t3. uncached\\n- virtual pages tend to be large because cache misses are large\\n- DRAM will be fully associative, write-back\\n- each process has a page table - maps virtual pages to physical pages\\n\\t- managed by OS\\n\\t- has PTEs\\n\\t- PTE - valid bit, n-bit address field\\n\\t\\t- valid bit - whether its currently cached in DRAm\\n\\t\\t- if yes, address is the start of corresponding physical page\\n\\t\\t- if valid bit not set && null address - has not been allocated\\n\\t\\t- if valid bit not set && real address - points to start of virtual page on disk\\n\\t- PTE - 3 permission bits\\n\\t\\t- SUP - does it need to be in kernel (supervisor) mode?\\n\\t\\t- READ - read access\\n\\t\\t- WRITE - write access\\n- page fault - DRAM cache miss\\n\\t- read valid bit is not set - triggers handler in kernel\\n- demand paging - waiting until a miss occurs to swap in a page\\n- malloc creates room on disk\\n- thrashing - not good locality - pages are swapped in and out continuoously\\n- virtual address space is typically larger\\n\\t- multiple virtual pages can be mapped to the same shared physical page (ex. everything points to printf)\\n- VM simplifies many things\\n\\t- linking\\n\\t\\t- each process follows same basic format for its memory image\\n\\t- loading\\n\\t\\t- loading executables / shared object files\\n\\t- sharing\\n\\t\\t- easier to communicate with OS\\n\\t- memory allocation\\n\\t\\t- physical pages don\\'t have to be contiguous\\n- memory protection\\n\\t- private memories are easily isolated\\n\\n#### 9.6 Address Translation\\n- low order 4 bits serve\\n2,3 - fault\\n8c: 1000 1100\\nb6',\n",
       " '---\\nlayout: notes\\ntitle: Java ref\\ncategory: cs\\n---\\n\\n* TOC\\n#  java ref\\n\\n## data structures\\n\\n```java\\n- LinkedList, ArrayList\\n\\t- add(Element e), add(int idx, Element e), get(int idx)\\n\\t- remove(int index)\\n\\t- remove(Object o)\\n- Stack\\n\\t- push(E item)\\n\\t- peek()\\n\\t- pop()\\n- PriorityQueue\\n\\t- peek()\\n\\t- poll()\\n\\t- default is min-heap\\n\\t- PriorityQueue(int initialCapacity, Comparator<? super E> comparator)\\n\\t- PriorityQueue(Collection<? extends E> c)\\n- HashSet, TreeSet\\n\\t- add, remove\\n- HashMap\\n\\t- put(K key, V value)\\n\\t- get(Object key)\\n\\t- keySet()\\n\\t- if you try to get something that\\'s not there, will return null\\n```\\n- default init capacities all 10-20\\n- clone() has to be cast from Object\\n\\n## useful\\n*iterator*\\n\\n```java\\n- it.next() - returns value\\n- it.hasNext() - returns boolean\\n- it.remove() - removes last returned value\\n```\\n\\n*strings*\\n\\n```java\\n- String.split(\" |\\\\\\\\.|\\\\\\\\?\") //split on space, ., and ?\\n- StringBuilder\\n\\t- much faster at concatenating strings\\n\\t- thread safe, but slower\\n\\t- StringBuilder s = new StringBuilder(CharSequence seq)();\\n\\t- s.append(\"cs3v\");\\n\\t- s.charAt(int x), s.deleteCharAt(int x), substring\\n\\t- s.reverse()\\n\\t- Since String is immutable it can safely be shared between many threads\\n- formatting\\n\\tString s = String.format(\"%d\", 3);\\n\\t\"%05d\"\\t//pad to fill 5 spaces\\n\\t\"%8.3f\" //max number of digits\\n\\t\"%-d\"\\t//left justify\\n\\t\"%,d\" \\t//print commas ex. 1,000,000\\n\\t| int | double | string |\\n\\t|-----|--------|--------|\\n\\t| d   | f      | s      |\\n\\tnew StringBuilder(s).reverse().toString()\\n\\tint count = StringUtils.countMatches(s, something);\\n- integer\\n\\t- String toString(int i, int base)\\n\\t- int parseInt(String s, int base)\\n- array\\n\\tchar[] data = {\\'a\\', \\'b\\', \\'c\\'};\\n\\tString str = new String(data);\\n```\\n\\n*sorting*\\n\\n```java\\n- Arrays.sort(Array a)\\n- Collections.sort(Collection c), Collections.sort(Collection l, Comparator c)\\n\\t- use mergeSort (with insertion sort if very small)\\n- Collections.reverseOrder() returns comparator opposite of default\\nclass ComparatorTest implements Comparator<String>\\n\\tpublic int compare(String one, String two) //if negative, one comes first\\nclass Test implements Comparable<Object>\\n\\tpublic int compareTo(Object two)\\n```\\n\\n*exceptions*\\n- ArrayIndexOutOfBoundsException\\n- `throw new Exception(\"Chandan type\")`\\n\\n## higher level\\n- *primitives* - `byte, short, char, int, long, float, double`\\n- java only has primitive and reference *types*\\n  - when you assign primitives to each other, it\\'s fine\\n  - when you pass in a primitive, its value is copied\\n  - when you pass in an object, its reference is copied\\n    - you can modify the object through the reference, but can\\'t change the object\\'s address\\n- *garbage collection*\\n  - once an object no longer referenced, gc removes it and reclaims memory\\n  - jvm intermittently runs a mark-and-sweep algorithm\\n    - runs when short-term stuff gets full\\n    - older stuff moves to different part\\n    - eventually older stuff is cleared\\n\\n## object-oriented\\n| declare | instantiate | initialize |\\n| ------- | ----------- | ---------- |\\n| Robot k | new         | Robot()    |\\n- *class method* = *static*\\n  - called with Foo.DoIt()\\n  - initialized before constructor\\n  - class shares one copy, can\\'t refer to non-static\\n- *instance method* - invoked on specific instance of the class\\n  -  called with f.DoIt()\\n- *protected* member is accessible within its class and subclasses',\n",
       " '---\\nlayout: notes\\ntitle: cs theory\\ncategory: cs\\n---\\n\\n#  cs theory\\n\\nSome notes on theoretical computer science, based on UVA\\'s course.\\n\\n## introduction\\n- Chomsky hierarchy of languages: $L_3 \\\\subset L_2 \\\\subset L_1 \\\\subset L_R \\\\subset L_0 \\\\subset Σ*$\\n  - each L is a set of languages\\n  - $L_0=L_{RE}$ - unrestricted grammars - general phase structure grammars - recursively enumerable languages - include all formal grammars. They generate exactly all languages that can be recognized by a Turing machine.\\n    - computable, maybe undecidable (if not in L_R)\\n  - L_R - recursive grammars - Turing machine that halts eventually\\n    - decidable\\n  - L_1 - context-sensitive grammars - all languages that can be recognized by a linear bounded automaton\\n  - L_2 - context-free grammars - these languages are exactly all languages that can be recognized by a non-deterministic pushdown automaton.\\n  - L_3 - regular grammars - all languages that can be decided by a finite state automaton\\n    - contains Σ*, $\\\\vert Σ*\\\\vert $ is countably infinite\\n- strings\\n- languages\\n  - Σ* Kleene Closure has multiple definitions\\n    - {w $\\\\vert $ w is a finite length string ^ w is a string over Σ}\\n    - {xw $\\\\vert $ w in Σ* ^ x in Σ} U {Ɛ}\\n  - Σ_i has strings of length i\\n- problems\\n- automata\\n  - delta v delta-hat - delta hat transitions on a string not a symbol\\n  - $\\\\vert $- notation writes the state between the symbols you have read and have yet to read\\n  - $\\\\vert $- notation with * writes the state before the symbols you have to read and after what you have read\\n- grammars\\n  - leftmost grammar - expand leftmost variables first - doesn\\'t matter for context-free\\n  - parse tree - write string on bottom\\n- sets\\n  - finite\\n  - countably infinite\\n  - not countably infinite\\n- mappings\\n  1. onto - each output has at least 1 input\\n  2. 1-1 - each output has at most 1 input\\n  3. total - each input has at least 1 output\\n  4. function - each input has at most 1 output\\n  - equivalence relation - reflexive, symmetric, transitive\\n- proof methods\\n  - **read induction **\\n- library of babel\\n  - distinct number of books, each contained, but infinite room\\n\\n## ch 1-3 - finite automata, regular expressions\\n- alphabet - any nonempty finite set\\n- string - finite sequence of symbols from an alphabet\\n- induction hypothesis - assumption that P(i) is true\\n- lexicographic ordering - {Ɛ,0,1,00,01,10,11,000,...}\\n- finite automata - like a Markov chain w/out probabilities - 5 parts\\n  1. states\\n  2. E - finite set called the alphabet \\n  3. f: Q x E -> Q is the transition function\\n    - ex. f(q,0) = q\\'\\n  4. start state\\n  5. final states\\n- language - L(M)=A - means A is the set of all strings that the machine M accepts\\n- A* = {$x_1x_2...x_k \\\\vert  k\\\\geq0 \\\\wedge x_i \\\\in A$}\\n- A+ = A* - Ɛ\\n- concatenation A o B = {xy $\\\\vert $ x in A and y in B}\\n- regular language - is recognized by a finite automata\\n  - class of regular languages is closed under union, concatenation, star operation\\n  - nondeterministic automata\\n    - can have multiple transition states for one symbol\\n    - can transition on Ɛ\\n    - can be thought of as a tree\\n    - After reading that symbol, the machine splits into multiple copies of itself and follows all the possibilities in parallel. Each copy of the machine takes one of the possible ways to proceed and continues as before. If there are subsequent choices, the machine splits again. \\n    - If the next input symbol doesn\\'t appear on any of the arrows exiting the current state, that copy of the machine dies. \\n    - if any copy is in an accept state at the end of the input, the NFA accepts the input string.\\n  - can also use regular expressions (stuff like unions) instead of finite automata \\n    - to convert, first convert to gnfa\\n  - gnfa (generalized nfa) - start state isn\\'t accept state\\n- nonregular languages - isn\\'t recognized by a finite automata\\n  - ex. C = {w $\\\\vert $ w has an equal number of Os and 1s}\\n  - requires infinite states\\n\\n## ch 4 - properties of regular languages (except Sections 4.2.3 and 4.2.4) \\n- pumping lemma- proves languages not to be regular\\n- if L regular, there exists a constant n such that for every string w in L such that \\\\vert w\\\\vert  ≥ n, we can break w into 3 strings w=xyz, such that:\\n  1. y≠Ɛ\\n  2. $\\\\vert xy\\\\vert $ ≤ n\\n  3. For all k ≥ 0, x y^k z is also in L\\n- closed under union, intersection, complement, concatenation, closure, difference, reversal\\n- convert NFA to DFA - write the possible routes to the final state, write the intermediate states, remove unnecessary ones\\n- minimization of DFAs\\n  - eliminate any state that can\\'t be reached\\n  - partition remaining states into blocks so all states in same block are equivalent\\n    - can\\'t do this grouping for nfas\\n\\n## ch 5 - context free grammars and languages\\n- $w^R$ = reverse\\n- context-free grammar - more powerful way to describe a language\\n- ex. substitution rules (generates 0#1)\\n  - A -> OA1\\n  - A -> B\\n  - B -> #\\n- def\\n  1. variables - finite set\\n  2. terminals - alphabet\\n  3. productions \\n  4. start variable\\n- recursive inference - start with terminals, show that string is in grammar\\n- derivation - sequence of substitutions to obtain a string\\n  - can also make these into parse trees\\n- leftmost derivation - at each step we expand leftmost variable\\n- arrow with a star does many derivations at once\\n- parse tree - final answer is at bottom\\n- sentential form - the string at any step in a derivation\\n- proofs in 5.2s\\n- w equivalence\\n  1. parse tree\\n  2. leftmost derivation\\n  3. rightmost derivation\\n  4. recursive inference\\n  5. derivation\\n- if else grammar: $S \\\\to \\\\epsilon \\\\vert  SS \\\\vert  iS \\\\vert  iSeS $\\n- context-free grammars used for parsers (compilers), matching parentheses, palindromes, if-else, html, xml\\n- if a grammar generates a string in several different ways, we say that the string is derived ambiguously in that grammar\\n- ambiguity resolution\\n  1. some operators take precedence\\n  2. make things left-associative\\n  - think about terms, expressions, factors\\n  - if unambiguous, leftmost derivation will be unique\\n- in an unambiguous grammar, leftmost derivations will be unique\\n- inherently ambiguous language - all its grammars are ambiguous\\n  - ex: $L = {a^nb^nc^md^m} \\\\cup {a^nb^mc^md^n} , n \\\\geq 1, m\\\\geq1$\\n\\n## ch 6 - pushdown automata (don\\'t need to know 6.3 proofs)\\n- pushdown automata - have extra stack of memory - equivalent to context-free grammar\\n  - similiar to parser in typical compiler\\n- two ways of accepting\\n  1. entering accept state\\n  2. accept by emptying stack\\n  - convert from empty stack to accept state\\n    - add symbol X_1\\n    - start by pushing it onto the stack then push on Z_1, spontaneously transition to q_0\\n    - everything has epsilon-transition to final accepting state when they read X_1\\n  - convert accept state to empty stack\\n    - add symbol X_1 under Z_1 (this is so we never empty stack unless we are in p- there are no transitions on X_1)\\n    - all accept states transition to new state p\\n    - p epsilon-transitions to itself, removes element from each stack every time\\n- 6.3\\n  - convert context free grammar to empty stack\\n    - simulate leftmost derivations\\n    - put answer on stack, most recent variable on top\\n    - if terminal remove\\n    - if variable nondeterministically expand\\n    - if empty stack, accept\\n  - convert PDA to grammar\\n    - every transition is of from pXq\\n    - variables of the form [pXq] (X is on the stack)\\n      - [pXq] -> a where a is what transitioned p to q\\n- pushdown automata can transition on epsilon\\n- def:\\n  1. transition function - takes (state,symbol,stack symbol) - returns set of pairs (new state, new string to put on stack - length 0, 1, or more)\\n  2. start state\\n  3. start symbol (stack starts with one instance of this symbol)\\n  4. set of accepting states\\n  5. set of all states\\n  6. alphabet\\n  7. stack alphabet\\n- ex. palindromes\\n  1. push onto stack and continue OR\\n  2. assume we are in middle and start popping stack - if empty, accept input up to this point\\n- label diagrams with i, X/Y - what input is used and new/old tops of stack\\n- ID for PDA: (state,remaining string,stack)\\n  - conventionally, we put top of stack on left\\n- parsers generally behave like deterministic PDA\\n- DPDA also includes all regular languages, not all context free languages\\n  - only include unambiguous grammars\\n\\n## ch 7 - properties of CFLs\\n\\n- Chomsky Normal Form\\n  1. A->BC\\n  2. A->a\\n    - no epsilon transitions\\t\\t\\n    - for any variable that derived to epsilon (ex. A -*> epsilon)\\n      - if B -> CAD\\n      - replace with B -> CD and B -> CAD and remove all places where A could become epsilon\\n  - no unit productions\\n  - eliminate useless symbols\\n  - works for any CFL\\n- Greibach Normal Form\\n  1. A->aw where a is terminal, w is string of 0 or more variables\\n  2. every derivation takes exactly n steps (n length)\\n- generating - if x produces some terminal string w\\n- reachable - x reachable if S ${\\\\to}^*$ aXb for some a,b \\n- CFL pumping lemma - pick two small strings to pump\\n- If L CFL, then $\\\\vert z\\\\vert  \\\\geq n$, we can break z into 5 strings z=uvwxy, such that:\\n  1. vx ≠ Ɛ\\n  2. $\\\\vert vwx\\\\vert  \\\\leq n$, middle portion not too long\\n  3. For all i ≥ 0, $u v^i w x^i y \\\\in$ L\\n  - ex. $\\\\{0^n1^n\\\\}$\\n  - often have to break it into cases\\n  - proof uses Chomsky Normal Form\\n- not context free examples\\n  - $\\\\{0^n1^n2^n\\\\vert n\\\\geq1\\\\}$\\n  - {$0^i1^j2^i3^j\\\\vert i\\\\geq 1,j\\\\geq 1$}\\n  - {ww$\\\\vert w \\\\in \\\\{0,1\\\\}^*$ }\\n- closed under union, concatenation, closure, and positive closure, homomorphism, reversal, inverse homomorphism, substitutions\\n  - intersection with a regular language (basically run in parallel)\\n- not closed under intersection, complement\\n- substitution - replace each letter of alphabet with a language\\n  - s(a) = $L_a$\\n  - if $w = ax$, $s(w) = L_aL_x$\\n  - if L CFL, s(L) CFL\\n- time complexities\\n  - O(n)\\n    - CFG to PDA\\n    - PDA final state -> empty stack\\n    - PDA empty stack -> final state\\n  - PDA to CFG: O($n^3$) with size O(n^3)\\n  - converstion to CNF: O(n^2) with size O(n^2)\\n  - emptiness of CFL: O(n)\\n- testing emptiness - O(n)\\n  - which symbols are reachable\\n- test membership with dynamic programming table - O(n^3)\\n  - CYK algorithm\\n\\n## ch 8 - intro to turing machines (except 8.5.3)\\n- Turing Machine def\\n  1. states\\n  2. start state\\n  3. final states\\n  4. input symbols\\n  5. tape symbols (includes input symbols)\\n  6. transition function $\\\\delta(q,X)=\\\\delta(q,Y,L)$\\n  7. B - blank symbol\\n    - infinite blanks on either side\\n- arc has X/Y  D with old/new tape symbols and direction\\n- if the TM enters accepting state, it accepts\\n   - assume it halts if it accepts\\n- we can think of Turing machine as having multiple tracks (symbol could represent a tuple like [X,Y])\\n- multitape TM has each head move independently, multitrack doesn\\'t\\n  - common use one track for data, one track for mark\\n\\n- running time - number of steps that TM makes\\n- NTM - nondeterministic Turing machine - accepts no languages not accepted by a deterministic TM\\n- halts if enters a state q, scanning X, and there is no move for (q,X)\\n- restrictions that don\\'t change things\\n  - tape infinite only to right\\n  - TM can\\'t print blank\\n- simplified machines\\n  - two stacks machine - one stack keeps track of left, one right\\n  - every recursively enumerable language is accepted by a two-counter machine\\n- TM can simulate computer, and time is some polynomial multiple of computer time (O(n^3))\\n  - limit on how big a number computer can store - one instruction - word can only grow by 1 bit\\n- LBA - linear bounded automaton - Turing machine with left and right end markers\\n- programs might take infinitely long before terminating - can\\'t be decided\\n- turing machine can take 2 inputs: program P and input I\\n- ID - instantaneous description      \\n  - write $X_1X_2...qX_iX_{i+1}...$ where q is scanning X_i\\n- program that prints \"h\" as input -> yes or no\\n  - imagine instead of no prints h\\n  - now feed it to itself\\n    - if it would print h, now prints yes - paradox! therefore such a machine can\\'t exist\\n- TM simulating computer\\n  1. tape that has memory\\n  2. tape with instruction counter\\n  3. tape with memory address\\n- reduction - we know X is undecidable - if solving Y implies solving X, then Y is undecidable\\n  - if X reduces to Y, solving Y solves X\\n  - define a total mapping from X to Y\\n    - $X \\\\leq _m Y$ - X reduces to Y - mapping reduction, solving Y solves X\\n- intractable - take a very long time to solve (not polynomial)\\n- <> notation means bitstring representation\\n- $<n> = 0^n$\\n- $<m,w> means w \\\\in L(M)$\\n- KD - \"known to be distinct\"\\n- idempotent - R + R = R\\n\\n## ch 9 - undecidability (9.1,9.2,9.3)\\n- does this TM accept (the code for) itself as input?\\n- enumerate binary strings - add a leading 1\\n- express TM as binary string\\n  - give it a number\\n  - TM uses this for each transition\\n    - separate transitions with 11\\n- diagonalization language - set of strings w_i such that w_i is not in L(M_i)\\n  - make table with M_i as rows, w_j as cols\\n  - complement the diagonal is characteristic vector in L_d\\n    - diagonal can\\'t be characteristic vector of any TM\\n  - not RE\\n- recursive - complement is also recursive\\n  - just switch accept and reject\\n- if language and complement are both RE, then L is recursive\\n- universal language - set of binary strings that encode a pair (M,W) where M is TM, w $\\\\in (0+1)^*$ - set of strings representing a TM and an input accepted by that TM\\n- there is a universal Turing machine such that L_u = L(U)\\n  - L_u is undecidable: RE but not recursive\\n- halting problem - RE but not recursive\\n- Rice\\'s Thm - all nontrivial properties of the RE languages are undecidable\\n  - property of the RE languages is a set of RE languages\\n  - property is trivial if it is either empty or is all RE languages\\n    - empty property $\\\\emptyset$ is different from the property of being an empty language {$\\\\emptyset$}\\n  - ex. \"the language is context-free, empty, finite, regular\"\\n- however properties such as 5 states are decidable\\n\\n## Ch 10 - 10.1-10.4 know the additional problems that are NP-complete \\n- intractable - can\\'t be solved in polynomial time\\n- NP-complete examples\\n  1. boolean satisfiability\\n    1. symbols ^-, etc. are represent by themselves\\n    2. x_i is represented by x followed by i in binary\\n    - Cook\\'s thm - SAT is NP-complete\\n      1. show SAT in NP\\n      2. show all other NP reduce to SAT\\n      - pf involves matrix of cell/ID facts\\n        - cols are ID 0,1,...,p(n)\\n        - rows are alpha_0,alpha_1,...alpha_p(n)\\n        - for any problem\\'s machine M, there is polynomial-time-converter for M that uses SAT decider to solve in polynomial time\\n  2. 3SAT - easier to reduce things to\\n    - AND of clauses each of which is the OR of exactly three variables or negated variables\\n    - conjunctive normal form - if it is the AND of clauses\\n    - conversion to cnf isn\\'t always polynomial time - don\\'t have to convert to equivalent expression, just have to both be satisfiable at the same times\\n      1. push all negatives down the expression tree - linear\\n      2. put it in cnf - demorgans, double negation\\n    - literal - variable or a negated variable\\n    - k-conjunctive normal form - k is number of literals in clauses\\n  3. traveling salesman problem - find cycle of weight less than W\\n    - O(m!)\\n  4. Independent Set - graph G and a lower bound k - yes if and only if G has an indpendent set of k nodes\\n    - none of them are connected by an edge\\n    - reduction from 3SAT\\n  5. node-cover problem\\n    - node cover - every node is on one of the edges \\n  6. Undirected Hamiltonian circuit problem\\n    - TSP with all weights 1\\n  7. Directed Hamiltonian-Circuit Problem\\n  8. subset sum\\n    - is there a subset of numbers that sums to a number \\n- reductions must be polynomial-time reductions\\n- P - solvable in polynomial by deterministic TM\\n- NP - solvable in polynomial time by nondeterministic TM\\n  - NP-completeness (Karp-completeness) - a problem is at least as hard as any problem in NP = for every language L\\' in NP, there is a polynomial-time reduction of L\\' to L\\n  - Cook-completeness equivalent to NP-completeness - if given a meachansim that in one unit of time would answer any equestion about membership of a string in P, it was possible to recognize any language in NP in polynomial time\\n  - NP-hard - we don\\'t know if L is in NP, but every problem  in NP reduces to L in polynomial time\\n- if some NP-complete problem p is in P then P=NP\\n- there are things between polynomial and exponential time (like 2^logn), and we group these in with the exponential category\\n- could have P polynomials run forever when they don\\'t accept\\n  - could simply tell them to stop after a certain amount of steps\\n- there are algorithms called verifiers\\n\\n## more on NP Completeness\\n- a language is polynomial-time reducible to a language B if there is a polynomial time comoputable function that maps one to the other\\n- to solve a problem, efficiently transform to another problem, and then use a solver for the other problem\\n- satisfiability problem - check if a boolean expression is true\\n  - have to test every possible boolean value - 2^n where n is number of variables\\n    - this can be mapped to all problems of NP\\n    - ex. traveling salesman can be reduced to satisfiability\\n- P - set of all problems that can be solved in polynomial time\\n- NP - solved in polynomial time if we allow nondeterminism\\n  - we count the time as the length of the shortest path\\n- NP-hard problem L\\'\\n  1. every L is NP reduces to L\\' in polynomial time\\n- NP-complete L\\'\\n  1. L\\' is NP-hard\\n  2. L\\' is in NP\\n  - ex. graph coloring\\n  - partitioning into equal sums\\n- if one NP-complete problem is in P, P=NP\\n- decider vs. optimizer\\n  - decider tells whether it was solved or not\\n  - if you keep asking it boolean questions it gives you the answer\\n- graph clique problem - given a graph and an integer k is there a subgraph in G that is a complete graph of size k\\n  - this is reduction from boolean satisfiability\\n- graph 3-colorability\\n  - reduction from satisfiability - prove with or gate type structure\\n- approximation algorithms\\n  - find minimum\\n    - greedy - keep going down\\n    - genetic algorithms - pretty bad\\n  - minimum vertex cover problem - given a graph, find a minimum set of vertices such that each edge is incident to at least one of these vertices\\n    - NP-complete\\n    - can not be approximated within 1.36*solution \\n    - can be approximated within 2*solution in linear time\\n      - pick an edge, pick its endpoints\\n      - put them in solution\\n      - eliminate these points and their edges from the graph\\n      - repeat\\n- maximum cut problem - given a graph, find a partition of the vertices maximizing the number of crossing edges\\n  - can not be approximated within 17/16*solution\\n  - can be approximated within 2*solution\\n    - if moving arbitrary node across partition will improve the cut, then do so\\n    - repeat',\n",
       " '---\\nlayout: notes\\ntitle: C/C++ ref\\ncategory: cs\\n---\\n\\n* TOC\\n#  c/c++ ref\\n\\n- The C memory model: global, local, and heap variables. Where they are stored, their properties, etc.\\n- Variable scope\\n- Using pointers, pointer arithmetic, etc.\\n- Managing the heap: malloc/free, new/delete\\n\\n## C basic\\n- #include <stdio.h>\\n- printf(\"the variable \\'i\\' is: %d\", i);\\n- can only use /* */ for comments\\n- for constants: ```#define MAX_LEN 1024```\\n\\n## malloc\\n- malloc ex. \\n- There is no bool keyword in C\\n\\n```c\\n  /* We\\'re going to allocate enough space for an integer \\n     and assign 5 to that memory location */\\n  int *foo;\\n  /* malloc call: note the cast of the return value\\n     from a void * to the appropriate pointer type */\\n  foo = (int *) malloc(sizeof(int));  \\n  *foo = 5;\\n   free(foo);\\n```\\n- `char *some_memory = \"Hello World\";`\\n- this creates a pointer to a read-only part of memory\\n- it\\'s disastrous to free a piece of memory that\\'s already been freed\\n- variables must be declared at the beginning of a function and must be declared before any other code\\n\\n## memory\\n- heap variables stay - they are allocated with malloc\\n- local variables are stored on the stack\\n- global variables are stored in an initialized data segment\\n\\n## structs\\n```c\\n  struct point {\\n    int x;\\n    int y;\\n  };\\n struct point *p;\\n p = (struct point *) malloc(sizeof(struct point));\\n p->x = 0;\\n p->y = 0;\\n```\\n\\n## strings\\n- array with extra null character at end \\'\\\\0\\'\\n- strLen doesn\\'t include null character\\n\\n## pointers\\n```c\\nint fake = NULL;\\nint val = 20;\\nint * x; // declare a pointer\\nx = &val; //take address of a variable\\n- can use pointer ++ and pointer -- to get next values\\n```\\n\\n//Hello World\\n#include <iostream>\\nusing namespace std; //always comes after the includes, like a weaker version of packages\\n//everything needs to be in a namespace otherwise you have to writed std::cout to look in iostream - you would use this for very long programs\\nint main(){ //main function, not part of a class, must return an int\\n    cout << \"Hello World\" << endl;\\n    return 0; //always return this, means it didn\\'t crash\\n}\\n\\nPreprocessor\\n    #include <iostream> //System file - angle brackets\\n    #include \"ListNode.h\" //user file - inserts the contents of the file in this place\\n    #ifndef - \"if not defined\"\\n    #define - defines a macro (direct text replacement)\\n    #define TRUE 0 //like a final int, we usually put it in all caps\\n    if(TRUE ==0)\\n    #define MY_OBJECT_H //doesn\\'t give it a value - all it does is make #ifdef true and #ifndef false\\n    #if/#ifdef needs to be closed with #endif\\n    if 2 files include each other, we get into an include loop\\n        we can solve this with the header of the .h files - everything is only defined once\\n        odd.h: \\n            #ifndef ODD_H\\n            #define ODD_H\\n            #include \"even.h\"\\n            bool odd (int x);\\n            #endif\\n        even.h:\\n            #ifndef EVEN_H\\n            #define EVEN_H\\n            #include \"odd.h\"\\n            bool even (int x);\\n            #endif\\n\\nI/O\\n    #include <iostream>\\n    using namespace std; \\n    int main(){\\n        int x;\\n        cout << \"Enter a value for x: \"; //the arrows show you which way the data is flowing\\n        cin >> x;\\n        return 0;\\n    }\\n\\nC++ Primitive Types\\n    int\\n        can be 16,32,64 bits depending on the platform\\n    double better than char\\n\\nIf statement can take an int, if (0) then false.  Otherwise true.\\n    //don\\'t do single equals instead of double equals, will return false\\n\\nCompiler: clang++\\n    \\nFunctions - you can only call methods that are above you in the file\\n    function prototype - to compile mutually recursive functions, you need to declare the function with a semicolon instead of brackets and no body.\\n    bool even(int x); //called forward declaration / function prototype\\n    bool odd(int x){\\n        if(x==0)\\n            return false;\\n        return even(x-1);\\n    }\\n    bool even(int x){\\n        if(x==0)\\n            return true;\\n        return odd(x-1);\\n    }\\nClasses\\n    Need 3 Separate files:\\n    1. Header file that contains class definition - like an interface - IntCell.h\\n        #ifndef INTCELL_H //all .h files start w/ these\\n        #define INTCELL_H\\n        class IntCell{\\n            public: //visibility blocks, everything in this block is public\\n                IntCell(int initialValue=0); //if you don\\'t provide a parameter, it assumes it is 0.  You can call it with 1 or no parameters.\\n                ~IntCell(); //destructor, takes no parameters        \\n        int getValue() const; //the const keyword when placed here means the method doesn\\'t modify the object\\n                void setValue(int val);\\n            private:\\n                int storedvalue;\\n                int max(int m);\\n        };\\n        #endif //all .h files end w/ these\\n    2. C++ file that contains class implementation -IntCell.cpp\\n        #include \"IntCell.h\" \\n        using namespace std; // (not really necessary, but...)\\n        IntCell::IntCell( int initialValue ) :  //default value only listed in .h file\\n                  storedValue( initialValue ) { //put in all the fieldname(value), this is shorthand\\n        }\\n        int IntCell::getValue( ) const { \\n            return storedValue; \\n        }\\n        void IntCell::setValue( int val ) { //this is how you define the body of a method\\n            storedValue = val; \\n        } \\n        int IntCell::max(int m){\\n            return 1;\\n        }\\n    3. C++ file that contains a main() - TestIntCell.cpp\\n        #include <iostream>\\n        #include \"IntCell.h\"\\n        using namespace std;\\n        int main(){\\n            IntCell m1; //calls default constructor - we don\\'t use parentheses!\\n            IntCell m2(37);\\n            cout << m1.getValue() << \" \" << m2.getValue() << endl;\\n            m1 = m2; //there are no references - copies the bits in m2 into m1\\n            m2.setValue(40);\\n            cout << m1.getValue() << \" \" << m2.getValue() << endl;\\n            return 0;\\n        }\\n               \\nPointers\\n    Stores a memory address of another object //we will assume everyhing is 32 bit\\n        Can be a primitive type or a class type\\n    int * x;\\n        pointer to int\\n    char *y;\\n        pointer to char\\n    Rational * rPointer;\\n        pointer to Rational\\n    all pointers are 32 bits in size because they are just addresses\\n    in a definition, * defines pointer type: int * x;\\n    in an expression, * dereferences: *x=2; (this sets a value for what the pointer points to)\\n    in a definition, & defines a reference type\\n        &x means get the address of x\\n    int x = 1;              //Address 1000, value 1 - don\\'t forget to make the pointee\\n    int y = 5;              //Address 1004, value 5\\n    int * x_pointer = &x;   //Address 1008, value 1000\\n    cout << x_pointer;      //prints the address 1000\\n    cout << *x_pointer;     //prints the value at the address\\n    *x_pointer = 2;         //this changes the value of x to 2\\n    x_pointer = &y;         //this means x_pointer now stores the address of y\\n    *x_pointer = 3;         //this changes the value of y to 3\\n               \\n    int n = 30;\\n    int * p;                //variables are not initialized to any value\\n    *p = n;                 //this throws an error because you have not requested enough memory, unless it happens to be pointing to memory that you have allocated\\n    int *p = NULL;          //this will still crash, but it is a better way to initialize\\n               \\n    void swap(int * x, int * y) {\\n        int temp = *x;      //temp takes the value x is pointing to\\n        *x = *y;            //x points to the value that y was pointing to\\n        *y = temp;          //y points to the value 3\\n    }                       //at the end, x and y still are the same addresses\\n              \\n    int main() {\\n        int a=0;\\n        int b=3;\\n        cout << \"Before swap(): a: \" << a << \"b: \" \\n             << b << endl;\\n        swap(&b,&a);\\n        cout << \"After swap(): a: \" << a << \"b: \" \\n             << b << endl;\\n        return 0;\\n    }\\nDynamic Memory Allocation   //not very efficient\\n    Static Memory Allocation - the compiler knows at compile time how much memory is needed\\n        int someArray[10];  //declare array of 10 elements\\n        int *value1_address = &someArray[3]; // declare a pointer to int\\n    new keyword\\n        returns a pointer to newly created \"thing\"\\n        int main() {\\n            int n;\\n            cout << \"Please enter an integer value: \" ;         // read in a value from the user\\n            cin >> n;\\n            int * ages = new int [n];// use the user\\'s input to create an array of int using new\\n            for (int i=0; i < n; i++) {                // use a loop to prompt the user to initialize array\\n                cout << \"Enter a value for ages[ \" << i << \" ]: \";\\n                cin >> ages[i];\\n            }\\n            for(int i=0; i<n; i++) {            // print out the contents of the array\\n                cout << \"ages[ \" << i << \" ]: \" << ages[i];\\n            delete [] ages;  //finished with the array - clean up the memory used by calling delete\\n            return 0;        //everything you allocate with new needs to be deleted, this is faster than java\\n        }\\n    Generally, SomeTypePtr = new SomeType;\\n        int * intPointer = new int;\\n        delete intPointer; //for array, delete [] ages; -this only deals with the pointee, not the pointer\\n    Accessing parts of an object\\n        regular object:\\n            Rational r;\\n            r.num = 3;\\n        for a pointer, dereference it:\\n            Rational *r = new Rational();\\n            (*r).num=4; //or r->num = 4; (shorthand)\\n    char* x,y; //y is not a pointer!  Write like char *x,y;\\nLinked Lists\\n    List object keeps track of size, pointers to head, tail\\n        head and tail are dummy nodes\\n    ListNode holds a value, previous, and next\\n    ListItr has pointer to current ListNode\\nFriend\\n    class ListNode {\\n    public:\\n        ListNode();                //Constructor\\n    private:                       //only this class can modify these fields\\n        int value;\\n        ListNode *next, *previous; //for doubly linked lists\\n        friend class List;         //these classes can bypass private visibility\\n        friend class ListItr;\\n    };\\nConstructor - just has to initialize fields\\n    Foo() {\\n        ListNode* head = new ListNode(); //because we put the class type ListNode*, then we are creating a new local variable and not modifying the field\\n                                        //head = new Listnode() - this works\\n    }\\n    Foo() {\\n      ListNode temp;\\n      head = &temp;                 //this ListNode is deallocated after the constructor ends, doesn\\'t work\\n    }\\n    Assume int *x has been declared\\n    And int y is from user input\\n    Consider these separate C++ lines of code:\\n    x = new int[10]; // 40 bytes\\n    x = new int;     // 4 bytes\\n    x = new int[y];  // y*4 bytes\\nsizeof(int) -> tells you how big an integer is (4 bytes)\\nReferences - like a pointer holds an address, with 3 main differences\\n    1. Its address cannot change (its address is constant)\\n    2. It MUST be initialized upon declaration\\n        Cannot (easily) be initialized to NULL\\n    3. Has implicit dereferencing\\n        If you try to change the value of the reference, it automatically assumes you mean the value that the reference is pointing to\\n    //can\\'t use it when you need to change ex. ListItr has current pointer that changes a lot\\n    Declaration\\n        List sampleList\\n        List & theList = sampleList;//references has to be initialized to the object, not the address\\n    void swap(int & x, int & y) {   //this passes in references\\n        int temp = x;\\n        x = y;\\n        y = temp;\\n    }\\n    int main() {                    //easier to call, references are nice when dealing with parameters\\n        int a=0;\\n        int b=3;\\n        cout << \"Before swap(): a: \" << a << \"b: \"\\n             << b << endl;\\n        swap(b,a);\\n        cout << \"After swap(): a: \" << a << \"b: \" \\n             << b << endl;\\n        return 0;\\n    }\\n    You can access its value with just a period\\n    Location\\t       *\\t           &\\n    Definition\\t\"pointer to\"\\t\"reference to\"\\n    Statement\\t\"dereference\"\\t\"address of\"\\nsubroutines \\n    methods are in a class\\n    functions are outside a class\\nParameter passing\\n    Call by value - actual parameter is copied into formal parameter\\n        This is what Java always does - can be slow if it has to copy a lot\\n            -actual object can\\'t be modified\\n    Call by reference - pass references as parameters\\n        Use when formal parameter should be able to change the value of the actual argument\\n        void swap (int &x, int &y);\\n    Call by constant reference - parameters are constant and are passed by reference\\n        Both efficient and safe\\n        bool compare(const Rational & left, const Rational & right);\\n    Can also return by different ways\\nC++ default class\\n    1. Destructor                                           //this will do nothing\\n        Frees up any resources allocated during the use of an object\\n    2. Copy Constructor                                     //copies something over\\n        Creates a new object based on an old object\\n        IntCell copy = original;                         //or Intcell copy(original)\\n        automatically called when object is passed by value into a subroutine\\n        automatically called when object is returned by value from a subroutine\\n    3. operator=()\\n        also known as the copy assignment operator\\n        intended to copy the state of original into copy\\n        called when = is applied to two objects after both have been previously constructed\\n            IntCell original;   //constructor called\\n            IntCell copy;\\n            copy = original;    //operator called\\n        overrides the = operator    //operator overrides only work on objects, not pointers\\n    (and a default constructor, if you don\\'t supply one) //this will do nothing        \\n            \\nC++ has visibility on the inheritance\\n\\nclass Name {\\npublic:\\n    Name(void) : myName(\"\") { }\\n    ~Name(void) {  }\\n    void SetName(string theName) {\\n        myName = theName;\\n    }\\n    void print(void) {\\n        cout << myName << endl;\\n    }\\n\\nprivate:\\n    string myName;\\n};\\n    \\nclass Contact: public Name { //this is like contact extends name\\npublic:\\n    Contact(void) {\\n        myAddress = \"\";\\n    }\\n    ~Contact(void) { }\\n    void SetAddress(string theAddress) {\\n        myAddress = theAddress;\\n    }\\n    void print(void) {\\n        Name::print();  //this can\\'t access private fields in Name, needs to call print from super class\\n        cout << myAddress << endl;\\n    }\\nprivate:\\n    string myAddress;\\n};\\n\\nC++ has multiple inheritance - you can have as many parent classes as you want\\n    class Sphere : public Shape, public Comparable, public Serializable {\\n    };\\n\\nDispatch\\n    Static - Decision on which member function to invoke made using compile-time type of an object\\n        when you have a pointer\\n        Person *p;\\n        p = new Student();\\n        p.print();  //will alway call the Person print method - uses type of the pointer\\n    Dynamic - Decision on which member function to invoke made using run-time type of an object\\n        Incurs runtime overhead\\n            Program must maintain extra information\\n            Compiler must generate code to determine which member function to invoke\\n        Syntax in C++: virtual keyword \\n            (Java does this by default, i.e. everything is virtual)\\n        Example\\n            class A \\n                virtual void foo()            \\n            class B : public A\\n                virtual void foo()\\n            void main () \\n                int which = rand() % 2;\\n                A *bar;\\n                if ( which )\\n                    bar = new A();\\n                else\\n                    bar = new B();\\n                bar->foo();\\n                return 0;\\n            \\n        Virtual method tables - stores the virtual methods in an array\\n            Each object contains a pointer to the virtual method table\\n                In addition to any other fields\\n            That table has the addresses of the methods\\n                Any virtual method must follow the pointer to the object... (one pointer dereference)\\n                Then follow the virtual method table pointer... (second pointer dereference)\\n                Then lookup the method pointer\\n                    In Java default is Dynamic\\n                    In C++, default is Static - this is faster\\n            When creating a subclass object, the constructor of each subclass overwrites the appropriate pointers in the virtual method table with the overridden method pointers\\n\\nAbstract Classes\\n    class foo {\\n        public:\\n          virtual void bar() = 0;\\n    };\\n\\nTypes of multiple inheritance\\n    1. Shared\\n        What Person is in the diagram on the previous slide\\n    2. Replicated (or repeated)\\n        What gp_list_node is in the diagram on the previous slide\\n    3. Non-replicated (or non-repeated)\\n        A language that does not allow shared or replicated (i.e. no common ancestors allowed)\\n    4. Mix-in\\n        What Java (and others) use to fake multiple inheritance through the use of interfaces\\n    \\n    In C++, replicated is the default\\n        Shared can be done by specifying that a base class is virtual:\\n            class student: public virtual person, public gp_list_node {\\n            class professor: public virtual person, public gp_list_node {\\n                \\n    Java has ArrayStoreException - makes sure the thing you are adding to the array is of the correct type\\n        String[] a = new String[1];\\n        Object[] b = a;\\n        b[0] = new Integer (1);\\n\\n\\n\\u200b    ',\n",
       " \"---\\nlayout: notes\\ntitle: Differential Equations\\ncategory: math\\n---\\n\\n#  differential equations\\n\\n## Differential Equations\\nSeparable: Separate and Integrate\\nFOLDE: y' + p(x)y = g(x)\\nIF: $e^{\\\\int{p(x)}dx}$\\n\\nExact: Mdx+Ndy = 0 $M_y=N_x$ \\nIntegrate Mdx or Ndy, make sure all terms are present\\n\\nConstant Coefficients: \\nPlug in $e^{rt}$, solve characteristic polynomial\\nrepeated root solutions: $e^{rt},re^{rt}$\\ncomplex root solutions: $r=a\\\\pm bi, y=c_1e^{at} cos(bt)+c_2e^{at} sin(bt)$\\n\\nSOLDE (non-constant): \\npy''+qy'+ry=0\\n\\nReduction of Order: Know one solution, can find other\\n\\nUndetermined Coefficients (doesn't have to be homogenous): solve homogenous first, then plug in form of solution with variable coefficients, solve polynomial to get the coefficients\\n\\nVariation of Parameters: start with homogenous solutions $y_1,y_2$\\n$Y_p=-y_1\\\\int \\\\frac{y_2g}{W(y_1,y_2)}dt+y_2\\\\int \\\\frac{y_1g}{W(y_1,y_2)}dt$\\n\\nLaplace Transforms - for anything, best when g is noncontinuous\\n\\n$\\\\mathcal{L}(f(t))=F(t)=\\\\int_0^\\\\infty e^{-st}f(t)dt$\\n\\nSeries Solutions: More difficult\\n\\nWronskian: $W(y_1 ,y_2)=y_1y _2' -y_2 y_1'$\\nW = 0 $\\\\implies$ solns linearly dependent\\n\\nAbel's Thm: y''+py'+q=0 $\\\\implies W=ce^{\\\\int pdt}$\",\n",
       " \"---\\nlayout: notes\\ntitle: Proofs\\ncategory: math\\n---\\n\\n#  proofs\\n\\n## proofs\\n- induction\\n\\t- must already know formula\\n\\t- doesn't give intuition\\n\\t- there are uncomputable functions e.g. Halting Problem, 3x+1 problem\\n- non-existence proofs\\n\\t- must cover all possible scenarios, harder than existence\\n\\n\\n\\n## interesting\\n\\n- http://acko.net/blog/how-to-fold-a-julia-fractal/ \\n- Biggest primes, the twins\\n- Infinite twin primes – guy working at subway\\n- Different infinites – people driven crazy\\n- Four-Color problem – Computer aided proof in 1977, basically checks cases\\n- Bruower’s Fixed Point Theorem\\n- Fermat’s Last Theorem – Pythagorean thm untrue for anything bigger than squared\\n\\t- Pf in 1995\\n- Godel’s Thm: there are unprovable statements\\n- Pigeonhole Principle\\n\\t- Claim: Consider a 10 foot by 10 foot square room. Let every point on the floor of the room be colored either red or blue. Then there exists somewhere on the floor two points of the same color that are exactly one foot apart.\\n\\t- Proof: Consider an equilateral triangle on the floor of the room with side lengths equal to one foot. As every point is either red or blue, two vertices of the triangle must have the same color, satisfying the claim.\\n\\t- There must be 2 people in London with the same amount of hairs on their head\",\n",
       " \"---\\nlayout: notes\\ntitle: real analysis\\ncategory: math\\n---\\n\\n#  real analysis\\n\\nSome notes on real analysis, following the textbook [Understanding analysis](https://link.springer.com/book/10.1007/978-1-4939-2712-8)\\n\\n## ch 1 - the real numbers\\n- there is no rational number whose square is 2 (proof by contradiction)\\n- *contrapositive*: $$-q \\\\to -p$$ - logically equivalent\\n- *triangle inequality*: $\\\\|a+b\\\\| \\\\leq \\\\|a\\\\| + \\\\|b\\\\|$ (often use \\\\|a-b\\\\| = \\\\|(a-c)+(c-b)\\\\|)\\n- *axiom of completeness* - every nonempty set $A \\\\subseteq \\\\mathbb{R}$ that is bounded above has a least upper bound\\n\\t- doesn't work for $\\\\mathbb{Q}$\\n- *supremum* = supA = least upper bound (similarly, *infimum*)\\n\\t1. supA is an upper bound of A\\n\\t2. if $s \\\\in \\\\mathbb{R}$ is another u.b. then $s \\\\geq supA$\\n\\t\\t- can be restated as $\\\\forall \\\\epsilon > 0, \\\\exists a \\\\in A$ $s-\\\\epsilon < a$\\n- *nested interval property* - for each $n \\\\in N$, assume we are given a closed interval $I_n = [a_n,b_n]=\\\\{ x \\\\in \\\\mathbb{R} : a_n \\\\leq x \\\\leq b_n \\\\}$  Assume also that each $I_n$ contains $I_{n+1}$.  Then, the resulting nested sequence of nonempty closed intervals $I_1 \\\\supseteq I_2 \\\\supseteq ...$ has a nonempty intersection\\nuse AoC with x = sup{$a_n: n \\\\in \\\\mathbb{N}$} in the intersection of all sets\\n- *archimedean property*\\n\\t1. $\\\\mathbb{N}$ is unbounded above (sup $\\\\mathbb{N}=\\\\infty$)\\n\\t2. $\\\\forall x \\\\in \\\\mathbb{R}, x>0, \\\\exists n \\\\in \\\\mathbb{N}, 0 < \\\\frac{1}{n} < x$\\n- $\\\\mathbb{Q}$ is dense in $\\\\mathbb{R}$ - for every $a,b \\\\in \\\\mathbb{R}, a<b$, $\\\\exists r \\\\in \\\\mathbb{Q}$ s.t. $a<r<b$\\n\\t- pf: want $a < \\\\frac{m}{n} < b$\\n\\t\\t- by Archimedean property, want $\\\\frac{1}{n} < b-a$\\n\\t- corollary: the irrationals are dense in $\\\\mathbb{R}$\\n- there exists a real number $r \\\\in \\\\mathbb{R}$ satisfying $r^2 = 2$\\n\\t- pf: let r = $sup \\\\{ t \\\\in \\\\mathbb{R} : t^2 < 2 \\\\}$.  disprove $r^2<2, r^2>2$ by considering $r+\\\\frac{1}{n},r-\\\\frac{1}{n}$\\n- A ~ B if there exists f:A->B that is 1-1 and onto\\n- A is *finite* - there exists n $\\\\in \\\\mathbb{N}$ s.t. $\\\\mathbb{N}_n$~A\\n- *countable* =  $\\\\mathbb{N}$~A.  \\n\\t- uncountable - inifinite set that isn't countable\\n\\t- Q is countable\\n\\t\\t- pf: Let $A_n = \\\\{ \\\\pm \\\\frac{p}{q}:$ where p,q $\\\\in \\\\mathbb{N}$ are in lowest terms with p+q=n}\\n\\t- R is uncountable\\n\\t\\t- pf: Assume we can enumerate $\\\\mathbb{R}$  Use NIP to exclude one point from $\\\\mathbb{R}$ each time.  The intersection is still nonempty, so we didn't succesfully enumerate $\\\\mathbb{R}$\\n\\t- $\\\\frac{x}{x^2-1}$ maps (0,1) $\\\\to \\\\mathbb{R}$\\n\\t- countable union of countable sets is countable\\n- if $A \\\\subseteq B$ and B countable, then A is either countable or finite\\n- if $A_n$ is a countable set for each $n \\\\in \\\\mathbb{N}$, then their union is countable\\n- the open interval (0,1) = $\\\\{ x \\\\in \\\\mathbb{R} : 0 < x < 1 \\\\}$ is uncountable\\n\\t- pf: diagonalization - assume there exists a function from (0,1) to $\\\\mathbb{R}$.  List the decimal expansions of these as rows of a matrix.  Complement of diagonal does not exist.\\n- *cantor's thm* - Given any set A, there does not exist a function f:$A \\\\to P(A)$ that is onto\\n\\t- P(A) is the set of all subsets of A\\n\\n## ch 2 - sequences and series\\n- a sequence $(a_n)$ *converges* to a real number if $\\\\forall \\\\epsilon > 0, \\\\exists N \\\\in \\\\mathbb{N}$ such that $\\\\forall n\\\\geq N, \\\\|a_n-a\\\\| < \\\\epsilon$\\n\\t\\n\\t- otherwise it *diverges*\\n- if a limit exists, it is unique\\n- a sequence $(x_n)$ is *bounded* if there exists a number M > 0 such that $\\\\|x_n\\\\|\\\\leq M \\\\forall n \\\\in \\\\mathbb{N}$\\n\\t\\n\\t- every convergent sequence is bounded\\n- *algebraic limit thm* - let lim $a_n = a$ and lim $b_n$ = b.  Then\\n\\t1. lim($ca_n$) = ca\\n\\t2. lim($a_n+b_n$) = a+b\\n\\t3. lim($a_n b_n$) = ab\\n\\t4. lim($a_n/b_n$) = a/b, provided b $\\\\neq$ 0\\n\\t- pf 3: use triangle inequality, $\\\\|a_nb_n-ab\\\\|=\\\\|a_nb_n-ab_n+ab_n-ab\\\\|=...=\\\\|b_n\\\\|\\\\|a_n-a\\\\|+\\\\|a\\\\|\\\\|b_n-b\\\\|$\\n\\t- pf 4: show $(b_n) \\\\to b$ implies $(\\\\frac{1}{b_n}) \\\\to \\\\frac{1}{b}$\\n- *order limit thm* - Assume lim $a_n = a$ and lim $b_n$ = b.\\n\\t1. If $a_n \\\\geq 0$ $\\\\forall n \\\\in \\\\mathbb{N}$, then $a \\\\geq 0$\\n\\t2. If $a_n \\\\leq b_n$ $\\\\forall n \\\\in \\\\mathbb{N}$, then $a \\\\leq b$\\n\\t3. If $\\\\exists c \\\\in \\\\mathbb{R}$ for which $c \\\\leq b_n$ $\\\\forall n \\\\in \\\\mathbb{N}$, then $c \\\\leq b$\\n\\t- pf 1: by contradiction\\n- *monotone* - increasing or decreasing (not strictly)\\n- *monotone convergence thm* - if a sequence is monotone and bounded, then it converges\\n- *convergence of a series*\\n\\t- define $s_m=a_1+a_2+...+a_m$\\n\\t- $\\\\sum_{n=1}^\\\\infty a_n$ converges to A $\\\\iff (s_m)$ converges to A\\n- *cauchy condensation test* - suppose $a_n$ is decreasing and satisfies $a_n \\\\geq 0$ for all $n \\\\in \\\\mathbb{N}$.  Then, the series $\\\\sum_{n=1}^\\\\infty a_n$ converges iff the series $\\\\sum_{n=1}^\\\\infty 2^na_{2^n}$ converges\\n\\t- *p-series* $\\\\sum_{n=1}^\\\\infty 1/n^p$ converges iff p > 1\\n\\t\\n### 2.5\\n- let $(a_n)$ be a sequence and $n_1<n_2<...$ be an increasing sequence of natural numbers.  Then $(a_{n_1},a_{n_2},...)$ is a *subsequence* of $(a_n)$\\n- subsequences of a convergent sequence converge to the same limit as the original sequence\\n\\t- can be used as divergence criterion\\n- *bolzano-weierstrass thm* - every bounded sequence contains a convergent subsequence\\n\\t- pf: use NIP, keep splitting interval into two\\n\\n### 2.6\\n- $(a_n)$ is a *cauchy sequence* if $\\\\forall \\\\epsilon > 0, \\\\exists N \\\\in \\\\mathbb{N}$ such that $\\\\forall m,n\\\\geq N, \\\\|a_n-a_m\\\\| < \\\\epsilon$\\n- *cauchy criterion* - a sequence converges $\\\\iff$ it is a cauchy sequence\\n\\t- cauchy sequences are bounded\\n- overview: AoC $\\\\iff$ NIP $\\\\iff$ MCT $\\\\iff$ BW $\\\\iff$ CC\\n\\n### 2.7\\n- *algebraic limit thm* - let $\\\\sum_{n=1}^\\\\infty a_n$ = A, $\\\\sum_{n=1}^\\\\infty b_n$ = B\\n\\t1. $\\\\sum_{n=1}^\\\\infty ca_n$ = cA\\n\\t2. $\\\\sum_{n=1}^\\\\infty a_n+b_n$ = A+B\\n1. *cauchy criterion for series* - series converges $\\\\iff$ $(s_m)$ is a cauchy sequence\\n- if the series $\\\\sum_{n=1}^\\\\infty a_n$ converges then lim $a_n=0$\\n2. *comparison test*\\n3. *geometric series* - \\t$\\\\sum_{n=0}^\\\\infty a r^n = \\\\frac{a}{1-r}$\\n\\t\\n\\t- $s_m = a+ar+...+ar^{m-1} = \\\\frac{a(1-r^m)}{1-r}$\\n4. *absolute convergence test*\\n5. *alternating series test*\\n\\t1. decreasing\\n\\t2. lim $a_n$ = 0\\n\\t- then, $\\\\sum_{n=1}^\\\\infty (-1)^{n+1} a_n$ converges\\n- rearrangements: there exists one-to-one correspondence\\n- if a series converges absolutely, any rearrangement converges to same limit\\t\\n\\n## ch 3 - basic topology of R\\n\\n### 3.1 cantor set\\n- C has small length, but its cardinality is uncountable\\n- discussion of dimensions, doubling sizes leads to 2^dimension sizes\\n\\t- Cantor set is about dimension .631\\n\\t\\n### 3.2 open/closed sets\\n- A set O $\\\\subseteq \\\\mathbb{R}$ is *open* if for all points a $\\\\in$ O there exists an $\\\\epsilon$-neighborhood $V_{\\\\epsilon}(a) \\\\subseteq O$\\n\\t-  $V_{\\\\epsilon}(a)=\\\\{ x \\\\in R : \\\\|x-a\\\\| < \\\\epsilon$}\\n1. the union of an arbitrary collection of open sets is open\\n2. the intersection of a finite collection of open sets is open\\n- a point x is a *limit point* of a set A if every $\\\\epsilon$-neighborhood $V_{\\\\epsilon}(x)$ of x intersects the set A at some point other than x\\n- a point x is a limit point of a set A if and only if x = lim $a_n$ for some sequence ($a_n$) contained in A satisfying $a_n \\\\neq x$ for all n $\\\\in$ N\\n- *isolated point* - not a limit point\\n- set $F \\\\subseteq \\\\mathbb{R}$ *closed* - contains all limit points\\n\\t- *closed* iff every Cauchy sequence contained in F has a limit that is also an element of F\\n- density of Q in R - for every $y \\\\in \\\\mathbb{R}$, there exists a sequence of rational numbers that converges to y\\n- *closure* - set with its limit points\\n\\t- closure $\\\\bar{A}$ is smallest closed set containing A\\n- iff set open, complement is closed\\n\\t- R and $\\\\emptyset$ are both open and closed\\n1. the union of a finite collection of closed sets is closed\\n2. the intersection of an arbitrary collection of closed sets is closed\\n\\n### 3.3 \\n- a set K $\\\\subseteq \\\\mathbb{R}$ is *compact* if every sequence in K has a subsequence that converges to a limit that is also in K\\n- *Nested Compact Set Property* - intersection of nested sequence of nonempty compact sets is not empty\\n- let A $\\\\subseteq \\\\mathbb{R}$.  *open cover* for A is a (possibly infinite) collection of  open sets whose union contains the set A.  \\n- given an open cover for A, a *finite subcover* is a finite sub-collection of open sets from the original open cover whose union still manages to completely contain A\\n- *Heine-Borel thm* - let K $\\\\subseteq \\\\mathbb{R}$.  All of the following are equivalent\\n\\t1. K is compact\\n\\t2. K is closed and bounded\\n\\t3. every open cover for K has a finite subcover\\n\\t\\n## ch 4\\t- functional limits and continuity\\n\\n### 4.1 \\n- dirichlet function: 1 if r $\\\\in \\\\mathbb{Q}$ 0 otherwise\\t\\n\\t\\n### 4.2 functional limits\\n- def 1. Let f:$A \\\\to R$, and let c be a limit point of the domain A.  We say that *$lim_{x \\\\to c} f(x) = L$* provided that for all $\\\\epsilon$ > 0, there exists a $\\\\delta$ > 0 s.t. whenever 0 < \\\\|x-c\\\\| < $\\\\delta$ (and x $\\\\in$ A) it follows that \\\\|f(x)-L\\\\|< $\\\\epsilon$\\n- def 2. Let f:$A \\\\to R$, and let c be a limit point of the domain A. We say that $lim_{x \\\\to c} f(x) = L$ provided that for every $\\\\epsilon$-neighborhood $V_{\\\\epsilon}(L)$ of L, there exists a $\\\\delta$-neighborhood $V_{\\\\delta}($c) around c with the property that for all x $\\\\in V_{\\\\delta}($c) different from c (with x $\\\\in$ A) it follows that f(x) $\\\\in V_{\\\\epsilon}(L)$.\\n- *sequential criterion for functional limits* - Given function f:$A \\\\to R$ and a limit point c of A, the following 2 statements are equivalent:\\n\\t1. $lim_{x \\\\to c} f(x) = L$\\n\\t2. for all sequences $(x_n) \\\\subseteq$ A satisfying $x_n \\\\neq$ c and $(x_n) \\\\to c$, it follows that $f(x_n) \\\\to L$.\\n- *algebraic limit thm for functional limits*\\n- *divergence criterion for functional limits*\\n\\n### 4.3 continuous functions\\n- a function f:$A \\\\to R$ is *continuous at a point* c $\\\\in$ A if, for all $\\\\epsilon$>0, there exists a $\\\\delta$>0 such that whenever \\\\|x-c\\\\|<$\\\\delta$ (and x$\\\\in$ A) it follows that $\\\\|f(x)-f( c)\\\\|<\\\\epsilon$.  F is *continous* if it is continuous at every point in the domain A\\n- characterizations of continuouty\\n- criterion for discontinuity\\n- algebraic continuity theorem\\n- if f is continuous at c and g is continous at f( c) then g $\\\\circ$ f is continuous at c\\n\\n### 4.4 continuous functions on compact sets\\n- *preservation of compact sets* - if f continuous and K compact, then f(K) is compact as well\\n- *extreme value theorem* - if f if continuous on a compact set K, then f attains a maximum and minimum value.  In other words, there exist $x_0,x_1 \\\\in K$ such that $f(x_0) \\\\leq f(x) \\\\leq f(x_1)$ for all x $\\\\in$ K\\n- f is *uniformly continuous on A* if for every $\\\\epsilon$>0, there exists a $\\\\delta$>0 such that for all x,y $\\\\in$ A, $\\\\|x-y\\\\| < \\\\delta \\\\implies \\\\|f(x)-f(y)\\\\| < \\\\epsilon$\\n\\t- a function f fails to be uniformly continuous on A iff there exists a particular $\\\\epsilon_o$ > 0 and two sequences $(x_n),(y_n)$ in A sastisfying $\\\\|x_n - y_n\\\\| \\\\to 0$ but $\\\\|f(x_n)-f(y_n)\\\\| \\\\geq \\\\epsilon_o$\\n- a function that is continuous on a compact set K is *uniformly continuous on K*\\n\\n### 4.5 intermediate value theorem\\n- *intermediate value theorem* - Let f:[a,b]$ \\\\to R$ be continuous.  If L is a real number satisfying f(a) < L < f(b) or f(a) > L > f(b), then there exists a point c $\\\\in (a,b)$ where f( c) = L\\n- a function f has the *intermediate value property* on an inverval [a,b] if for all x < y in [a,n] and all L between f(x) and f(y), it is always possible to find a point c $\\\\in (x,y)$ where f( c)=L.\\n\\n## ch 5 - the derivative\\n\\n### 5.2 derivatives and the intermediate value property\\n- let g: A -> R be a function defined on an interval A.  Given c $\\\\in$ A, the *derivative of g at c* is defined by g'( c) = $\\\\lim_{x \\\\to c} \\\\frac{g(x) - g( c)}{x-c}$, provided this limit exists.  Then g is differentiable at c.  If g' exists for all points in A, we say g is *differentiable* on A\\n- identity: $x^n-c^n = (x-c)(x^{n-1}+cx^{n-2}+c^2x^{n-3}+...+c^{n-1}$)\\n- differentiable $\\\\implies$ continuous\\n- *algebraic differentiability theorem*\\n\\t1. adding\\n\\t2. scalar multiplying\\n\\t3. product rule\\n\\t4. quotient rule\\n- *chain rule*: let f:A-> R and g:B->R satisfy f(A)$\\\\subseteq$ B so that the composition g $\\\\circ$ f is defined.  If f is differentiable at c in A and g differentiable at f( c) in B, then g $\\\\circ$ f is differnetiable at c with (g$\\\\circ$f)'( c)=g'(f( c))*f'( c)\\n- *interior extremum thm* - let f be differentiable on an open interval (a,b).  If f attains a maximum or minimum value at some point c $\\\\in$ (a,b), then f'( c) = 0.  \\n- *Darboux's thm* - if f is differentiable on an interval [a,b], and a satisfies f'(a) < $\\\\alpha$ < f'(b)  (or f'(a) > $\\\\alpha$ > f'(b)), then there exists a point c $\\\\in (a,b)$ where f'( c) = $\\\\alpha$\\n\\t- derivative satisfies intermediate value property\\n\\n### 5.3 mean value theorems\\n- *mean value theorem* - if f:[a,b] -> R is continuous on [a,b] and differentiable on (a,b), then there exists a point c $\\\\in$ (a,b) where $f'( c) = \\\\frac{f(b)-f(a)}{b-a}$\\n\\t- *Rolle's thm* - f(a)=f(b) -> f'( c)=0\\n\\t- if f'(x) = 0 for all x in A, then f(x) = k for some constant k\\n- if f and g are differentiable functions on an interval A and satisfy f'(x) = g'(x) for all x $\\\\in$ A, then f(x) = g(x) + k for some constant k\\n- *generalized mean value theorem* - if f and g are continuous on the closed interval [a,b] and differentiable on the open interval (a,b), then there exists a point c $\\\\in (a,b)$ where \\\\|f(b)-f(a)\\\\|g'( c) = \\\\|g(b)-g(a)\\\\|f'( c).  If g' is never 0 on (a,b), then can be restated $\\\\frac{f'( c)}{g'( c)} = \\\\frac{f(b)-f(a)}{g(b)-g(a)}$\\n- given g: A -> R and a limit point c of A, we say that *$lim_{x \\\\to c} g(x) = \\\\infty$* if, for every M > 0, there exists a $\\\\delta$> 0 such that whenever 0 < \\\\|x-c\\\\| < $\\\\delta$ it follows that g(x) ≥ M\\n- *L'Hospital's Rule: 0/0* - let f and g be continuous on an interval containing a, and assume f and g are differentiable on this interval with the possible exception of the point a.  If f(a) = g(a) = 0 and g'(x) ≠ 0 for all x ≠ a, then $lim_{x \\\\to a} \\\\frac{f'(x)}{g'(x)} = L \\\\implies lim_{x \\\\to a} \\\\frac{f'(x)}{g'(x)} = L$\\n- *L'Hospital's Rule: $\\\\infty / \\\\infty$* - assume f and g are differentiable on (a,b)  and g'(x) ≠ 0 for all x in (a,b).  If $lim_{x \\\\to a} g(x) = \\\\infty $, then $lim_{x \\\\to a} \\\\frac{f'(x)}{g'(x)} = L \\\\implies lim_{x \\\\to a} \\\\frac{f'(x)}{g'(x)} = L$\\n\\n## ch 6 - sequences and series of function\\n\\n### 6.2 uniform convergence of a sequence of functions\\n- for each n $\\\\in \\\\mathbb{N}$ let $f_n$ be a function defined on a set A$\\\\subseteq R$.  The sequence ($f_n$) of functions *converges pointwise* on A to a function f if, for all x in A, the sequence of real numbers $f_n(x)$ converges to f(x)\\n- let ($f_n$) be a sequence of functions defined on a set A$\\\\subseteq$R.  Then ($f_n$) *converges unformly* on A to a limit function f defined on A if, for every $\\\\epsilon$>0, there exists an N in $\\\\mathbb{N}$ such that $\\\\forall n ≥N,  x \\\\in A , \\\\|f_n(x)-f(x)\\\\|<\\\\epsilon$\\n\\t- *Cauchy Criterion* for uniform convergence - a sequence of functions $(f_n)$ defined on a set A $\\\\subseteq$ R  converges uniformly on A iff $\\\\forall \\\\epsilon > 0 \\\\exists N \\\\in \\\\mathbb{N}$ s.t. whenever m,n ≥N and x in A, $\\\\|f_n(x)-f_m(x)\\\\|<\\\\epsilon$\\n- *continuous limit thm* - Let ($f_n$) be a sequence of functions defined on A that converges uniformly on A to a function f.  If each $f_n$ is continuous at c in A, then f is continuous at c\\n\\n### 6.3 uniform convergence and differentiation\\n- *differentiable limit theorem* - let $f_n \\\\to f$ pointwise on the closed interval [a,b], and assume that each $f_n$ is differentiable.  If $(f'_n)$ converges uniformly on [a,b] to a function g, then the function f is differentiable and f'=g\\n- let ($f_n$) be a sequence of differentiable functions defined on the closed interval [a,b], and assume $(f'_n)$ converges uniformly to a function g on [a,b].  If there exists a point $x_0 \\\\in [a,b]$ for which $f_n(x_0)$ is convergent, then ($f_n$) converges uniformly.  Moreover, the limit function f = lim $f_n$ is differentiable and satisfies f' = g\\n\\n### 6.4 series of functions\\n- *term-by-term continuity thm* - let $f_n$ be continuous functions defined on a set A $\\\\subseteq$ R and assume $\\\\sum f_n$ converges uniformly on A to a function f.  Then, f is continuous on A.\\n- *term-by-term differentiability thm* - let $f_n$ be differentiable functions defined on an interval A, and assume $\\\\sum f'_n(x)$ converges uniformly to a limit g(x) on A.  If there exists a point $x_0 \\\\in [a,b]$ where $\\\\sum f_n(x_0)$ converges, then the series $\\\\sum f_n(x)$ converges uniformly to a differentiable function f(x) satisfying f'(x) = g(x) on A.  In other words, $f(x) = \\\\sum f_n(x)$ and $f'(x) = \\\\sum f'_n(x)$\\n- *Cauchy Criterion for uniform convergence of series* - A series $\\\\sum f_n$ converges uniformly on A iff $\\\\forall \\\\epsilon > 0 \\\\exists N \\\\in N$ s.t. whenever n>m≥N, x in A $\\\\|f_{m+1}(x) + f_{m+2}(x) + f_{m+3}(x) + ...+f_n(x)\\\\| < \\\\epsilon$\\n\\t- *Wierstrass M-Test* - For each n in N, let $f_n$ be a function defined on a set A $\\\\subseteq$ R, and let $M_n > 0$ be a real number satisfying $\\\\|f_n(x)\\\\| ≤ M_n$ for all x in A.  If $\\\\sum M_n$ converges, then $\\\\sum f_n$ converges uniformly on A\\n\\n### 6.5 power series\\n- power series f(x) = $\\\\sum_{n=0}^\\\\infty a_n x^n = a_0 + a_1 x_1 + a_2 x^2 + a_3 x^3 + ...$\\n- if a power series converges at some point $x_0 \\\\in \\\\mathbb{R}$, then it converges absolutely for any x satisfying \\\\|x\\\\|<\\\\|$x_0$\\\\|\\n- if a power series converges pointwise on the set A, then it converges uniformly on any compact set K $\\\\subseteq$ A\\n\\t- if a power series converges absolutely at a point $x_0$, then it converges uniformly on the closed interval [-c,c], where c = \\\\|$x_0$\\\\|\\n\\t- *Abel's thm* - if a power series converges at the point x = R > 0, the the series converges uniformly on the interval [0,R].  A similar result holds if the series converges at x = -R\\n- if $\\\\sum_{n=0}^\\\\infty a_n x^n$ converges for all x in (-R,R), then the differentiated series $\\\\sum_{n=0}^\\\\infty n a_n x^{n-1}$ converges at each x in (-R,R) as well.  Consequently the convergence is uniform on compact sets contained in (-R,R).\\n\\t- can take infinite derivatives\\n\\n### 6.6 taylor series\\n- *Taylor's Formula* $\\\\sum_{n=0}^\\\\infty a_n x^n = a_0 + a_1 x_1 + a_2 x^2 + a_3 x^3 + ...$\\n\\t- centered at 0: $a_n = \\\\frac{f^{(n)}(0)}{n!}$\\n- *Lagrange's Remainder thm* - Let f be differentiable N+1 times on (-R,R), define $a_n = \\\\frac{f^{(n)}(0)}{n!}.....$\\n- not every infinitely differentiable function can be represented by its Taylor series (radius of convergence zero)\\n\\n## ch 7 - the Riemann Integral\\n\\n### 7.2 def of Riemann integral\\n- *partition* of [a,b] is a finite set of points from [a,b] that includes both a and b\\n- *lower sum* - sum all the possible smallest rectangles\\n- a partition Q is a *refinement* of a partition P if $P \\\\subseteq Q$ \\n- if $P \\\\subseteq Q$, then L(f,P)≤L(f,Q) and U(f,P)≥U(f,Q)\\n- a bounded function f on the interval [a,b] is *Riemann-integrable* if U(f) = L(f) = $\\\\int_a^b f$\\n\\t- iff $\\\\forall \\\\epsilon >0$, there exists a partition P of [a,b] such that $U(f,P)-L(f,P)<\\\\epsilon$\\n\\t- U(f) = inf{U(f,P)} for all possible partitions P\\n- if f is continuous on [a,b] then it is integrable\\n\\n### 7.3 integrating functions with discontinuities\\n- if f:[a,b]->R is bounded and f is integrable on [c,b] for all c in (a,b), then f is integrable on [a,b]\\n\\n### 7.4 properties of Integral\\n- assume f: [a,b]->R is bounded and let c in (a,b).  Then, f is integrable on [a,b] iff f is integrable on [a,c] and [c,b].  In this case we have $\\\\int_a^b f = \\\\int_a^c f + \\\\int_c^b f.$F\\n- *integrable limit thm* - Assume that $f_n \\\\to f$ uniformly on [a,b] and that each $f_n$ is integarble.  Then, f is integrable and $lim_{n \\\\to \\\\infty} \\\\int_a^b f_n = \\\\int_a^b f$.\\n\\n### 7.5 fundamental theorem of calculus\\n1. If f:[a,b] -> R is integrable, and F:[a,b]->R satisfies F'(x) = f(x) for all x $\\\\in$ [a,b], then $\\\\int_a^b f = F(b) - F(a)$\\n2. Let f: [a,b]-> R be integrable and for x $\\\\in$ [a,b] define G(x) = $\\\\int_a^x g$.  Then G is continuous on [a,b]. If g is continuous at some point $c \\\\in [a,b]$ then G is differentiable at c and G'(c) = g(c).\\n\\n## overview\\n- convergence\\n\\t1. sequences\\n\\t2. series\\n\\t3. functional limits\\n\\t\\t- normal, uniform\\n\\t4. sequence of funcs\\n\\t\\t- pointwise, uniform\\n\\t5. series of funcs\\n\\t\\t- pointwise, uniform\\n\\t6. integrability\\n- sequential criterion - usually good for proving discontinuous\\n\\t1. limit points\\n\\t2. functional limits\\n\\t3. continuity\\n\\t4. absence of uniform continuity\\n- algebraic limit theorem ~ scalar multiplication, addition, multiplication, division\\n\\t1. limit thm\\n\\t1. sequences\\n\\t2. series - can't multiply / divide these\\n\\t3. functional limits\\n\\t4. continuity\\n\\t6. differentiability\\n\\t7. ~integrability~\\n- limit thms\\n\\t- *continuous limit thm* - Let ($f_n$) be a sequence of functions defined on A that converges uniformly on A to a function f.  If each $f_n$ is continuous at c in A, then f is continuous at c\\n\\t- *differentiable limit theorem* - let $f_n \\\\to f$ pointwise on the closed interval [a,b], and assume that each $f_n$ is differentiable.  If $(f'_n)$ converges uniformly on [a,b] to a function g, then the function f is differentiable and f'=g\\n\\t\\t- convergent derivatives almost proves that $f_n \\\\to f$\\n\\t\\t- let ($f_n$) be a sequence of differentiable functions defined on the closed interval [a,b], and assume $(f'_n)$ converges uniformly to a function g on [a,b].  If there exists a point $x_0 \\\\in [a,b]$ for which $f_n(x_0) \\\\to f(x_0)$ is convergent, then ($f_n$) converges uniformly\\n\\t- *integrable limit thm* - Assume that $f_n \\\\to f$ uniformly on [a,b] and that each $f_n$ is integarble.  Then, f is integrable and $lim_{n \\\\to \\\\infty} \\\\int_a^b f_n = \\\\int_a^b f$.\\n- functions are continuous at isolated points, but limits don't exist there\\n- uniform continuity: minimize $\\\\|f(x)-f(y)\\\\|$\\n- derivative doesn't have to be continuous\\n- integrable if finite amount of discontinuities and bounded\\n\",\n",
       " \"---\\nlayout: notes\\ntitle: linear algebra\\ncategory: math\\ntypora-copy-images-to: ../assets\\n---\\n\\n#  linear algebra\\n\\n## linear basics\\n\\n### notation\\n\\n- $x \\\\preceq y$ - these are vectors and x is less than y elementwise\\n- $X \\\\preceq Y$ - matrices, $Y-X$ is PSD\\n  - $v^TXv \\\\leq v^TYv \\\\:\\\\: \\\\forall v$\\n\\n### linearity\\n- inner product $<X, Y> = tr(X^TY) = \\\\sum_i \\\\sum_j X_{ij} Y_{ij}$\\n  - like inner product if we collapsed into big vector\\n  - linear\\n  - symmetric\\n  - gives angle back\\n- linear \\n    1. superposition $f(x+y) =  f(x)+f(y) $\\n    2. proportionality $f(k\\\\cdot x) = k \\\\cdot f(x)$\\n- bilinear just means a function is linear in 2 variables\\n- vector space\\n    1. closed under addition\\n    2. contains identity\\n- det - sum of products including one element from each row / column with correct sign\\n    - absolute value = area of parallelogram made by rows (or cols)\\n    - ![220px-Area_parallellogram_as_determinant.svg](../assets/220px-Area_parallellogram_as_determinant.svg.png)\\n- lin independent: $c_1x_1+c_2x_2=0 \\\\implies c_1=c_2=0$\\n- *cauchy-schwartz inequality*: $|x^T y| \\\\leq ||x||_2 ||y|||_2$\\n  - implies *triangle inequality*: $||x+y||^2 \\\\leq (||x|| + ||y||)^2$\\n\\n### matrix properties\\n\\n- $x^TAx = tr(xx^TA)$\\n- *nonsingular* = invertible = nonzero determinant = null space of zero\\n    - only square matrices\\n    - *rank* of mxn matrix- max number of linearly independent columns / rows\\n      - rank==m==n, then nonsingular\\n    - *ill-conditioned matrix* - matrix is close to being singular - very small determinant\\n- inverse\\n    - *orthogonal matrix*: all columns are *orthonormal*\\n      - $A^{-1} = A^T$\\n      - preserves the Euclidean norm $||Ax||_2 = ||x||_2$\\n    - if diagonal, inverse is invert all elements\\n    - inverting 3x3 - transpose, find all mini dets, multiply by signs, divide by det\\n    - *psuedo-inverse* = *Moore-Penrose inverse* $A^\\\\dagger = (A^T A)^{-1} A^T$\\n      - if A is nonsingular, $A^\\\\dagger = A^{-1}$\\n      - if rank(A) = m, then must invert using $A A^T$\\n      - if rank(A) = n, then must use $A^T A$\\n    - inversion of matrix is $\\\\approx O(n^3)$\\n    - inverse of psd symmetric matrix is also psd and symmetric\\n    - if A, B invertible $(AB)^{-1} = B^{-1} A^{-1}$\\n- *orthogonal complement* - set of orthogonal vectors\\n  - define R(A) to be *range space* of A (column space) and N(A) to be *null space* of A\\n  - R(A) and N(A) are orthogonal complements\\n  - dim $R(A)$ = r\\n  - dim $N(A)$ = n-r\\n  - dim $R(A^T)$ = r\\n  - dim $N(A^T)$ = m-r    \\n- *adjoint* - compute with mini-dets\\n    - $A^{-1} = adj(A) / \\\\det(A)$\\n- *Schur complement* of $X = \\\\begin{bmatrix}  A & B \\\\\\\\  C & D\\\\end{bmatrix}$\\n   - $M/D = A - BD^{-1}C$\\n   - $M/A = D-CA^{-1}B$\\n   - $X \\\\succeq 0 \\\\iff M/D \\\\succeq 0$\\n\\n## matrix calc\\n\\n- overview: imagine derivative $f(x + \\\\Delta)$\\n- function f: $\\\\text{anything} \\\\to \\\\mathbb{R}^m$ \\n    - *gradient* vector $\\\\nabla_A f(\\\\mathbf{A})$- partial derivatives with respect to each element of A (vector or matrix)\\n    - gradient = $\\\\frac{\\\\partial f}{\\\\partial A}^T$\\n- these next 2 assume numerator layout (numerator-major order, so numerator constant along rows)\\n- function f: $\\\\mathbb{R}^n \\\\to \\\\mathbb{R}^m$ \\n    - **Jacobian matrix**: $$\\\\mathbf J = \\\\begin{bmatrix}    \\\\dfrac{\\\\partial \\\\mathbf{f}}{\\\\partial x_1} & \\\\cdots & \\\\dfrac{\\\\partial \\\\mathbf{f}}{\\\\partial x_n} \\\\end{bmatrix}= \\\\begin{bmatrix}    \\\\dfrac{\\\\partial f_1}{\\\\partial x_1} & \\\\cdots & \\\\dfrac{\\\\partial f_1}{\\\\partial x_n}\\\\\\\\   \\\\vdots & \\\\ddots & \\\\vdots\\\\\\\\    \\\\dfrac{\\\\partial f_m}{\\\\partial x_1} & \\\\cdots & \\\\dfrac{\\\\partial f_m}{\\\\partial x_n} \\\\end{bmatrix}$$ - this is dim(f) x dim(x)\\n- function f: $\\\\mathbb{R}^n \\\\to \\\\mathbb{R}$ \\n    - 2nd derivative is **Hessian matrix**\\n      - $\\\\bold H = \\\\nabla^2 f(x)_{ij} = \\\\frac{\\\\partial^2 f(x)}{\\\\partial x_i \\\\partial x_j} = \\\\begin{bmatrix}  \\\\dfrac{\\\\partial^2 f}{\\\\partial x_1^2} & \\\\dfrac{\\\\partial^2 f}{\\\\partial x_1\\\\,\\\\partial x_2} & \\\\cdots & \\\\dfrac{\\\\partial^2 f}{\\\\partial x_1\\\\,\\\\partial x_n} \\\\\\\\[2.2ex]  \\\\dfrac{\\\\partial^2 f}{\\\\partial x_2\\\\,\\\\partial x_1} & \\\\dfrac{\\\\partial^2 f}{\\\\partial x_2^2} & \\\\cdots & \\\\dfrac{\\\\partial^2 f}{\\\\partial x_2\\\\,\\\\partial x_n} \\\\\\\\[2.2ex]  \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\[2.2ex]  \\\\dfrac{\\\\partial^2 f}{\\\\partial x_n\\\\,\\\\partial x_1} & \\\\dfrac{\\\\partial^2 f}{\\\\partial x_n\\\\,\\\\partial x_2} & \\\\cdots & \\\\dfrac{\\\\partial^2 f}{\\\\partial x_n^2}\\\\end{bmatrix}$\\n- examples\\n    - $\\\\nabla_x a^T x = a$\\n    - $\\\\nabla_x x^TAx = 2Ax$ (if A symmetric, else $(A+A^T)x)$)\\n    - $\\\\nabla_x^2 x^TAx = 2A$ (if A symmetric, else $A+A^T$)\\n    - $\\\\nabla_x \\\\log \\\\: \\\\det X = X^{-1}$\\n- we can calculate derivs of quadratic forms by calculating derivs of traces\\n    - $x^TAx = tr[x^TAx] = tr[xx^TA]$\\n    - $\\\\implies \\\\frac{\\\\partial}{\\\\partial A} x^TAx = \\\\frac{\\\\partial}{\\\\partial A} tr[xx^TA] = [xx^T]^T = xx^T$\\n    - useful result: $\\\\frac{\\\\partial}{\\\\partial A} log|A| = A^{-T}$\\n\\n## norms\\n\\n- def\\n  1. nonnegative\\n  2. definite f(x) = 0 iff x = 0\\n  3. proportionality (also called homogenous)\\n  4. triangle inequality\\n- properties\\n  - convex\\n\\n### vector norms\\n\\n- **$L_p-$norms**: $||x||_p = (\\\\sum_{i=1}^n |x_i|^p)^{1/p}$\\n  - $L_0$ norm - number of nonzero elements (this is not actually a norm!)\\n  - $||x||_1 = \\\\sum |x_i|$\\n  - $||x||_2$ - Euclidean norm\\n  - $||x||_\\\\infty = \\\\max_i |x_i|$ - also called *Cheybyshev norm*\\n- *quadratic norms*\\n  - *P-quadratic norm*: $||x||_P = (x^TPx)^{1/2} = || P^{1/2} x ||_2$ where $P \\\\in S_{++}^n$\\n- *dual norm*\\n  - given a norm $|| \\\\cdot ||$, dual norm $||z||_* = sup\\\\{ z^Tx \\\\: | \\\\: ||x|| \\\\leq 1\\\\}$\\n  - dual of the dual is the original\\n  - dual of Euclidean is just Euclidean\\n  - dual of $l_1$ is $l_\\\\infty$\\n  - dual of spectral norm is some of the singular values\\n\\n### matrix norms\\n\\n- schatten p-norms: $||X||_p = (\\\\sum \\\\sigma^p_i(A) )^{1/p}$ - note this is nice for organization but this p is never really mentioned\\n  - p=1: **nuclear norm** = **trace norm**: $||X||_* = \\\\sum_i \\\\sigma_i$\\n  - p=2: **frobenius norm** = **euclidean norm**: $||X||_F^2 = \\\\sqrt {\\\\sum_{ij} X_{ij}^2} = \\\\sqrt{\\\\sum_i \\\\sigma_i^2}$\\n\\t\\t- like vector $L_2$ norm\\n  - p=$\\\\infty$: **spectral norm** = **$\\\\mathbf{L_2}$-norm** (of a matrix) = $||X||_2 = \\\\sigma_\\\\text{max}(X) $\\n\\n\\n- entrywise norms\\n\\n  - sum-absolute-value norm (like vector $l_1$) \\n  - maximum-absolute-value norm (like vector $l_\\\\infty$)\\n- *operator norm*\\n  - let $||\\\\cdot||_a$ and $|| \\\\cdot ||_b$ be vector norms\\n  - operator norm $||X||_{a,b} = sup\\\\{ ||Xu||_a \\\\: | \\\\: ||u||_b \\\\leq 1 \\\\}$\\n     - represents the maximum stretching that X does to a vector u\\n  - if using p-norms, can get Frobenius and some others\\n\\n## eigenstuff\\n\\n### eigenvalues intro - strang 5.1\\n\\n- [nice viz](http://setosa.io/ev/eigenvectors-and-eigenvalues/)\\n- elimination changes eigenvalues\\n- eigenvector application to diff eqs $\\\\frac{du}{dt}=Au$\\n  - soln is exponential: $u(t) = c_1 e^{\\\\lambda_1 t} x_1 + c_2 e^{\\\\lambda_2 t} x_2$\\n- *eigenvalue eqn*: $Ax = \\\\lambda x \\\\implies (A-\\\\lambda I)x=0$\\n  - $det(A-\\\\lambda I) = 0$ yields *characteristic polynomial*\\n- eigenvalue properties\\n  - 0 eigenvalue $\\\\implies$ A is singular\\n  - eigenvalues are on the main diagonal when the matrix is triangular\\n- expressions when $A \\\\in \\\\mathbb{S}$\\n  - $\\\\det(A) = \\\\prod_i \\\\lambda_i$\\n  - $tr(A) = \\\\sum_i \\\\lambda_i$\\n  - $||A||_2 = \\\\max | \\\\lambda_i |$\\n  - $||A||_F = \\\\sqrt{\\\\sum \\\\lambda_i^2}$\\n  - $\\\\lambda_{max} (A) = \\\\sup_{x \\\\neq 0} \\\\frac{x^T A x}{x^T x}$\\n  - $\\\\lambda_{min} (A) = \\\\inf_{x \\\\neq 0} \\\\frac{x^T A x}{x^T x}$\\n- *defective matrices* - lack a full set of eigenvalues\\n- *positive semi-definite*:  $A \\\\in R^{nxn}$\\n  - basically these are always *symmetric* $A=A^T$\\n  - all eigenvalues are nonnegative\\n  - if $\\\\forall x \\\\in R^n, x^TAx \\\\geq 0$ then A is positive semi definite (PSD)\\n    - like it curves up\\n    - Note: $x^TAx = \\\\sum_{i, j} x_iA_{i, j} x_j$\\n  - if $\\\\forall x \\\\in R^n, x^TAx > 0$ then A is positive definite (PD)\\n    - PD $\\\\to$ full rank, invertible\\n  - PSD + symmetric $\\\\implies$ can be written as *Gram matrix* $G = X^T X $\\n      - if X full rank, then $G$ is PD\\n  - PSD notation\\n    - $S^n$ - set of symmetric matrices\\n    - $S^n_+$ - set of PSD matrices\\n    - $S^n_{++}$ - set of PD matrices\\n\\n### strang 5.2 - diagonalization\\n\\n- *diagonalization* = *eigenvalue decomposition* = *spectral decomposition*\\n- assume A (nxn) is symmetric\\n  - $A = Q \\\\Lambda Q^T$\\n  - Q := eigenvectors as columns, Q is orthonormal\\n- only diagonalizable if n independent eigenvectors\\n  - not related to invertibility\\n  - eigenvectors corresponding to different eigenvalues are lin. independent\\n  - other Q matrices won't produce diagonal\\n  - there are always n complex eigenvalues\\n  - orthogonal matrix $Q^TQ=I$\\n- examples\\n  - if X, Y symmetric, $tr(YX) = tr(Y \\\\sum \\\\lambda_i q_i q_i^T)$\\n  - lets us easily calculate $A^2$, $sqrt(A)$\\n  - eigenvalues of $A^2$ are squared, eigenvectors remain same\\n  - eigenvalues of $A^{-1}$ are inverse eigenvalues\\n  - eigenvalue of rotation matrix is $i$\\n- eigenvalues for $AB$ only multiply when A and B share eigenvectors\\n  - diagonalizable matrices *share the same eigenvector* matrix S iff $AB = BA$\\n- *generalized eigenvalue decomposition* - for 2 symmetric matrices\\n  - $A = V \\\\Lambda V^T$, $B=VV^T$\\n\\n### strang 6.3 - singular value decomposition\\n\\n- SVD for any nxp matrix: $X=U \\\\Sigma V^T$\\n  - U columns (nxn) are eigenvectors of $XX^T$\\n  - columns of V (pxp) are eigenvectors of $X^TX$\\n  - r singular values on diagonal of $\\\\Sigma$ (nxp) - square roots of nonzero eigenvalues of both $XX^T$ and $X^TX$\\n  - like rotating, scaling, and rotating back\\n  - SVD ex. $A=UDV^T \\\\implies A^{-1} = VD^{-1} U^T$\\n  - $X = \\\\sum_i \\\\sigma_i u_i v_i^T$\\n- properties\\n  1. for PD matrices, $\\\\Sigma=\\\\Lambda$, $U\\\\Sigma V^T = Q \\\\Lambda Q^T$\\n    - for other symmetric matrices, any negative eigenvalues in $\\\\Lambda$ become positive in $\\\\Sigma$\\n- applications\\n  - very numerically stable because U and V are orthogonal matrices\\n  - *condition number* of invertible nxn matrix = $\\\\sigma_{max} / \\\\sigma_{min}$\\n  - $A=U\\\\Sigma V^T = u_1 \\\\sigma_1 v_1^T + ... + u_r \\\\sigma_r v_r^T$\\n    - we can throw away columns corresponding to small $\\\\sigma_i$\\n  - pseudoinverse $A^+ = V \\\\Sigma^+ U^T$\\n\\n###  strang 5.3 - difference eqs and power $A^k$\\n\\n- compound interest\\n- solving for fibonacci numbers\\n- Markov matrices\\n  - steady-state Ax = x \\n  - corresponds to $\\\\lambda = 1$\\n- stability of $u_{k+1} = A u_k$\\n  - stable if all eigenvalues satisfy $|\\\\lambda_i|$  <1\\n  - neutrally stable if some $|\\\\lambda_i|=1$\\n  - unstable if at least one $|\\\\lambda_i|$ > 1\\n- Leontief's input-output matrix\\n- **Perron-Frobenius thm** - if A is a positive matrix (positive values), so is its largest eigenvalue and every component of the corresponding eigenvector is also positive\\n  - useful for ranking, etc.\\n- **power method**: want to find eigenvector $v$ corresponding to largest eigenvalue\\n  - $v = \\\\underset{n \\\\to \\\\infty}{\\\\lim} \\\\frac{A^n v_0}{|A^nv_0|}$ where $v_0$ is nonnegative\\n\\n\\n\\n\",\n",
       " \"---\\nlayout: notes\\ntitle: signals\\ncategory: math\\ntypora-copy-images-to: ../assets\\n---\\n\\n#  signals\\n\\n## basics\\n\\n- dirac-delta infinity at one point, zero everywhere else\\n- ![amplitude-period-ex3](../assets/amplitude-period-ex3.svg)\\n\\n## intro\\n\\n- signal can be *continuous* or *discrete* based on its domain (not values)\\n- *analog signal* - continuous in time\\n  - *sampling* - the process of taking individual values of a continuous-time signal\\n  - *sampling rate* $f_s$ - the number of samples taken per second (Hz)\\n  - *sampling period* - time interval between samples\\n- *digital signal* - discrete in time and value\\n- ”If a function x(t) contains no frequencies higher than B hertz, it is completely determined by giving its ordinates at a series of points spaced 1 2B seconds apart” –Shannon\\n- signals are usually studied in\\n  - time-domain (with respect to time)\\n  - frequency-domain (with respect to frequency) - use Fourier transform\\n  - time-frequency representation (TFR) - use short-time Fourier transform (STFT) or wavelets\\n- harmonic analysis - studies relationship between time and frequency domain\\n- common filters\\n  - low-pass filter - pass only low frequencies\\n  - high-pass filter - pass only high frequencies\\n  - band-pass filter - pass only frequencies within a specified range\\n  - band-stop filter - pass only frequences outside a specified range\\n- **power spectrum** - how much of the signal is at a frequency $\\\\omega$? - square of the magnitude of the coefficients of the Fourier coefficients for $\\\\omega$\\n\\n![Screen Shot 2019-12-11 at 1.37.55 PM](../assets/transforms.png)\\n\\n## fourier analysis\\n\\n- [good tutorial](http://www.thefouriertransform.com/)\\n- Fourier analysis - study of way general functions can be represented by Fourier series\\n- Fourier series - periodic function composed of harmonically related sinusoids, combined by a weighted summation\\n  - one period of the summation can approximate an arbitrary function in that interval\\n- (continuous) Fourier transform $\\\\hat f$: time (x) -> frequency (u)\\n    - $\\\\hat{f}(u) = \\\\int_{-\\\\infty}^{\\\\infty} f(x)\\\\ e^{-2\\\\pi i x u}\\\\,dx$\\n    - $f(x) = \\\\int_{-\\\\infty}^{\\\\infty} \\\\hat f(u)\\\\ e^{2\\\\pi i x u} \\\\,du$\\n    - 2-dimensional (good [ref](http://www.robots.ox.ac.uk/~az/lectures/ia/lect2.pdf))\\n        - $F(u, v) = \\\\int_{-\\\\infty}^{\\\\infty} \\\\int_{-\\\\infty}^{\\\\infty} f(x, y) e^{-i 2 \\\\pi (ux + vy)}\\\\,dx\\\\, dy$\\n        - inverse: $f(x,y) = \\\\int_{-\\\\infty}^{\\\\infty} \\\\int_{-\\\\infty}^{\\\\infty} F(u, v) e^{i 2 \\\\pi (ux + vy)}\\\\,du\\\\, dv$\\n        - for each basis, magnitude of vector [u, v] is frequency and direction gives orientation\\n- Fourier transform of Gaussian is Gaussian\\n- discrete-time Fourier transform - values are still continuous\\n  - $X_{2\\\\pi}(\\\\omega) = \\\\sum_{n=-\\\\infty}^{\\\\infty} x_n \\\\,e^{-i \\\\omega n}$\\n    - $\\\\omega$ is frequency\\n- **discrete Fourier transform**  (DFT or the analysis equation) - this is by far the most common\\n    - $\\\\begin{align}X_k &= \\\\sum_{n=0}^{N-1} x_n e^{-2\\\\pi i k n / N}\\\\\\\\&=\\\\sum_{n=0}^{N-1} x_n \\\\left[ \\\\cos(2\\\\pi k \\\\frac n N ) - i \\\\sin(2 \\\\pi k \\\\frac n N )\\\\right]\\\\end{align}$\\n    - larger k is higher freq.\\n    - **inverse transform**: $x_n = \\\\frac{1}{N} \\\\sum_{k=0}^{N-1} X_k\\\\cdot e^{i 2 \\\\pi k n / N}$\\n    - $x_0, x_1, ... x_{N-1}$ is a sequence of N complex numbers (i.e. time domain) and we transform to another sequence of complex numbers $X_0, X_1, ..., X_{N-1}$\\n        - we write $\\\\mathbf X = \\\\mathcal F (\\\\mathbf x)$ \\n        - vectors $u_k = \\\\left[\\\\left. e^{ \\\\frac{i 2\\\\pi}{N} kn} \\\\;\\\\right|\\\\; n=0,1,\\\\ldots,N-1 \\\\right]^\\\\mathsf{T}$\\n          form an *orthogonal basis* over the set of *N*-dimensional complex vectors\\n    - interpreting units\\n      - a frequency of 1/N would correspond to a period of N\\n        - use $2\\\\pi/N$ so that it goes through one cycle with period of N\\n      - the n=0 parts correspond to a constant\\n      - all other frequencies are integer multiples of the first fundamental frequency\\n      - finding coefs: basically want to use the correlation between the signal and the basis element (this is what the summation and muliptlying does)\\n      - real part - corresponds to even part of the signal (the cosines)\\n      - imaginary part - corresponds to odd parts of the signal\\n- can be quickly computed using the **Fast Fourier Transform** in $O(n \\\\log n)$ instead of $O(n^2)$\\n- **inverse discrete Fourier Transform** (IDFT) \\n- *windowed fourier transform* - chop signal into sections and analyze each section separately\\n\\n## wavelet analysis\\n\\n- [wavelets for kids](http://www.gtwavelet.bme.gatech.edu/wp/kidsA.pdf) - good reference\\n- [overview](https://www.eecis.udel.edu/~amer/CISC651/IEEEwavelet.pdf), [matlab wavelets](https://www.mathworks.com/help/wavelet/ug/wavelet-families-additional-discussion.html), [python wavelets](http://wavelets.pybytes.com/)\\n- different based on choice of wavelet\\n\\n- wavelet has both time and freq information\\n- 2 differences with Fourer\\n  - replace sinusoid $\\\\phi(x) = e^{i 2 \\\\pi k x/N}$ with wavelet\\n  - coefficients must be indexed by more than just frequency (also position, scale, and/or orientation)\\n\\n### using wavelets\\n\\n- **$\\\\phi(x)$ = mother wavelet** (or analyzing wavelet)\\n  - now form translations and dilations of the mother wavelet $\\\\phi(\\\\frac{x-b}{a})$\\n    - it is convenient to set $a=2^{-j}, b = k \\\\cdot 2^{-j}$, where k and j are integers\\n- website to [explore different wavelets](http://wavelets.pybytes.com/)\\n  - ex. **Haar wavelet** (step function on [0, 1]\\n    - ![haar](../assets/haar.png)\\n    - define translations and dilations $\\\\phi_{jk}(x) = \\\\text{const} \\\\cdot \\\\phi(2^j x - k)$\\n      - j, k are still integers\\n      - this is still orthogonal\\n  - ex. **Gabor==Morlet wavelet**: $\\\\phi_\\\\sigma(x) = c \\\\cdot \\\\underbrace{e^{-\\\\frac 1 2 x^2}}_{\\\\text{gaussian window}} \\\\underbrace{(e^{i\\\\sigma x} - \\\\kappa_\\\\sigma)}_{\\\\text{frequency}}$\\n  - ex. **Daubechies wavelet**\\n  - ex. **coiflet**\\n  - ex. scattering transform\\n  - ex. Mallat's MRA - stretch/scale wavelets in a smart way to tile space\\n- [how are wavelets implemented?]() (figs taken from blog)\\n  - note: Continuous Wavelet Transform, (CWT), and the Discrete Wavelet Transform (DWT), **are both, point-by-point, digital, transformations that are easily implemented on a computer**\\n    - DWT restricts the value of the scale and translation of the wavelets (e.g. scale must increase in powers of 2 and translation must be integer)\\n  - ![Screen Shot 2019-12-11 at 2.00.27 PM](../assets/wavelet_comp.png)\\n  - The **approximation coefficients** represent the output of the low pass filter (averaging filter) of the DWT.\\n  - The **detail coefficients** represent the output of the high pass filter (difference filter) of the DWT\\n  - [pywt 2d](https://pywavelets.readthedocs.io/en/latest/ref/2d-decompositions-overview.html) can decompose in different ways\\n    - ![Screen Shot 2019-12-11 at 2.47.53 PM](../assets/wavelet_coefs.png)\\n\\n### wavelet analysis\\n\\n- orthogonal wavelet basis: $\\\\phi_{(s,l)} (x) = 2^{-s/2} \\\\phi (2^{-s} x-l)$\\n- *scaling function* $W(x) = \\\\sum_{k=-1}^{N-2} (-1)^k c_{k+1} \\\\phi (2x+k)$ where $\\\\sum_{k=0,N-1} c_k=2, \\\\: \\\\sum_{k=0}^{N-1} c_k c_{k+2l} = 2 \\\\delta_{l,0}$\\n- one pattern of coefficients is smoothing and another brings out detail = called *quadrature mirror filter pair*\\n- there is also a fast discrete wavelet transform (Mallat)\\n- wavelet packet transform - basis of *wavelet packets* = linear combinations of wavelets\\n- *basis of adapted waveform* - best basis function for a given signal representation\\n- *Marr wavelet* - developed for vision\\n- differential operator and capable of being tuned to act at any desired scale\\n\\n\\n\\n### more general wavelets\\n\\n- [generalized daubechet wavelet families](http://bigwww.epfl.ch/publications/vonesch0702.pdf)\\n- [generalized coiflets](https://pdfs.semanticscholar.org/46e3/4016b8c4b187118e83392242c2165a6db3db.pdf)\\n- [Wavelet families of increasing order in arbitrary dimensions](https://ieeexplore.ieee.org/abstract/document/826784)\\n- [Parametrizing smooth compactly supported wavelets](https://www.ams.org/journals/tran/1993-338-02/S0002-9947-1993-1107031-8/)\\n  - just for daubuchet\",\n",
       " '---\\nlayout: notes\\ntitle: Optimization\\ncategory: math\\ntypora-copy-images-to: ../assets\\n---\\n\\n#  optimization\\n\\n## convex optimization\\n\\n### convex sets (boyd 2)\\n\\n- *affine set*: $x_1, x_2 \\\\in C, \\\\theta \\\\in \\\\mathbb{R} \\\\implies \\\\theta x_1 + (1 - \\\\theta) x_2 \\\\in C$\\n  - *affine hull*: aff C = {$\\\\sum \\\\theta_i x_i | x_i \\\\in C, \\\\sum \\\\theta_i =1 $}\\n- *convex set*: $x_1, x_2 \\\\in C, 0 \\\\leq \\\\theta \\\\leq 1 \\\\implies \\\\theta x_1 + (1 - \\\\theta) x_2 \\u200b$\\n  - *convex hull*: conv C = {$\\\\sum \\\\theta_i x_i \\\\: | x_i \\\\in C, \\\\theta_i \\\\geq 0, \\\\sum \\\\theta_i = 1$}\\n- *cone*: $\\\\theta \\\\geq 0 \\\\implies \\\\theta x \\\\in C\\u200b$\\n- operations that preserve convexity\\n  - intersection (finite intersection of half-spaces)\\n  - pointwise max of affine funcs\\n  - composition\\n  - affine\\n  - perspective\\n  - linar fractional = projective\\n- generalized inequalities:\\n  - *proper cone* K: convex, closed, pointed, solid\\n    - $x \\\\preceq_K y \\\\iff y-x \\\\in K$\\n- *separating hyperplane thm*: C, D convex $C \\\\cap D =\\\\emptyset \\\\implies \\\\exists a \\\\neq 0, b \\\\: s.t. \\\\\\\\ a^Tx \\\\leq b \\\\forall x \\\\in C, \\\\\\\\a^Tx \\\\geq b \\\\forall x \\\\in D$\\n- *supporting hyperplane thm*: {$x|a^tx = a^t x_0$} where $x_0$ on boundary of convex C\\n- dual cone $K^*$ = {$y|x^Ty \\\\geq 0 \\\\: \\\\forall x \\\\in K$}\\n  - $\\\\preceq_{K^*}$ is dual of $\\\\preceq_K$\\n  - $x \\\\preceq_K y \\\\iff \\\\lambda^T x \\\\leq \\\\lambda^T y \\\\quad \\\\forall \\\\: \\\\lambda \\\\succeq_{K^*} 0$\\n\\n#### geometry\\n- *ellipsoid*: {$x \\\\in \\\\mathbb{R}^n | (x-x_c)^T P^{-1} (x-x_c) \\\\leq 1$} where P symmetric, PSD\\n  - {$x_c + Au | ||u||_2 \\\\leq 1$}\\n- *hyperplane*: {$x|a^Tx = b$} ~ creates a halfspace\\n- *norm cone*: {$(x, t)| \\\\: ||x|| \\\\leq t$}\\n- *polyhedron*: {x | Ax=b, Cx=d} = $\\\\{ \\\\sum_i^k \\\\theta_i v_i \\\\; | \\\\sum_i^m \\\\theta_i = 1, \\\\theta_i \\\\geq 0 \\\\} \\\\: m \\\\leq k$  \\n- *simplex*: conv{$v_{0:k}$}\\n\\n### convex funcs (boyd 3)\\n- definitions\\n  1. *Jensen\\'s inequality* $0 \\\\leq \\\\theta \\\\leq 1$\\n    - $f(\\\\theta x_1 + (1 - \\\\theta) x_2) \\\\leq \\\\theta f(x_1) + (1 - \\\\theta) f(x_2)$\\n    - $f(E[X]) \\\\leq E[f(X)]$\\n  2. $\\\\nabla^2 f(x) \\\\succeq 0$\\n  3. $f(x_2) \\\\geq f(x_1) + \\\\nabla f(x_1)^T (x_2 - x_1)$\\n    - can show this by restricting to an arbitrary line\\n  4. consider epi f - also use things that preseve convexity\\n- concepts\\n  - *epigraph* epi f = $\\\\{ (x, t) \\\\; | \\\\: x \\\\in dom f, f(x) \\\\leq t \\\\}$\\n  - *extended value extension*: $\\\\tilde{f}(x) = f(x)$ if $x \\\\in dom f$ else $\\\\infty$\\n  - *wide sense function* - can take on values $\\\\pm \\\\infty$\\n    - = dom f = {$x | f(x) < \\\\infty$}\\n  - *wide sense convex func*: $f(x) = inf \\\\{ t \\\\in \\\\mathbb{R} | (x, t) \\\\in F\\\\}$ where $F \\\\subseteq \\\\mathbb{R}^{n+1}$\\n    - $F(x) = inf \\\\{ t \\\\in \\\\mathbb{R} | (x, t) \\\\in F \\\\}$\\n  - $\\\\alpha$-sublevel set of convex func is convex\\n- operations that preserve convexity\\n  - nonnegative weighted sums ~ multiplies for logs\\n  - affine map\\n  - pointwise max of convex\\n  - composition\\n  - perspective\\n  - minimization ~ sometimes\\n- *conjugate* of f\\n  - $f^*(y) = \\\\underset{x \\\\in dom f}{sup} \\\\: y^T x - f(x)$\\n  - dom $f^*$ = {$y|f^*(y)$ is finite}\\n  - called *Legendre transform* when f differentiable\\n  - *fenchel\\'s inequality*: $f(x) + f^*(y) \\\\geq x^ty$\\n  - $f^{**} = f$ iff convex, closed\\n- ex. $f(S) = log \\\\: det X^{-1}$\\n  - $f^*(Y) = \\\\underset{X}{sup} [tr(YX) + log \\\\: det X]$\\n    - $= -n - log \\\\: det(-Y) $ if $-Y \\\\in S^n_{++}$\\n- can use conj. to go other way: $f(y) = \\\\underset{x}{sup}(y^Tx - f^*(x))$\\n\\n\\n### optimization problems (boyd 4)\\n### optimization\\n\\n- standard form: $p^* = min \\\\: f_0(x)\\\\\\\\s.t. \\\\: f_i(x) \\\\leq 0 \\\\\\\\ h_i(x) = 0$\\n- equivalent problems\\n  - change of vars\\n  - constraint transformations\\n  - slack vars\\n  - eliminating equalities\\n  - eliminating linear equalities\\n  - introducing equalities\\n  - optimizing over some vars ~ ex. quadratic\\n  - epigraph form: $min \\\\: t \\\\: s.t. \\\\: f_0 \\\\leq t$\\n  - implicit + explicit constraints\\n\\n### convex optimization\\n- standard form: $$p^* = min \\\\: f_0(x)\\\\\\\\s.t. \\\\: f_i(x) \\\\leq 0 \\\\\\\\ a_i^Tx = b_i$$ where all f are convex\\n- optimality criteria (special cases of KKT)\\n  - x optimal if\\n    1. x is feasible\\n    2. $\\\\nabla f_0 (x)^T(y-x) \\\\geq 0 \\\\: \\\\forall y $ feasible\\n  - if unconstrained $\\\\nabla f_0 (x) = 0$\\n  - if equality only Ax=b, $\\\\nabla f_0 (x) \\\\perp N(A)$\\n  - $x \\\\succeq 0$, $\\\\nabla f_0 (x) \\\\succeq 0; x_i (\\\\nabla f_0 (x))_i = 0$\\n- equivalent convex problems\\n  - eliminating equality constraints\\n  - introducing equality constraints\\n  - slack vars ~ for linear inequalities\\n  - epigraph form\\n  - minimizing over some vars\\n\\n### linear optimization\\n- $$p^* = min \\\\: c^T x + d\\\\\\\\s.t. \\\\: Gx \\\\succeq h \\\\\\\\ Ax=b$$\\n- standard form $x \\\\succeq 0$ is the only inequality\\n- standard dual: max $-b^T \\\\nu$ s.t. $A^T \\\\nu + c \\\\geq 0$\\n- *linear-fractional* program\\n  - $min \\\\: \\\\frac{c^Tx + d}{e^tx+f} \\\\\\\\ s.t \\\\: Gx \\\\succeq h \\\\\\\\ Ax = b$ ~ can be converted to LP\\n\\n### quadratic optimization\\n- $$min \\\\: 1/2 x^TPx + q^Txr \\\\\\\\s.t. \\\\: Gx \\\\succeq h$$ where $P \\\\in S_+^n$\\n- *QCQP* - inequality constraints also convex\\n  - ex. $min \\\\: ||Ax-b||_2^2$\\n- *SOCP* - $$min f^Tx \\\\\\\\ s.t. \\\\: ||A_ix+b_i||_2 \\\\leq c_i^T + d_i \\\\\\\\ Fx=g$$\\n\\n### geometric program\\n- $$min \\\\: f_0(x) \\\\\\\\ s.t. \\\\: f_i(x) \\\\leq 1 \\\\: i = 1:m \\\\\\\\ h_i(x) = 1 \\\\: i = 1:p$$ where $f_{0:m}$ posynomials, $h_i$ monomials\\n  - *monomial* $f(x) = c x_1^{a_1} \\\\cdot x_n^{a_n}, c>0$\\n  - posynomial ~ sum of monomials ~ can transform into convex w/ $y_i = log x_i$\\n\\n### generalized inequality\\n- $$min \\\\: f_0(x)\\\\\\\\s.t. \\\\: f_i(x) \\\\preceq_{K_i} 0, i=1:m\\\\\\\\Ax=b$$\\n- *conic form problem*: $$min \\\\: c^Tx \\\\\\\\ s.t. \\\\: Fx +g \\\\preceq_K 0\\\\\\\\Ax=b$$ ~ set $K=S_+^K$\\n- *SDP* = *semi-definite program*: $$min \\\\: c^T x\\\\\\\\s.t. \\\\: x_1F_1+...+x_nF_n+G \\\\preceq 0\\\\\\\\Ax=b$$ ~ where $F_1, ..., F_n \\\\in S^k$\\n  - standard form: $min \\\\: tr(CX) \\\\\\\\ s.to \\\\: tr(A_iX)=b_i \\\\\\\\ X \\\\succeq 0$\\n\\n## duality (boyd 5)\\n- consider $\\\\min \\\\: f_0 (x) \\\\\\\\ s.t. \\\\: f_i(x) \\\\leq 0 \\\\\\\\ h_i(x) = 0$\\n- *lagrangian* $L(x, \\\\lambda, \\\\nu) = f_0(x) + \\\\sum \\\\lambda_i f_i(x) + \\\\sum \\\\nu_i h_i(x)$\\n- *dual function* $g(\\\\lambda, \\\\nu) = \\\\underset{x \\\\in D}{\\\\inf} L(x, \\\\lambda, \\\\nu)$ ~ g always concave\\n\\n  - $\\\\lambda \\\\succeq 0 \\\\implies g(\\\\lambda, \\\\nu) \\\\leq p^*$\\n- $(\\\\lambda, \\\\nu)$ *dual feasible* if\\n  1. $\\\\lambda \\\\succeq 0$\\n  2. $(\\\\lambda, \\\\nu) \\\\in dom \\\\: g$\\n  - when $p^* = - \\\\infty$, dual infeasible\\n  - when $d^*=\\\\infty$, primal infeasible\\n- dual related to to conjugate func\\n\\n  - ex. min f(x) s.t. $x = 0 \\\\implies g(\\\\nu) = -f^*(-\\\\nu)$\\n- *lagrange dual problem*: $\\\\max \\\\: g(\\\\lambda, \\\\nu)\\\\\\\\s.t. \\\\: \\\\lambda \\\\succeq 0$\\n- *weak duality*: $d^* \\\\leq p^*$\\n\\n  - *optimal duality gap*: $p^* - d^*$\\n- *strong duality*: $d^* = p^*$ ~ requires more than convexity\\n- *slater\\'s condition* ~ if problem convex $\\\\implies$ strong duality + $\\\\exists$ dual optimal point\\n  - $\\\\exists x \\\\in relint \\\\: D\\\\\\\\f_i(x) < 0\\\\\\\\Ax = b$ ~ point is strictly feasible\\n  - to weaken this, affine $f_i$ can be $\\\\leq 0$\\n- *sion\\'s minimax thm*: $x \\\\to f(x, y)$ ~ conditions\\n\\n  - $\\\\implies \\\\underset{x}{min} \\\\: \\\\underset{y}{sup} \\\\: f(x,y) = \\\\underset{y}{sup} \\\\: \\\\underset{x}{min} \\\\: f(x,y)$\\n\\n### optimality conditions\\n- *duality gap*: $f_0(x) - g(\\\\lambda, \\\\nu)$\\n- can use stopping condition duality gap $\\\\leq \\\\epsilon_{abs}$ to be $\\\\epsilon_{abs}$ - suboptimal\\n- strong duality yields *complementary slackness*\\n  - $\\\\lambda_i f_i(x^*)=0$\\n- *KKT optimality conditions* ~ assume $f_0, f_i, h_i$ differentiable, strong duality\\n  1. $f_i(x^*) \\\\leq 0$\\n  2. $h_i(x^*) = 0$\\n  3. $\\\\lambda_i^* \\\\geq 0$\\n  4. $\\\\lambda_i^*f(x_i^*) = 0$\\n  5. $\\\\nabla f_0 (x^*) + \\\\sum \\\\lambda_i^* \\\\nabla f_i (x_i^*) + \\\\sum \\\\nu_i^* \\\\nabla h_i (x^*) = 0$\\n\\n\\n### thms of alternatives\\n- *weak alternative* - at most one of 2 is true\\n- *strong alternative* - exactly one is true\\n  - ex. Fredholm alternative\\n  - ex. Farkas\\'s lamma\\n    1. $\\\\exists x \\\\: Ax \\\\leq 0, c^Tx < 0$\\n    2. $\\\\exists y \\\\: y \\\\geq 0, A^Ty + c = 0$\\n\\n## approx + fitting (boyd 6)\\n### norm approx problem\\n- minimize $||Ax-b||$\\n- ex. weighted norm approx. min||W(Ax-b)||\\n- ex. least squares min ||Ax-b||$_2^2$\\n- ex. chebyshev approx norm min||Ax-b||$_\\\\infty$\\n- ex. penalty function approx problem: $min \\\\: \\\\phi(r_1) + ... + \\\\phi(r_m)\\\\\\\\s.t. \\\\: r=Ax-b$\\n\\n### least norm problem\\n- min $||x||\\\\\\\\s.t. \\\\: Ax=b$ ~ min $||x_0+ Zu||$, Z cols basis for N(A)\\n\\n### regularized approximation\\n- min $||Ax-b|| + \\\\gamma ||x||$\\n- min $||Ax-b||^2 + \\\\gamma ||x||^2$\\n- *Tikhonov*: $min \\\\: ||Ax-b||_2^2 + \\\\gamma ||x||_2^2$\\n- examples\\n  - ex. regularize w/ ||Dx||\\n  - ex. lasso\\n  - ex. quadratic smoothing\\n  - ex. total variation\\n\\n### robust approximation\\n- $A = \\\\bar{A} + U$ ~ random w/ mean 0\\n1. stochastic robust approx problem: $min \\\\: E||Ax-b||$\\n2. (worst-case) robust approx prob: $min \\\\: sup ||Ax-b|| \\\\: | A \\\\in \\\\mathcal{A}$\\n\\n### function fitting\\n- $f(u) = x_1 f_1 (u) + .... + x_n f_n (u)$ ~ $f_i$ are basis funcs, $x_i$ are coefficients\\n- sparse descriptions + basis pursuit\\n- interpolation\\n\\n## unconstrained minimization (boyd 9)\\n\\n### unconstrained problems\\n- $x^* = \\\\text{argmin} \\\\: f(x) \\\\implies \\\\nabla f(x^*) = 0$\\n- examples\\n  - ex. quadratic: $\\\\min \\\\: 1/2 x^TPX + q^Tx + r$\\n    - solved w/ $Px^* + q = 0$, if $P \\\\succeq 0$, unique soln $-P^{-1}q$\\n  - ex. unconstrained geometric program\\n  - ex. analytic center of linear inequalities\\n    - $\\\\min \\\\: f(x) = -\\\\sum \\\\: \\\\log (b_i - a_i^Tx)$ where dom f = $\\\\{x|a_i^Tx< b_i, i = 1:m\\\\}$\\n- 3 definitions of convexity\\n  - $0 \\\\leq \\\\theta \\\\leq 1$\\n    - $f(\\\\theta x_1 + (1 - \\\\theta) x_2) \\\\leq \\\\theta f(x_1) + (1 - \\\\theta) f(x_2)$\\n  - $\\\\nabla^2 f(x) \\\\succeq 0$\\n  - $f(x_2) \\\\geq f(x_1) + \\\\nabla f(x_1)^T (x_2 - x_1)$\\n- $\\\\color{red}0 \\\\preceq \\\\color{green}{\\\\underset{\\\\text{strong convexity}}{mI}} \\\\preceq \\\\nabla^2 \\\\color{cornflowerblue}{f(x)} \\\\preceq \\\\underset{\\\\text{smoothness}}{MI}$\\n  - $\\\\kappa = M/m$ bounds *condition number* of $\\\\nabla^2 f = \\\\frac{\\\\lambda_{\\\\max}(\\\\nabla^2 f)}{\\\\lambda_{\\\\min}(\\\\nabla^2 f)}$\\n  - *strongly convex*: $\\\\nabla^2 f(x) \\\\succeq mI$\\n    - $\\\\implies f(x_2) \\\\geq f(x_1) + \\\\nabla f(x_1)^T(x_2-x_1) + m/2 ||x_2-x_1||_2^2$\\n    - minimizing yields $p^* \\\\geq f(x) - 1/(2m) ||\\\\nabla f(x)||_2^2$\\n    - if the gradient of f at x is small enough, then the difference between f(x) and p⋆ is small \\n  - *smooth*: $\\\\exists \\\\: M, \\\\: \\\\nabla^2f(x) \\\\preceq MI$\\n    - $\\\\implies f(y) \\\\leq f(x) + \\\\nabla f(x)^T(y-x) + M/2 ||y-x||_2^2$\\n- cond(*C*) = $W_{\\\\max}^2 / W_{\\\\min}^2\\u200b$\\n  - *width* of convex set $C \\\\subset \\\\mathbb{R}^n$ in direction q with $||q||_2=1$\\n  - $W(C, q) = \\\\underset{z \\\\in C}{\\\\sup} \\\\: q^Tz - \\\\underset{z \\\\in C}{\\\\inf} \\\\: q^Tz$\\n- *alpha-level subset*: $C_\\\\alpha = \\\\{x|f(x) \\\\leq \\\\alpha\\\\}$\\n\\n### descent methods\\n\\n- update rule $x = x + t \\\\Delta x$\\n- *exact line search*: $t = \\\\underset{s \\\\geq 0}{\\\\text{argmin}} \\\\:f(x+s \\\\Delta x)$\\n- *backtracking line search*\\n  - given a descent direction $\\\\Delta x \\\\text{ for } f, x \\\\in dom \\\\: f, \\\\alpha \\\\in (0, 0.5), \\\\beta \\\\in (0, 1)$\\n  - t:=1, $\\\\alpha \\\\in (0, 0.5), \\\\beta \\\\in (0, 1)$\\n  - while $f(x + t \\\\Delta x) > f(x) + \\\\alpha t \\\\nabla f(x)^T \\\\Delta x$\\n    - $t *= \\\\beta$\\n  - ![Screen Shot 2018-07-30 at 10.23.19 PM-3014637](assets/Screen Shot 2018-07-30 at 10.23.19 PM-3014637.png)\\n\\n### gd method\\n\\n- convergence\\n  - can bound number of iterations required to be less than $\\\\epsilon$\\n- examples\\n  - a quadratic problem in $R^2$\\n  - non-quadratic problem in $R^2$\\n  - a problem in $R^{100}$\\n  - gradient method and condition number\\n- conclusions\\n  - gd often exhibits approximately linear convergence\\n  - convergence rate depends greatly on $cond (\\\\nabla^2 f(x))$ or sublevel sets\\n\\n### steepest descent method\\n- examples\\n  - euclidean norm: $\\\\Delta x_{sd} = - \\\\nabla f(x)$\\n  - quadratic norm $||z||_P = (z^TPz)^{1/2} = ||P^{1/2}z||_2$ where $P \\\\in S_{++}^n$\\n    - $\\\\Delta x_{sd} = -P^{-1} \\\\nabla f(x)$\\n  - $\\\\ell_1$ norm: $\\\\Delta_{sd} = -\\\\frac{\\\\partial f(x)}{\\\\partial x_i} e_i$\\n\\n### newton\\'s method\\n- *Newton step* $\\\\Delta x_{nt} = - \\\\nabla^2 f(x)^{-1} \\\\nabla f(x)$\\n  - PSD $\\\\implies \\\\nabla f(x)^T \\\\Delta x_{nt} = - \\\\nabla f(x)^T \\\\nabla^2 f(x)^{-1} \\\\nabla f(x) < 0$\\n- *Newton\\'s method*\\n  1. compute the newton step $\\\\Delta x_{nt}$ and decrement $\\\\lambda^2 = \\\\nabla f(x)^T \\\\nabla^2 f(x)^{-1} \\\\nabla f(x)$\\n  2. stopping criterion: quit if $\\\\lambda^2 / 2 \\\\leq \\\\epsilon\\u200b$\\n  3. line search: choose step size t w/ backtracking line search\\n  4. update: $x += t \\\\Delta x_{nt}$\\n\\n\\n## basic algorithms\\n\\n- types: *batch* (have full data) vs *online*\\n\\n1. gradient descent = *batch gradient descent*\\n   - *gradient* - vector that points to direction of maximum increase\\n   - at every step, subtract gradient multiplied by learning rate: $x_k = x_{k-1} - \\\\alpha \\\\nabla_x F(x_{k-1})$\\n   - alpha = 0.05 seems to work\\n   - $J(\\\\theta) = 1/2 (\\\\theta ^T X^T X \\\\theta - 2 \\\\theta^T X^T y + y^T y)$\\n   - $\\\\nabla_\\\\theta J(\\\\theta) = X^T X \\\\theta - X^T Y$\\n     - = $\\\\sum_i  x_i  (x_i^T - y_i)$\\n     - this represents residuals * examples\\n2. stochastic gradient descent\\n   - don\\'t use all training examples - approximates gradient\\n     - single-sample\\n     - mini-batch (usually better in offline case)\\n   - *coordinate-descent* algorithm\\n   - *online* algorithm - update theta while training data is changing\\n   - when to stop?\\n     - predetermined number of iterations\\n     - stop when improvement drops below a threshold\\n   - each pass of the whole data = 1 epoch\\n   - benefits\\n     1. less prone to getting stuck to shallow local minima\\n     2. don\\'t need huge ram\\n     3. faster\\n3. newton\\'s method for optimization\\n   - *second-order* optimization - requires 1st & 2nd derivatives\\n   - $\\\\theta_{k+1} = \\\\theta_k - H_K^{-1} g_k$\\n   - update with inverse of Hessian as alpha - this is an approximation to a taylor series\\n   - finding inverse of Hessian can be hard / expensive\\n4. ADMM - *alternating direction method of multipliers* (ADMM) is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle\\n\\n## expectation maximization - j 11\\n\\n- method to maximize likelihood on model with observed X and hidden Z\\n  1. *expectation step* - values of unobserved latent variables are filled in\\n     - calculates prob of latent variables given observed variables and current param values\\n  2. *maximization step* - parameters are adjusted based on filled-in variables\\n- goal: maximize *complete log-likelihood*, but don\\'t know z\\n  - *expected complete log-likelihood* $E_{p\\'}[l(\\\\theta; x,z)] = \\\\sum_z p\\'(z|x,\\\\theta) \\\\cdot \\\\log \\\\: p(x,z|\\\\theta)$\\n    - p\\' distribution is assignment to z vars\\n  - deriving *auxilary function* $\\\\mathcal L(q, \\\\theta, x) = \\\\sum_z p\\'(z|x) \\\\log \\\\frac{p(x,z|\\\\theta)}{p\\'(z|x)}$ - lower bound for the log likelihood\\n  - $\\\\begin{align} l(\\\\theta; x) &= \\\\log \\\\: p(x|\\\\theta) & \\\\text{incomplete log-likelihood} \\\\\\\\&= \\\\log \\\\sum_z p(x,z|\\\\theta) &\\\\text{complete log-likelihood}\\\\\\\\&= \\\\log\\\\sum_z p\\'(z|x) \\\\frac{p(x,z|\\\\theta)}{p\\'(z|x)} &\\\\text{multiplying by 1} \\\\\\\\ &\\\\geq \\\\sum_z p\\'(z|x) \\\\log \\\\frac{p(x,z|\\\\theta)}{p\\'(z|x)} &\\\\text{Jensen\\'s inequality}\\\\\\\\&\\\\triangleq \\\\mathcal L (p\\', \\\\theta) \\\\end{align}$\\n  - this removes dependence on z\\n- steps\\n  - E: $p\\'(z|x, \\\\theta) = \\\\underset{p\\'}{\\\\text{argmax}}\\\\: \\\\mathcal L(p\\',\\\\theta, x)$\\n  - M: $\\\\theta = \\\\underset{\\\\theta}{\\\\text{argmax}} \\\\: \\\\mathcal L(p\\', \\\\theta, x)$\\n  - equivalent to maximizing expected complete log-likelihood\\n  - *stochastically* converges to *local* minimum\\n- alternatively, can look at kl-divergences\\n\\n## nn optimization\\n\\n### why is it hard?\\n\\n- plateaus\\n- winding canyons\\n- cliffs\\n- local maxima to dodge\\n- saddle points (local max and local min)\\n- most popular\\n  - sgd\\n  - sgd + nesterov momentum\\n  - adam\\n  - adagrad - maintains a per-parameter learning rate that improves performance on problems with sparse gradients\\n  - rmsprop - (ignore) per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing)\\n- adam - \"adaptive moment estimation\" (kingma_2015)\\n  - keep track of per-parameter learning rate (based on first moment of gradients tracked) and per-parameter second moment (based on variance of gradients tracked)\\n  - alpha - learning rate\\n  - beta1 - exponential decay rate for first moment estimate\\n    - default 0.9\\n  - beta2 - exponential decay rate for 2nd moment estimates (should be higher when gradients sparser)\\n    - default 0.999\\n  - epsilon - small number to prevent division by zero\\n    - default 1e-8 - usually requires tuning (ex. inception requires 1e-1) ![Screen Shot 2018-10-11 at 8.07.56 AM](../assets/adam.png)visualization\\n\\n- requires low dims\\n  - goodfellow 2015 \"Qualitatively characterizing neural network optimization problems\" plots loss on line from starting point to ending point\\n  - could do PCA on params\\n\\n### complicated is simpler\\n\\n- ex. $x^3 \\\\sin(x)$ is simpler than just $x$ on the domain [−0.01, 0.01]\\n- dropout is like ridge',\n",
       " \"---\\nlayout: notes\\ntitle: calculus\\ncategory: math\\n---\\n\\n#  calculus\\n\\n## taylor expansion\\n- taylor expansion: $f(x) \\\\approx f(x_0) + \\\\frac{f'(x_0)}{1!}(x-x_0) + \\\\frac{f''(x_0)}{2!}(x-x_0)^2 + ...$\\n\\n\\n## Single-variable calculus\\nDerivatives:\\n\\n$\\\\frac{d}{dx}x^n = nx^{n-1}$\\n\\n$\\\\frac{d}{dx}a^x = a^{x}ln(a)$\\n\\n$\\\\frac{d}{dx}ln(x) = 1/x$\\n\\n$\\\\frac{d}{dx}tan(x)= sec^2(x)$\\n\\n$\\\\frac{d}{dx}cot(x)= -csc^2(x)$\\n\\n$\\\\frac{d}{dx}sec(x)= sec(x)tan(x)$\\n\\n$\\\\frac{d}{dx}csc(x)= -csc(x)cot(x)$\\n\\n$\\\\int tan = ln\\\\|sec\\\\|$\\n\\n$\\\\int cot = ln\\\\|sin\\\\|$\\n\\n$\\\\int sec = ln\\\\|sec+tan\\\\|$\\n\\n$\\\\int csc = ln\\\\|csc-cot\\\\|$\\n\\n$\\\\int \\\\frac{du}{\\\\sqrt{a^2-u^2}} = sin^{-1}(\\\\frac{u}{a})$\\n\\n$\\\\int \\\\frac{du}{u\\\\sqrt{u^2-a^2}} = \\\\frac{1}{a}sec^{-1}(\\\\frac{u}{a})$\\n\\n$\\\\int \\\\frac{du}{a^2+u^2} = \\\\frac{1}{a} tan^{-1}(\\\\frac{u}{a})$\\n\\nContinuous: left limit = right limit = value\\n\\nDifferentiable: continuous and no sharp points / asymptotes\\n\\nL'Hospital's - for indeterminate forms: $(\\\\frac{f(x)}{g(x)})' = \\\\frac{f'(x)}{g'(x)}$\\n\\nIntegration by parts: $\\\\int{udv}=uv-\\\\int{duv}$, LIATE\\n\\nExpansions:\\n\\n$e^x = \\\\sum{\\\\frac{x^n}{n!}}$\\n\\n$sin(x) = \\\\sum_0^\\\\infty{\\\\frac{(-1)^n x^{2n+1}}{(2n+1)!}}$\\n\\n$cos(x) = \\\\sum_0^\\\\infty{\\\\frac{(-1)^n x^{2n}}{(2n)!}}$\\n\\nGeometric Sum: $a_{1st}\\\\frac{1-r^{n+1}}{1-r}$\\n\\n## Multivariable calculus\\n- Polar: r,$\\\\theta$,z\\n- Spherical: $\\\\rho,\\\\theta,\\\\phi$\\n- Clairut's Thm: Conservative function $f_{xy}=f_{yx}$\\n- *Lagrangian* - solves minimize f subject to g = c\\n\\t- solution will always be *tangent* to f\\n\\t- $\\\\nabla f = \\\\lambda \\\\nabla g$ - gives us n constraints\\n\\t- remember g = c is a constraint too\\n\\t- to do this efficiently, define the *Lagrangian* $L(x, \\\\lambda) = f - \\\\lambda \\\\cdot g$\\n\\t\\t- taking deriv wrt $\\\\lambda$ and setting = 0 enforces g = c \\n\\t\\t- taking deriv wrt other variables and setting = 0 enforces other conditions\\n\\t\\t- therefore final eq just becomes $\\\\nabla L = 0$\",\n",
       " \"---\\nlayout: notes\\ntitle: chaos\\ncategory: math\\n---\\n\\n#  chaos\\n\\n## Normal forms of Hopf bifurcations\\n- pitchfork: $$\\\\dot{x} = \\\\lambda x - x^3$$ ![](../assets/pitch.png)\\n- subcritical pitchfork: $\\\\dot{x} = \\\\lambda x + x^3$ ![](../assets/subpitch.png)\\n- saddle node (turning point): $\\\\dot{x} = \\\\lambda - x^2$ ![](../assets/sn.png)\\n- transcritical: $\\\\dot{x} = \\\\lambda x - x^2$ ![](../assets/trans.png)\\n\\n## important figs\\n- period-doubling (flip bifurcation) $f = \\\\mu x (1-x) (f = \\\\mu sin (\\\\pi x) $ is similiar) ![](../assets/flip.png)\\n- inverse tangent bifurcation - unstable and stable P-3 orbits coalesce, move slightly off bisector and becomes chaotic ![](../assets/inverse.png)\\n- pendulum ![](../assets/pendulum.png)\\n- energy surface - trajectories run around the surface, not down it ![](../assets/e_surface.png)\\n- Conservative systems: 6.5\\n\\t- study Hamiltonian p. 187-188\\n- Pendulum: 6.7\\n- dynamics - study of things that evolve with time\\n- chaos - deterministic, aperiodic, sensitive, long-term prediction impossible\\n1. phase space - has coordinates $x_1,...,x_n$\\n2. phase portrait - variable x-axis, derivative y-axis\\n3. bifurcation diagram - parameter x-axis, steady state y-axis\\n\\t- draw separate graphs for these\\n- first check - look for fixed points\\n- for 1-D, if f' $<$ 0 then stable\\n- stable f.p. = all possible ICs in a.s.b.f.n. result in trajectories that remain in a.s.b.f.n. for all time\\n- asymptotically stable f.p. - stable and approaches f.p. as $t\\\\ra\\\\infty$\\n- hyperbolic f.p. - eigenvals aren't strictly imaginary\\n- bifurcation point of f.p. - point where num solutions change or phase portraits change significantly\\n- globally stable - stable from any ICs\\n- autonomous = f is a function of x, not t\\n- we can always make a system autonomous by having $x_n$ = t, so $\\\\dot{x_n}$ = 1\\n- dimension = number of 1st order ODEs, dimension of phase-space\\n- existence and uniqueness thm: if $\\\\dot{x}$ and $\\\\dot{x}'$ are continuous, then there is some unique solution\\n- linearization - used to find stability of f.p.s\\n- $$\\n\\\\dot{x} = f(x) &\\\\\\\\\\n\\\\text{define }\\\\delta x = (x-\\\\bar{x}) \\\\\\\\\\n\\\\dot{\\\\delta x} = \\\\frac{d}{dt}(x-\\\\bar{x}) = \\\\dot{x} = f(x) = f(\\\\bar{x}+\\\\delta x) \\\\\\\\\\n\\\\dot{\\\\delta x} =\\\\cancelto{0}{f(\\\\bar{x})} + \\\\delta x f'(\\\\bar{x}) + \\\\cancelto{\\\\text{0 by HGT iff f'!=0}}{O(x^2)} \\\\\\\\\\n\\\\dot{\\\\delta x} = \\\\delta x f'(\\\\bar{x}) \\\\to \\\\text{ now solve FOLDE} \\\\\\\\\\n$$\\n\\n- solving Hopf: use polar to get $\\\\dot{\\\\rho}, \\\\dot{\\\\theta}$\\n- multiply one thing by cos, one by sin, then add\\n\\n- $ \\\\rho = \\\\sqrt{x_1^2 + x_2^2} \\\\\\\\\\n\\\\theta = tan^{-1}(\\\\frac{x_2}{x_1})$\\n- Hysterisis curve - S-shaped curve of fixed branches - ruler getting larger - snap bifurcation - both axes are parameters\\n\\n## Systems of Linear ODEs\\n- solutions are of the form $\\\\underbar{x}(t) = \\\\underbar{C}_1e^{\\\\alpha_1 t} + \\\\underbar{C}_2e^{\\\\alpha_2 t}$\\n- Eigenspaces: $E^S$ (stable), $E^U$ (unstable), $E^C$ (center - real part) - plot eigenvectors\\n- how to solve these systems?\\n\\t- solve eigenvectors\\n- positive real part - goes out\\n- negative real part - goes in\\n- bifurcation requires 0 as eigenvalue\\n- has imaginary component: spiral / focus\\n- purely imaginary - center = stable, but not a.s.\\n- finite velocity = $\\\\frac{dRe(\\\\alpha)}{d\\\\lambda}$\\n- change coordinates to polar\\n- for $\\\\lambda \\\\geq 0$, solution is a stable L.C. (from either direction spirals into a circular orbit)\\n- attracting - any trajectory that starts within $\\\\delta$ of $\\\\bar{\\\\underbar{x}}$ evolves to $\\\\bar{\\\\underbar{x}}$ as t $\\\\to \\\\infty$ (it doesn't have to remain within $\\\\delta$ at all times\\n- stable (Lyapanov stable) - any trajectory that starts within $\\\\delta$ remains within $\\\\varepsilon$ for all time ($\\\\varepsilon$ is chosen first)\\n- asymptotically stable - attracting and stable\\n- hyperbolic f.p. - iff all eigenvals of the linearization of the nds about the f.p. have nonzero real parts \\\\\\\\\\n\\n## Discrete Nonlinear Dynamical Systems\\n- functional iteration: $x_{n+m} = f^m(x_n)$ (apply f m times)\\n- fixed point: $f(x^*)=x^*$\\n- f.p. stable if $\\\\|\\\\frac{df}{dx}(x^*)\\\\|<1$, unstable if $>$ 1\\n- check n-orbit by checking nth derivative: $\\\\frac{df^n}{dx}(x_i^*) = \\\\prod_{i=1}^{n-1} \\\\frac{df}{dx}(x_i^*)$\\n- period-doubling bifurcations\\n- self-stability - orbit for which the stability-determining derivative is zero.  This means that the max of the map and the point at which the max occurs are in the orbit.\\n- type I intermittency - exhibited by inverse tangent bifurcation\\n- Feigenbaum sequence - period-doubling path to chaos, keep increasing parameter until period is chaotic\\n\\n\\\\begin{center}\\n\\\\begin{tabular}{ \\\\| m{4cm} \\\\| m{4cm} \\\\| } \\n\\\\hline\\n\\\\multicolumn{2}{\\\\|c\\\\|}{3D Attractors} \\\\\\\\\\n \\\\hline\\nType of Attractor & Sign of Exponents \\\\\\\\ \\n\\\\hline\\nFixed Point & (-, -, -)\\\\\\\\ \\nLimit Cycle & (0, -, -) \\\\\\\\ \\nTorus & (0, 0, -) \\\\\\\\\\nStrange Attractor & (+, 0, -) \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\n- homoclinic orbit - connects unstable manifold of saddle point to its own stable manifold\\n\\t- e.g. trajectory that starts and ends at the same fixed point\\n- manifolds are denoted by a W (ex. $W^S$ is the stable manifold)\\n- heteroclinic orbit - connects unstable manifold of fp to stable manifold of another fp \\\\\\\\\\n\\n## Conservative Systems\\n- $F(x) = -\\\\frac{dV}{dx}$ (by defn.)\\n- $m\\\\ddot{x}+\\\\frac{dV}{dx}=0$, multiply by $\\\\dot{x} \\\\to \\\\frac{d}{dt}[\\\\frac{1}{2}m\\\\dot{x}^2+V(x)]=0$\\n- so total energy $E=\\\\frac{1}{2}m\\\\dot{x}^2+V(x)$\\n- motion of pendulum: $\\\\frac{d^2\\\\theta}{dt^2}+\\\\frac{g}{L}sin\\\\theta=0$\\n- nondimensionalize with $\\\\omega=\\\\sqrt{g/L}, \\\\tau=\\\\omega t \\\\to \\\\ddot{\\\\theta}+sin\\\\theta =0$\\n- can multiply this by $\\\\dot{\\\\theta}$\\n- $\\\\omega$-limit $t \\\\to \\\\infty$\\n- $\\\\alpha$-limit  $t \\\\to -\\\\infty$\\n- libration - small orbit surrounding center\\n- system: $\\\\dot{\\\\theta}=\\\\nu$, $\\\\dot{\\\\nu} = -sin\\\\theta$\\n\\n### Hamiltonian Dynamical System\\n- $\\\\dot{\\\\underbar{x}}=\\\\frac{\\\\partial H}{\\\\partial y}(\\\\underbar{x},\\\\underbar{y})$\\n, $\\\\dot{\\\\underbar{y}}=-\\\\frac{\\\\partial H}{\\\\partial x}(\\\\underbar{x},\\\\underbar{y})$ for some function H called the Hamiltonian\\n- we can only have centers (minima in the potential) and saddle points (maxima)\\n- separatrix - orbit that separates trapped and passing orbits\\n- Poincare Benderson Thm - can't have chaos in a 2D system\\n\\n## Ref\\n- $\\\\frac{\\\\partial}{\\\\partial x}(f_1 * f_2 * f_3) = \\\\frac{\\\\partial f_1}{\\\\partial x} f_2 f_3 + \\\\frac{\\\\partial f_2}{\\\\partial x} f_1 f_3 + \\\\frac{\\\\partial f_3}{\\\\partial x} f_1 f_2$\\n- $e^{\\\\mu it} = cos(\\\\mu t)+ isin(\\\\mu t)$\\n- $x = A e^{(\\\\lambda + i)t} + B e^{(\\\\lambda - i)t} \\\\implies x = (A' sin(t) + B' cos(t)) e^{\\\\lambda t} $\\nIf we have $\\\\dot{x_1},\\\\dot{x_2}$ then we can get $x_2(x_1) with \\\\frac{dx_1}{dx_2} = \\\\frac{\\\\dot{x_1}}{\\\\dot{x_2}}$\\n\\n\\n\\n## Benard Convection\\n\\n- The Navier-Stokes Equations\\n\\t1. Position\\n\\t2. Differential element in configuration space\\n\\t3. Time\\n\\n## Lorenz equations\\n- Lorenz was studying hurricanes \\n- ODEs for 3 coefficients in larger system (others are pretty small\\n\\t- use stream functions\\n- Navier-Stokes PDEs - conservation of momentum - derive these equations as project?\\n\\t- no-flow steady-state $T(y) = T_B + (T_T-T_B)y$\\n- Lorenz (ODE's): x,y,z are coefficients in Fourier expansion, not dimensions\\n$$\\\\dot{x}(t) = -\\\\sigma x+ \\\\sigma y$$\\n$$\\\\dot{y}(t) = (\\\\rho_o)x - y - xz$$\\n$$\\\\dot{z}(t) = -bz + xy$$\\n- $p_o$ - proportional to size of system\\n- b - aspect ration - related to height:width\\n- Navier-Stokes\\n\\t- independent variables\\n\\t\\t1. Position (vector)\\n\\t\\t2. Time (scalar)\\n\\t- dependent variables\\n\\t\\t1. Density (scalar)\\n- fixed points\\n\\t- $b>0, \\\\rho_0 > 0, \\\\sigma>0$ - let b,$\\\\sigma$ be fixed\\n\\t- fixed point $(\\\\bar{x},\\\\bar{y},\\\\bar{z})$ is constant\\n\\t- equations\\n\\t\\t- $\\\\dot{\\\\bar{x}}(t) = 0 = -\\\\sigma \\\\bar{x}+ \\\\sigma \\\\bar{y}$\\n\\t\\t- $\\\\dot{\\\\bar{y}}(t) = 0 = (\\\\rho_o) \\\\bar{x} - \\\\bar{y} - \\\\bar{x}\\\\bar{z}$\\n\\t\\t- $\\\\dot{z}(t) = 0 = -b\\\\bar{z} + \\\\bar{x}\\\\bar{y}$\\n\\t- doing some algebra we get\\n\\t\\t- $FP_0, \\\\forall \\\\rho_o$\\n\\t\\t\\t- $\\\\bar{x} = 0$\\n\\t\\t\\t- $\\\\bar{y} = 0$\\n\\t\\t\\t- $\\\\bar{z} = 0$\\n\\t\\t- $FP_+, \\\\rho_o \\\\geq 1$\\n\\t\\t\\t- $\\\\bar{x} = +\\\\sqrt{b (\\\\rho_o-1)} $\\n\\t\\t\\t- $\\\\bar{y} = +\\\\sqrt{b (\\\\rho_o-1)}$\\n\\t\\t\\t- $\\\\bar{z} = \\\\rho_o-1$\\n\\t\\t- $FP_-, \\\\rho_o \\\\geq 1$\\n\\t\\t\\t- $\\\\bar{x} = -\\\\sqrt{b (\\\\rho_o-1)} $\\n\\t\\t\\t- $\\\\bar{y} = -\\\\sqrt{b (\\\\rho_o-1)}$\\n\\t\\t\\t- $\\\\bar{z} = \\\\rho_o-1$\\n- these equations have symmetry\\n\\t- replace $(x(t),y(t),z(t))$ by $(-x(t),-y(t)),z(t)) \\\\to$ we get back the original equations\\n\\t\\n\\n$\\nM=\\n  \\\\begin{bmatrix}\\n    1 & 2 & 3 & 4 & 5 \\\\newline\\n    3 & 4 & 5 & 6 & 7\\n  \\\\end{bmatrix}\\n$\\n\\n## Linearization of Lorenz Equations about FPs $\\\\bar{\\\\underline{x}}$\\n- $\\\\frac{d}{dt}\\\\delta x(t) = \\\\frac{\\\\partial f}{\\\\partial x}|_ \\\\bar{\\\\underline{x}}= -\\\\sigma$ \\n- $\\\\frac{d}{dt}\\\\delta y(t) = \\\\frac{\\\\partial f_2}{\\\\partial x}|_ \\\\bar{\\\\underline{x}}*\\\\delta x(t) + \\\\frac{\\\\partial f_2}{\\\\partial y}|_ \\\\bar{\\\\underline{x}}*\\\\delta y(t) + \\\\frac{\\\\partial f_2}{\\\\partial z}|_ \\\\bar{\\\\underline{x}}*\\\\delta z(t) +...$ (higher order derivatives)\\n\\t- $ = (r-\\\\bar{z})*\\\\delta x(t) + -1*\\\\delta y(t) + -\\\\bar{x}*\\\\delta z(t) + \\\\delta x \\\\delta z$\\n- do the same thing for $\\\\delta z(t)$\",\n",
       " \"---\\nlayout: notes\\ntitle: Math Basics\\ncategory: math\\n---\\n\\n#  math basics\\n\\n## misc\\n\\n- $\\\\left( \\\\frac{n}{k} \\\\right) < \\\\left( \\\\frac{ne}{k} \\\\right)^k$\\n- Stirling's formula: $ n! ~= (\\\\frac{n}{e})^n $\\n  - corollary: log(n!) = 0(n log n)\\n  - gives us a bound on sorting\\n  - $\\\\left( \\\\frac{n}{e} \\\\right)^n < n!$\\n- $(1-x)^N \\\\leq e^{-Nx}$\\n- Poisson pmf approximates binomial when N large, p small\\n\\n## functions\\n\\n- *Gamma*: $\\\\Gamma(n)=(n-1)!=\\\\int_0^\\\\infty x^{n-1}e^{-x}dx$\\n- *Zeta*: $\\\\zeta(x) = \\\\sum_1^\\\\infty \\\\frac{1}{x^2} $\\n- Sigmoid (logistic): $f(x) = \\\\frac{1}{1+e^{-x}} = \\\\frac{e^x}{e^x+1}$\\n- Softmax: $f(x) = \\\\frac{e^{x_i}}{\\\\sum_i e^{x_i}}$\\n- spline: piecewise polynomial\\n\\n## stochastic processes\\n\\n- Stochastic - random process evolving with time\\n- Markov: $P(X_t=x\\\\|X_{t-1})=P(X_t=x\\\\|X_{t-1}...X_1)$\\n- Martingale: $E[X_t]=X_{t-1}$ \\n\\n## abstract algebra\\n\\n- Group: set of elements endowed with operation satisfying 4 properties:\\n\\n1. closed 2. identity 3. associative 4. inverses\\n\\n- Equivalence Relation;\\n\\n1. reflexive 2. transitive 3. symmetric\\n\\n## discrete math\\n- Goldbach's strong conjecture: Every even integer greater than 2 can be expressed as the sum of two primes (He considered one a prime).\\n- Goldbach's weak conjecture: All odd numbers greater than 7 are the sum of three primes.\\n- Set - An unordered collection of items without replication\\n- Proper subset - subset with cardinality less than the set\\n  - A U A = A\\t\\t\\tIdempotent law\\n- Disjoint: A and B = empty set\\n- Partition: mutually disjoint, union fills space\\n- powerset $\\\\mathcal{P}$(A) = set of all subsets\\n- Converse: $q\\\\to p$ (same as inverse: $-p \\\\to -q$)\\n- $p_1 \\\\to p_2 \\\\iff - p_1 \\\\lor p_2 $\\n- The greatest common divisor of two integers a and b is the largest integer d such that d $\\\\|$ a and d $\\\\|$ b\\n- Proof Techniques\\n    - Proof by Induction\\n    - Direct Proof\\n    - Proof by Contradiction - assume p $\\\\land$ -q, show contradiction\\n    - Proof by Contrapositive - show -q $\\\\to$ -p\\n\\n## identities\\n\\n- $e^{-2lnx}= \\\\frac{1}{e^{2lnx}} = \\\\frac{1}{e^{lnx}e^{lnx}} = \\\\frac{1}{x^2}$\\n- $\\\\ln(xy) = \\\\ln(x)+\\\\ln(y)$\\n- $\\\\ln x * \\\\ln y = \\\\ln(x^{\\\\ln y})$\\n  - difference between log 10n and log 2n is always a constant (about 3.322)\\n- $\\\\log_b (x) = \\\\log_d (x) / \\\\log_d (b)$\\n- partial fractions: $\\\\frac{3x+11}{(x-3)(x+2)} = \\\\frac{A}{x-3} + \\\\frac{B}{x+2}$\\n- $(ax+b)^k = \\\\frac{A_1}{ax+b}+\\\\frac{A_2}{(ax+b)^2}+...$\\n- $(ax^2+bx+c)^k = \\\\frac{A_1x+B_1}{ax^2+bx+c}+...$\\n- $\\\\cos(a\\\\pm b) = \\\\cos(a)\\\\cos(b)\\\\mp \\\\sin(a)\\\\sin(b)$\\n- $\\\\sin(a \\\\pm b) = \\\\sin(a)\\\\cos(b) \\\\pm \\\\sin(b)\\\\cos(a)$\\n\\n\\n\\n## imaginary numbers\\n\\n- complex conjugate of z=x+iy is  $z^*$ = x - iy\\n- Euler's formula $e^{i \\\\theta} = \\\\cos (\\\\theta) + i \\\\sin (\\\\theta)$\\n- sometimes we write imaginary numbers in polar form: $z = |z| e^{i \\\\theta}$\\n  - makes multiplication / division simpler\\n- absolute value / modules of imaginary numbers: $|a + ib| = \\\\sqrt{a^2 + b^2}$\\n\\n\\n\\n## spaces\\n\\n- hilbert space - requires an inner product (useful in analyzing kernels) - more general than an inner product space\\n  - reproducing kernel hilbert space with extra property\",\n",
       " '---\\nlayout: notes\\ntitle: graphical models\\ncategory: stat\\ntypora-copy-images-to: ../assets\\n---\\n\\n#  graphical models\\n\\n\\n*Material from Russell and Norvig \"Artifical Intelligence\" 3rd Edition* and *Jordan \"Graphical Models\"*\\n\\n## overview\\n\\n- network types\\n  1. *bayesian networks* - directed\\n  2. undirected models\\n- latent variable types\\n  1. *mixture models* - discrete latent variable\\n  2. *factor analysis models* - continuous latent variable\\n- graph representation: missing edges specify independence (converse is not true)\\n  - encode conditional independence relationships\\n    - helpful for inference\\n  - compact representation of joint prob. distr. over the variables\\n\\n![](../assets/models.png)\\n\\n- dark is observed for HMMs, for other things unclear what it means\\n\\n## bayesian networks - R & N 14.1-5 + J 2\\n\\n- examples\\n   1. *causal model*: causes $\\\\to$ symptoms\\n   2. *diagnostic model*: symptoms $\\\\to$ causes\\n      - generally requires more dependencies\\n- learning\\n   1. expert-designed\\n   2. data-driven\\n- properties\\n  - each node is random variable\\n  - weights as tables of conditional probabilities for all possibilities\\n  - represented by directed acyclic graph\\n- joint distr: $P(X_1 = x_1,...X_n=x_n)=\\\\prod_{i=1}^n P[X_i = x_i \\\\vert  Parents(X_i)]$\\n  - *markov condition*: given parents, node is conditionally independent of its non-descendants\\n    - marginally, they can still be dependent (e.g. explaining away)\\n  - given its *markov blanket* (parents, children, and children\\'s parents), a node is independent of all other nodes\\n- BN has no redundancy $\\\\implies$ no chance for inconsistency\\n\\n  - forming a BN: keep adding nodes, and only previous nodes are allowed to be parents of new nodes\\n\\n### hybrid BN (both continuous & discrete vars)\\n\\n- for continuous variables can sometimes discretize\\n1. *linear Gaussian* - for continuous children\\n  - parents all discrete $\\\\implies$ *conditional Gaussian* - multivariate Gaussian given assignment to discrete variables\\n  - parents all continuous $\\\\implies$ *multivariate Gaussian* over all the variables, and a multivariate posterior distribution (given any evidence)\\n  - parents some discrete, some continuous\\n    - h is continuous, s is discrete; a, b, $\\\\sigma$ all change when s changes\\n    - $P(c|h,s) = N(a \\\\cdot h + b, \\\\sigma^2)$, so mean is linear function of h\\n2. discrete children (continuous parents)\\n  1. *probit distr* - $P(buys|Cost=c) = \\\\phi[(\\\\mu - c)/\\\\sigma]$ - integral of standard normal distr\\n    - like a soft threshold\\n  2. *logit distr.* - $P(buys|Cost=c) =s\\\\left(\\\\frac{-2 (\\\\mu - c)}\\\\sigma \\\\right)$\\n    - logistic function (sigmoid s) produces thresh\\n\\n### exact inference\\n\\n- given assignment to *evidence variables* E, find probs of *query variables* X\\n  - other variables are *hidden variables* H\\n1. *enumeration* - just try summing over all hidden variables\\n  - $P(X|e) = \\\\alpha P(X, e) = \\\\alpha \\\\sum_h P(X, e, h)$\\n    - $\\\\alpha$ can be calculated as $1 / \\\\sum_x P(x, e)$\\n  - $O(n \\\\cdot 2^n)$\\n    - one summation for each of *n* variables\\n  - ENUMERATION-ASK evaluates in depth-first order: $O(2^n)$\\n    - we removed the factor of *n*\\n2. *variable elimination* - dynamic programming **(see elimination)**\\n  - ![Screen Shot 2018-07-26 at 8.52.30 AM](../assets/bayesian_net_example.png)\\n  - $P(B|j, m) = \\\\alpha \\\\underbrace{P(B)}_{f_1(B)} \\\\sum_e \\\\underbrace{P(e)}_{f_2(E)} \\\\sum_a \\\\underbrace{P(a|B,e)}_{f_3(A, B, E)} \\\\underbrace{P(j|a)}_{f_4(A)} \\\\underbrace{P(m|a)}_{f_5(A)}$\\n    - calculate factors in reverse order (bottom-up)\\n    - each factor is a vector with num entries = $\\\\prod$ |num_elements| * |num_values| \\n    - when we multiply them, pointwise products\\n  - ordering\\n    - any ordering works, some are more efficient\\n    - every variable that is not an ancestor of a query variable or evidence variable is irrelevant to the query\\n    - complexity depends on largest factor formed\\n3. *clustering algorithms* = *join tree* algorithms **(see propagation factor graphs)**\\n  - join individual nodes in such a way that resulting network is a polytree\\n    - ![Screen Shot 2018-07-26 at 8.52.30 AM-2621781](../assets/bayesian_net_example2.png)\\n    - *polytree*=*singly-connected network* - only 1 undirected paths between any 2 nodes\\n      - time and space complexity of exact inference is linear in the size of the network\\n      - holds even if the number of parents of each node is bounded by a constant\\n  - can compute posterior probabilities in $O(n)$\\n    - however, conditional probability tables may still be exponentially large\\n\\n### approximate inferences in BNs\\n\\n- randomized sampling algorithms = *monte carlo* algorithms\\n1. *direct sampling* methods:\\n  - *simplest* - sample network in topological order\\n  - *rejection sampling* - sample in order and stop once evidence is violated\\n    - want P(D|A)\\n    - sample N times, throw out samples where A is false\\n    - return probability of D being true\\n    - this is slow\\n  - *likelihood weighting* - fix evidence to be more efficient\\n    - generating a sample\\n      - fix our evidence variables to their observed values, then simulate the network\\n      - can\\'t just fix variables - distr. might be inconsistent\\n      - calculate *W* = prob of sample being generated\\n        - when we get to an evidence variable, multiply by prob it appears given its parents\\n    - for each observation\\n      - if positive, Count = Count + *W*\\n      - Total = Total + *W*\\n    - return Count/Total\\n    - this way we don\\'t have to throw out wrong samples\\n    - doesn\\'t solve all problems - evidence only influences the choice of downstream variables\\n\\n2. *Markov chain monte carlo* - ex. *Gibbs sampling*, *Metropolis-Hastings*\\n  - fix evidence variables\\n  - sample a nonevidence variable $X_i$ conditioned on the current values of its Markov blanket\\n  - repeatedly resample one-at-a-time in arbitrary order\\n  - why it works\\n    - the sampling process settles into a dynamic equilibrium where time spent in each state is proportional to its posterior probability\\n    - provided transition matrix q is *ergodic* - every state is reachable and there are no periodic cycles - only 1 steady-state soln\\n  - 2 steps\\n    1. create markov chain with write stationary distr.\\n    2. draw samples by simulating the chain\\n  - methods\\n    - 0th order methods - query density\\n      - metropolized random walk\\n      - ball walk\\n      - hit-and-run algorithm\\n    - 1st order methods - uses gradient of the density\\n      - Gibbs: we have conditionals\\n      - metropolis adjusted langevin algorithm (MALA) = langevin monte carlo\\n        - use gradient to propose new states\\n        - accept / reject using metropolis-hastings algorithm\\n      - unadjusted langevin algorithm (ULA)\\n      - hamiltonian monte carlo (neal, 2011)\\n  - log-concave distr. density (analog of convexity)\\n    - $\\\\pi(x) = \\\\frac{e^{-f(x)}}{\\\\int e^{-f(y)}dy}$\\n    - examples: normal distr., exponential distr., Laplace distr.\\n\\n3. *variational inference* - formulate inference as optimization\\n   - [good intro paper](https://arxiv.org/abs/1601.00670)\\n   - minimize KL-divergence between observed samples and assumed distribution\\n     - the actual KL is hard to minimize so instead we maximize the ELBO, which is equivalent\\n   - do this over a class of possible distrs.\\n   - variational inference tends to be faster, but may not be as good as MCMC\\n\\n### conditional independence properties\\n\\n- multiple, competing explanations (\"explaining-away\")\\n\\n  ![](../assets/j2_1.png) \\n\\n  -  in fact any descendant of the base of the v suffices for explaining away\\n\\n- *d-separation* = directed separation\\n\\n- *Bayes ball algorithm* - is $( X_A \\\\perp X_B )| X_C$?\\n\\n  - initialize\\n    - shade $X_C$\\n    - place ball at each of $X_A$\\n    - if any ball reaches $X_B$, then not conditionally independent\\n  - rules\\n    - balls can\\'t pass through shaded unless shaded is at base of v\\n    - balls pass through unshaded unless unshaded is at base of v\\n\\n- ![Screen Shot 2018-09-16 at 7.12.22 PM](../assets/triples.png)\\n\\n## undirected\\n\\n- $X_A \\\\perp X_C | X_B$ if the set of nodes $X_B$ separates the nodes $X_A$ from $X_C$\\n\\n  ![Screen Shot 2018-07-24 at 11.16.29 PM](../assets/graph_separation.png)\\n\\n- can\\'t convert directed / undirected\\n\\n![Screen Shot 2018-07-24 at 11.17.57 PM](../assets/full_graph_vs_missing.png)\\n\\n- factor over *maximal cliques* (largest sets of fully connected nodes)\\n- potential function $\\\\psi_{X_C} (x_C)$ function on possible realizations $x_C$ of the maximal clique $X_C$\\n  - non-negative, but not a probability (specifying conditional probs. doesn\\'t work)\\n  - commonly let these be exponential: $\\\\psi_{X_C} (x_C) = \\\\exp(-f_C(x_C))$\\n    - yields energy $f(x) = \\\\sum_C f_C(x_C)$\\n    - yields *Boltzmann distribution*: $p(x) = \\\\frac{1}{Z} \\\\exp (-f(x))$\\n- $p(x) = \\\\frac{1}{Z} \\\\prod_{C \\\\in Cliques} \\\\psi_{X_C}(x_c)$\\n  - $Z = \\\\sum_x \\\\prod_{C \\\\in Cliques} \\\\psi_{X_C} (x_C)$\\n- *reduced parameterizations* - impose constraints on probability distributions (e.g. Gaussian)\\n- if x is dependent on all its neighbors\\n  - *Ising model* - if x is binary\\n  - *Potts model* - x is multiclass\\n\\n## elimination - J 3\\n\\n- the elimination algorithm is for *probabilistic inference*\\n  - want $p(x_F|x_E)$ where E and F are disjoint\\n  - any var that is not ancestor of evidence or ancestor of query is irrelevant\\n- here, let $X_F$ be a single node\\n- notation\\n  - define $m_i (x_{S_i})$ = $\\\\sum_{x_i}$ where $x_{S_i}$ are the variables, other than $x_i$, that appear in the summand\\n  - define *evidence potential* $\\\\delta(x_i, \\\\bar{x_i})$ is defined as $x_i == \\\\bar{x_i}$\\n    - then $$g(\\\\bar{x_i}) = \\\\sum_{x_i} \\\\delta (x_i, \\\\bar{x_i})$$\\n    - for a set $\\\\delta (x_E, \\\\bar{x_E}) = \\\\prod_{i \\\\in E} \\\\delta (x_i, \\\\bar{x_i})$\\n    - lets us define $p(x, \\\\bar{x}_E) = p^E(x) = p(x) \\\\delta (x_E, \\\\bar{x_E})$\\n  - undirected graphs\\n    - $\\\\psi_i^E(x_i) \\\\triangleq \\\\psi_i(x_i) \\\\delta(x_i, \\\\bar{x}_i)$\\n    - this lets us write $p^E (x) = \\\\frac{1}{Z} \\\\prod_{c\\\\in C} \\\\psi^E_{X_c} (x_c)$\\n      - can ignore z since this is unnormalized anyway\\n      - to find conditional probability, divide by all sum of $p^E(x)$ for all values of E\\n    - in actuality don\\'t compute the product, just take the correct slice\\n- eliminate algorithm\\n  1. initialize: choose an ordering with query last\\n  2. evidence: set evidence vars to their values\\n  3. update: loop over element $x_i$ in ordering\\n     1. let $\\\\phi_i(x_{T_i})$ be product of all potentials involving $x_i$\\n     2. sum over the product of these potentials $m_i(x_{S_i}) = \\\\sum_x \\\\phi_i(x_{T_i})$\\n  4. normalize: $p(x_F|\\\\bar{x}_E) = \\\\phi_F(x_F) / \\\\sum_{x_F} \\\\phi_F (x_F)$\\n- undirected graph elimination algorithm\\n  - for directed graph, first *moralize*\\n    - for each node connect its parents\\n    - drop edges orientation\\n  - for each node X\\n    - connect all remaining neighbors of X\\n    - remove X from graph\\n- *reconstituted graph* - same nodes, includes all edges that were added\\n  - *elimination cliques* - includes X and its neighbors when X is removed\\n  - computational complexity is the exponential in the number of variables in the elimination clique\\n  - involves *treewidth* - one less than smallest achievable value of cardinality of largest elimination clique\\n    - range over all possible elimination orderings\\n    - NP-hard to find elimination ordering that achieves the treewidth\\n\\n## propagation factor graphs - J 4\\n\\n- *tree* - undirected graph in which there is exactly one path between any pair of nodes\\n   - if directed, then moralized graph should be a tree\\n   - *polytree* - directed graph that reduces to an undirected tree if we convert each directed edge to an undirected edge\\n   - ![Screen Shot 2018-07-31 at 11.44.52 AM](../assets/polytree.png)\\n   - $$p(x) = \\\\frac{1}{Z} \\\\left[ \\\\prod_{i \\\\in V} \\\\psi (x_i) \\\\prod_{(i,j)\\\\in E} \\\\psi (x_i,x_j) \\\\right]$$\\n      - for directed, root has individual prob and others are conditionals\\n    - can once again use evidence potentials for conditioning\\n\\n### probabilistic inference on trees\\n\\n- eliminate algorithm through message-passing\\n   - ordering I should be **depth-first traversal** of tree with f as root and all edges pointing away\\n      - *message* $m_{ji}(x_i)$ from $j$ to $i$ =*intermediate factor*\\n\\n- 2 key equations\\n\\n    - $m_{ji}(x_i) = \\\\sum_{x_j} \\\\left( \\\\psi^E (x_j) \\\\psi (x_i, x_j) \\\\prod_{k \\\\in N(j) \\\\backslash i} m_{kj} (x_j) \\\\right)$\\n    - $p(x_f | \\\\bar{x}_E) \\\\propto \\\\psi^E (x_f) \\\\prod_{e \\\\in N(f)} m_{ef} (x_f) $\\n       - ![Screen Shot 2018-07-31 at 9.55.08 PM](../assets/undirected_message_passing.png)\\n\\n- **sum-product** = **belief propagation** - inference algorithm\\n  - computes all single-node marginals (for certain classes of graphs) rather than only a single marginal\\n\\n  - only works in trees or tree-like graphs\\n\\n  - ![Screen Shot 2018-07-31 at 10.07.15 PM](../assets/message_passing_misc.png)\\n\\n    ![Screen Shot 2018-07-31 at 10.07.40 PM](../assets/message_passing_parallel.png)\\n\\n  - *message-passing protocol* - a node can send a message to a neighboring node when, and only when, it has received messages from all of its other neighbors (parallel algorithm)\\n    1. evidence(E)\\n    2. choose root\\n    3. collect: send messages evidence to root\\n    4. distribute: send messages root back out\\n\\n    ![Screen Shot 2018-07-31 at 10.22.55 PM](../assets/message_passing_individual.png) ![Screen Shot 2018-07-31 at 10.23.01 PM](../assets/message_passing_distribute.png)\\n\\n\\n### factor graphs\\n\\n- *factor graphs* capture factorizations, not conditional independence statements \\n   - ex $\\\\psi (x_1, x_2, x_3) = f_a(x_1,x_2) f_b(x_2,x_3) f_c (x_1,x_3)$ factors but has no conditional independence\\n       - ![Screen Shot 2018-07-31 at 11.30.19 PM](../assets/factor_graph.png)\\n    - $$f(x_1,...,x_n) = \\\\prod_s f_s (x_{C_s})$$\\n    - neighborhood N(s) for a factor index s is all the variables the factor references\\n    - neighborhood N(i) for a node i is set of factors that reference $x_i$\\n    - provide more fine-grained representation of prob. distr.\\n       - could add more nodes to normal graphical model to do this\\n - *factor tree* - if factors are made nodes, resulting undirected graph is tree\\n    - two kinds of messages (variable-> factor & factor-> variable)\\n    - run all the factor $\\\\to$ variables first\\n     - ![](../assets/j4_2.png)\\n     - $$p(x_i) \\\\propto \\\\prod_{s \\\\in N(i)} \\\\mu_{si} (x_i)$$\\n     - if a graph is originally a tree, there is little to be gained from factor graph framework\\n        - sometimes factor graph is factor tree, but original graph is not\\n\\n### maximum a posteriori (MAP)\\n\\n- want $\\\\max_{x_F} p(x_F | \\\\bar{x}_E)$ \\t\\n- MAP-eliminate algorithm is very similar to before\\n  - initialize - choose ordering\\n  - evidence - set evidence\\n  - update - for each take max over variable and make new factor\\n  - maximum - marginalize\\n\\n - products of probs tend to underflow, so take $\\\\max_x \\\\log p^E (x)$\\n - can also derive a *max-product algorithm* for trees\\n\\n 1. find $\\\\text{argmax}_x p^E (x)$\\n    - can solve by keeping track of maximizing values of variables in max-product algorithm\\n\\n## dynamic bayesian nets\\n\\n- *dynamic bayesian nets* - represents a temporal prob. model\\n\\n### state space model\\n\\n- state space model ![](../assets/j15_1.png)\\n\\n\\n- $P(X_{0:t}, E_{1:t}) = P(X_0) \\\\prod_{i} \\\\underbrace{P(X_i | X_{i-1}) }_{\\\\text{transition model}}  \\\\: \\\\underbrace{P(E_i|X_i)}_{\\\\text{sensor model}}$\\n\\n  - agent maintains *belief state* of state variables $X_t$ given evidence variables $E_t$\\n\\n  - improve accuracy\\n    1. increase order of Markov transition model\\n    2. increase set of state variables (can be equivalent to 1)\\n    - hard to maintain state variables over time, want more sensors\\n\\n- 4 inference problems (here $\\\\cdot$ is elementwise multiplication)\\n\\n  1. *filtering* = *state estimation* - compute $P(X_t | e_{1:t})$\\n    - *recursive estimation*:  $$\\\\underbrace{P(X_{t+1}|e_{1:t+1})}_{\\\\text{new state}} = \\\\alpha \\\\: \\\\underbrace{P(e_{t+1}|X_{t+1})}_{\\\\text{sensor}} \\\\cdot \\\\underset{x_t}{\\\\sum} \\\\: \\\\underbrace{P(X_{t+1}|x_t)}_{\\\\text{transition}} \\\\cdot \\\\underbrace{P(x_t|e_{1:t})}_{\\\\text{old state}}$$ where $\\\\alpha$ normalizes probs\\n\\n  2. *prediction* - compute $P(X_{t+k}|e_{1:t})$ for $k>0$\\n- $\\\\underbrace{P(X_{t+k+1} |e_{1:t})}_{\\\\text{new state}} = \\\\sum_{x_{t+k}} \\\\underbrace{P(X_{t+k+1} |x_{t+k})}_{\\\\text{transition}}  \\\\cdot \\\\underbrace{P(x_{t+k} |e_{1:t})}_{\\\\text{old state}}$\\n  \\n3. *smoothing* - compute $P(X_{k}|e_{1:t})$ for $0 < k < t$\\n     1. 2 components $P(X_k|e_{1:t}) = \\\\alpha \\\\underbrace{P(X_k|e_{1:k})}_{\\\\text{forward}} \\\\cdot \\\\underbrace{P(e_{k+1:t}|X_k)}_{\\\\text{backward}}$\\n     \\n     1. forward pass: filtering from $1:t$\\n       2. backward pass from $t:1$ $\\\\underbrace{P(e_{k+1:t}|X_k)}_{\\\\text{sensor past k}} = \\\\sum_{x_{k+1}} \\\\underbrace{P(e_{k+1}|x_{k+1})}_{\\\\text{sensor}} \\\\cdot \\\\underbrace{P(e_{k+2:t}|x_{k+1})}_{\\\\text{recursive call}} \\\\cdot \\\\underbrace{P(x_{k+1}|X_k)}_{\\\\text{transition}}$\\n       3. this is called the forward-backward algo(also there is a separate algorithm that doesn\\'t use the observations on the backward pass)\\n  \\n4. *most likely explanation* - $\\\\underset{x_{1:t}}{\\\\text{argmax}}\\\\:P(x_{1:t}|e_{1:t})$\\n  \\n   1. *Viterbi algorithm*: $\\\\underbrace{\\\\underset{x_{1:t}}{\\\\text{max}} \\\\: P(x_{1:t}, X_{t+1}|e_{1:t+1})}_{\\\\text{mle x}} = \\\\alpha \\\\: \\\\underbrace{P(e_{t+1}|X_{t+1})}_{\\\\text{sensor}} \\\\cdot \\\\underset{x_t}{\\\\text{max}} \\\\left[ \\\\: \\\\underbrace{P(X_{t+1}|x_t)}_{\\\\text{transition}} \\\\cdot \\\\underbrace{\\\\underset{x_{1:t-1}}{\\\\text{max}} \\\\:P(x_{1:t-1}, x_{t+1}|e_{1:t})}_{\\\\text{max prev state}} \\\\right]$\\n     2. complexity\\n        - K = number of states\\n        - M = number of observations\\n        - n = length of sequence\\n        - memory - $nK$\\n        - runtime - $O(nK^2)$\\n\\n- *learning* - form of EM\\n\\n  - basically just count (maximizing joint likelihood of input and output)\\n  - initial state probs $\\\\frac{count(start \\\\to s)}{n}$\\n  - $P(x\\'|x) = \\\\frac{count(s \\\\to s\\')}{count(s)}$\\n  - $P(y|x) = \\\\frac{count (x \\\\to y)}{count(x)}$\\n\\n### hmm\\n\\n- **state is a single discrete process**\\n- transitions are all matrices (and no zeros in sensor model)$\\\\implies$ forward pass is invertible so can use constant space\\n- ***online smoothing (with lag)***\\n- ex. robot localization\\n\\n### kalman filtering\\n\\n- **state is continuous**\\n- ex. ![](../assets/r15_9.png)\\n- type of nodes (real-valued vectors) and prob model (linear-Gaussian) changes from HMM\\n- 1d example: *random walk*\\n- state nodes: $x_{t+1} = Ax_t + Gw_t$\\n\\n\\n- output nodes: $y_t = Cx_t+v_t$\\n  - x is linear Gaussian\\n  - w is noise Gaussian\\n  - y is linear Gaussian\\n- doing the integral for prediction involves completing the square\\n- properties\\n  1. new mean is weighted mean of new observation and old mean\\n  2. update rule for variance is independent of the observation\\n  3. variance converges quickly to fixed value that depends only on $\\\\sigma^2_x, \\\\sigma^2_z$\\n- **Lyapunov eqn**: evolution of variance of states\\n- **information filter** - mathematically the same but different parameterization\\n- *extended Kalman filter*\\n  - works on nonlinear systems\\n  - locally linear\\n- *switching Kalman filter* - multiple Kalman filters run in parallel and weighted sum of predictions is used \\n  - ex. one for straight flight, one for sharp left turns, one for sharp right turns\\n  - equivalent to adding discrete \"maneuver\" state variable\\n\\n### general dbns\\n\\n- can be better to decompose state variable into multiple vars\\n  - reduces size of transition matrix\\n- *transient failure model* - allows probability of sensor giving wrong value\\n- *persistent failure model* - additional variable describing status of battery meter\\n- exact inference - *våariable elimination* mimics recursive filtering\\n  - still difficult\\n- approximate inference - modification of likelihood weighting\\n  - use samples as approximate representation of current state distr.\\n  - ***particle filtering*** - focus set of samples on high-prob regions of the state space\\n    - consistent\\n    - sample a state\\n    - sample the next state given the previous state\\n    - weight each sample by $P(e_t | x_t)$\\n    - resample based on weight\\n\\n## structure learning\\n\\n- conditional correlation - inverse covariance matrix = precision matrix\\n    - estimates only good when $n >> p$\\n    - eigenvalues are not well-approximated\\n    - often enforce sparsity\\n    - ex. threshold each value in the cov matrix (set to 0 unless greater than thresh) - this threshold can depend on different things\\n    - can also use regularization to enforce sparsity\\n    - POET doesn\\'t assume sparsity',\n",
       " '---\\nlayout: notes\\ntitle: data analysis\\ncategory: stat\\n---\\n\\n#  data analysis\\n\\n## pqrs\\n\\n- Goal: *inference* - conclusion or opinion formed from evidence\\n- *PQRS*\\n  - P - population\\n  - Q - question - 2 types\\n    1. hypothesis driven - does a new drug work\\n    2. discovery driven - find a drug that works\\n  - R - representative data colleciton\\n    - simple random sampling = *SRS*\\n      - w/ replacement: $var(\\\\bar{X}) = \\\\sigma^2 / n$\\n      - w/out replacement: $var(\\\\bar{X}) = (1 - \\\\frac{n}{N}) \\\\sigma^2 / n$ \\n  - S - scrutinizing answers\\n\\n## visualization\\n\\nFirst 5 parts here are based on the book [storytelling with data](http://www.storytellingwithdata.com/) by cole nussbaumer knaflic\\n\\n- difference between showing data + storytelling with data\\n\\n### understand the context (1)\\n\\n- who is your audience? what do you need them to know/do?\\n- **exploratory** vs **explanatory** analysis\\n- slides (need little details) vs email (needs lots of detail) - usually need to make both in slideument\\n- should know how much nonsupporting data to show\\n- distill things down into a 3-minute story or a 1-sentence Big Idea\\n- easiest to start things on paper/post-it notes\\n\\n### choose an effective visual (2)\\n\\n| ![Screen Shot 2020-09-28 at 8.08.38 PM](../assets/good_plots.png) | ![Screen Shot 2020-09-28 at 8.08.30 PM](../assets/good_visualizations.png) |\\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\\n| ![Screen Shot 2020-09-28 at 8.23.47 PM](../assets/stacked_hbar_chart.png) | ![Screen Shot 2020-09-28 at 8.24.37 PM](../assets/grid_breakdown.png) |\\n| ![Screen Shot 2020-09-28 at 8.29.34 PM](../assets/avoiding_secondary_axis.png) | ![Screen Shot 2020-09-29 at 9.50.15 AM](../assets/strategic_contrast.png) |\\n| ![Screen Shot 2020-09-29 at 9.56.44 AM](../assets/preattentive_attributes.png) | ![Screen Shot 2020-09-29 at 11.20.07 AM](../assets/line_charts.png) |\\n\\n- generally avoid pie/donut charts, 3D charts, 2nd y-axes\\n- tables\\n  - best for when people will actually read off numbers\\n  - minimalist is best\\n- bar charts should basically always start at 0\\n  - horizontal charts typically easy to read\\n- on axes, retain things like dollar signs, percent, etc.\\n\\n### eliminate clutter (3)\\n\\n- gestalt principles of  vision\\n  - proximity - close things are grouped\\n  - similarity - similar things are grouped\\n  - connection - connected things are grouped\\n  - enclosure\\n  - closure\\n  - continuity\\n- generally good to have titles and such at top-left!\\n- diagonal lines / text should be avoided\\n  - center-aligned text should be avoided\\n- label lines directly\\n\\n### focus attention (4)\\n\\n- visual hierarchy - outlines what is important\\n\\n### tell a story / think like a designer (5)\\n\\n- affordances - aspects that make it obvious how something will be used (e.g. a button affords pushing)\\n- “You know you’ve achieved perfection, not when you have nothing more to add, but when you have nothing to take away” (Saint‐Exupery, 1943)\\n- stories have different parts, which include conflict + tension\\n  - beginning - introduce a problem / promise\\n  - middle - what could be\\n  - end - call to action\\n- horizontal logic - people can just read title slides and get out what they need\\n- can either convince ppl through conventional rhetoric or through a story\\n\\n### visual summaries\\n\\n- numerical summaries\\n  - mean vs. median\\n  - sd vs. iq range\\n- visual summaries\\n  - histogram\\n  - *kernel density plot* - Gaussian kernels\\n    - with *bandwidth* h $K_h(t) = 1/h K(t/h)$\\n- plots\\n  1. box plot / pie-chart\\n  2. scatter plot / q-q plot\\n    - *q-q plot* = *probability plot* - easily check normality\\n      - plot percentiles of a data set against percentiles of a theoretical distr.\\n      - should be straight line if they match\\n  3. transformations = feature engineering\\n    - log/sqrt make long-tail data more centered and more normal\\n    - **delta-method** - sets comparable bw (wrt variance) after log or sqrt transform: $Var(g(X)) \\\\approx [g\\'(\\\\mu_X)]^2 Var(X)$ where $\\\\mu_X = E(X)$\\n    - if assumptions don\\'t work, sometimes we can transform data so they work\\n    - *transform x* - if residuals generally normal and have constant variance \\n      - *corrects nonlinearity*\\n    - *transform y* - if relationship generally linear, but non-constant error variance\\n      - *stabilizes variance*\\n    - if both problems, try y first\\n    - Box-Cox: Y\\' = $Y^l \\\\: if \\\\: l \\\\neq 0$, else log(Y)\\n  4. *least squares*\\n    - inversion of pxp matrix ~O(p^3)\\n    - regression effect - things tend to the mean (ex. bball children are shorter)\\n    - in high dims, l2 worked best\\n  5. kernel smoothing + lowess\\n    - can find optimal bandwidth\\n    - *nadaraya-watson kernel smoother* - locally weighted scatter plot smoothing\\n      - $$g_h(x) = \\\\frac{\\\\sum K_h(x_i - x) y_i}{\\\\sum K_h (x_i - x)}$$ where h is bandwidth\\n    - *loess* - multiple predictors / *lowess* - only 1 predictor\\n      - also called *local polynomial smoother* - locally weighted polynomial\\n      - take a window (span) around a point and fit weighted least squares line to that point\\n      - replace the point with the prediction of the windowed line\\n      - can use local polynomial fits rather than local linear fits\\n  6. *silhouette plots* - good clusters members are close to each other and far from other clustersf\\n\\n     1. popular graphic method for K selection\\n     2. measure of separation between clusters $s(i) = \\\\frac{b(i) - a(i)}{max(a(i), b(i))}$\\n       1. a(i) - ave dissimilarity of data point i with other points within same cluster\\n       2. b(i) - lowest average dissimilarity of point i to any other cluster\\n     3. good values of k maximize the average silhouette score\\n  7. lack-of-fit test - based on repeated Y values at same X values\\n\\n## dealing with imbalanced data\\n\\n1. randomly oversample minority class\\n2. randomly undersample majority class\\n3. weighting classes in the loss function - more efficient, but requires modifying model code\\n4. generate synthetic minority class samples\\n   1. [smote](https://jair.org/index.php/jair/article/view/10302) (chawla et al. 2002) - interpolate betwen points and their nearest neighbors (for minority class) - some heuristics for picking which points to interpolate![smote](../assets/smote.png)\\n      1. [adasyn](https://ieeexplore.ieee.org/abstract/document/4633969/) (he et al. 2008) - smote, generate more synthetic data for minority examples which are harder to learn (number of samples is proportional to number of nearby samples in a different class)\\n   2. [smrt](https://github.com/tgsmith61591/smrt) - generate with vae\\n5. selectively removing majority class samples\\n   1. [tomek links](https://pdfs.semanticscholar.org/090a/6772a1d69f07bfe7e89f99934294a0dac1b9.pdf?_ga=2.141687734.587787484.1573518991-2102528433.1505064485) (tomek 1976) - selectively remove majority examples until al lminimally distanced nearest-neighbor pairs are of the same class\\n   2. [near-miss](https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf) (zhang & mani 2003) - select samples from the majority class which are close to the minority class. Example: select samples from the majority class for which the average distance of the N *closest* samples of a minority class is smallest\\n   3. [edited nearest neighbors](https://ieeexplore.ieee.org/abstract/document/4309137) (wilson 1972) - \"edit\" the dataset by removing samples that don\\'t agree \"enough\" with their neighborhood\\n6. feature selection and extraction\\n   1. minority class samples can be discarded as noise - removing irrelevant features can reduce this risk\\n   2. feature selection - select a subset of features and classify in this space\\n   3. feature extraction - extract new features and classify in this space\\n   4. ideas\\n      1. use majority class to find different low dimensions to investigate\\n      2. in this dim, do density estimation\\n      3. residuals - iteratively reweight these (like in boosting) to improve performance\\n7. incorporate sampling / class-weighting into ensemble method (e.g. treat different trees differently)\\n   1. ex. undersampling + ensemble learning (e.g. [IFME](https://dl.acm.org/citation.cfm?id=2467736), Becca\\'s work)\\n8. algorithmic classifier modifications\\n9. misc papers\\n   1. [enrichment](https://arxiv.org/pdf/1911.06965v1.pdf) (jegierski & saganowski 2019) - add samples from an external dataset\\n10. ref\\n   1. [imblanced-learn package](https://imbalanced-learn.readthedocs.io/en/stable/api.html) with several methods for dealing with imbalanced data\\n   2. [good blog post](https://www.jeremyjordan.me/imbalanced-data/)\\n   3. [Learning from class-imbalanced data: Review of methods and applications](https://www-sciencedirect-com.libproxy.berkeley.edu/science/article/pii/S0957417416307175) (Haixiang et al. 2017)\\n   4. sample majority class w/ density (to get best samples)\\n   5. log-spline - doesn\\'t scale\\n\\n\\n\\n## feature engineering\\n\\n- often good to discretize/binarize features\\n  - e.g. [from genomics](http://cis.jhu.edu/people/faculty/geman/publications/pdf/Digitizing_omics_profiles_by_divergence_from_a_baseline.pdf)\\n\\n## principles\\n\\n### breiman\\n\\n- conversation\\n  - moved sf -> la -> caltech (physics) -> columbia (math) -> berkeley (math)\\n  - info theory + gambling\\n  - CART, ace, and prob book, bagging\\n  - ucla prof., then consultant, then founded stat computing at berkeley\\n  - lots of cool outside activities\\n    - ex. selling ice in mexico\\n- 2 cultures paper\\n  1. *generative* - data are generated by a given stochastic model\\n    - stat does this too much and needs to move to 2\\n    - ex. assume y = f(x, noise, parameters)\\n    - validation: goodness-of-fit and residuals\\n  2. *predictive* - use algorithmic model and data mechanism unknown\\n    - assume nothing about x and y\\n    - ex. generate P(x, y) with neural net\\n    - validation: prediction accuracy\\n  - axioms\\n    1. Occam\\n    2. *Rashomon* - lots of different good models, which explains best?\\n      - ex. rf is not robust at all\\n    3. *Bellman* - curse of dimensionality\\n      - might actually want to increase dimensionality (ex. svms embedded in higher dimension)\\n  - industry was problem-solving, academia had too much culture\\n\\n### box + tukey\\n\\n- questions\\n  1. what points are relevant and irrelevant today in both papers? \\n    - relevant\\n      - box\\n        - thoughts on scientific method\\n        - solns should be simple\\n        - necessity for developing experimental design\\n        - flaws (cookbookery, mathematistry)\\n      - tukey\\n        - separating data analysis and stats\\n        - all models have flaws\\n        - no best models\\n        - lots of goold old techniques (e.g. LSR)\\n    - irrelevant\\n      - some of the data techniques (I think)\\n      - tukey multiple-response data has been better attacked (graphical models)\\n  2. how do you think the personal traits of Tukey and Box relate to the scientific opinions expressed in their papers?\\n    - probably both pretty critical of the science at the time\\n    - box - great respect for Fisher\\n    - both very curious in different fields of science\\n  3. what is the most valuable msg that you get from each paper?\\n    - box - data analysis is a science\\n    - tukey - models must be useful\\n      - no best models\\n      - find data that is useful\\n      - no best models\\n- box_79 \"science and statistics\"\\n  - scientific method - iteration between theory and practice\\n    - learning - discrepancy between theory and practice\\n    - solns should be simple\\n  - fisher - founder of statistics (early 1900s)\\n    - couples math with applications\\n    - data analysis - subiteration between tentative model and tentative analysis\\n    - develops experimental design\\n  - flaws\\n    - *cookbookery* - forcing all problems into 1 or 2 routine techniques\\n    - *mathematistry* - development of theory for theory\\'s sake\\n- tukey_62 \"the future of data analysis\"\\n  - general considerations\\n    - data analysis - different from statistics, is a science\\n    - lots of techniques are very old (LS - Gauss, 1803)\\n    - all models have flaws\\n    - no best models\\n    - must teach multiple data analysis methods\\n  - spotty data - lots of irregularly non-constant variability\\n    - could just trim highest and lowest values\\n      - *winzorizing* - replace suspect values with closest values that aren\\'t\\n    - must decide when to use new techniques, even when not fully understood\\n    - want some automation\\n    - FUNOP - fulll normal plot\\n      - can be visualized in table\\n  - spotty data in more complex situations\\n    \\n    - FUNOR-FUNOM\\n  - multiple-response data\\n    - understudied except for factor analysis\\n    - multiple-response procedures have been modeled upon how early single-response procedures were supposed to have been used, rather than upon how they were in fact used\\n    - factor analysis\\n      1. reduce dimensionality with new coordinates\\n      2. rotate to find meaningful coordinates\\n      \\n      - can use multiple regression factors as one factor if they are very correlated\\n    - regression techniques always offer hopes of learning more from less data than do variance-component techniques\\n  - flexibility of attack\\n    \\n    - ex. what unit to measure in\\n\\n### models\\n\\n- normative - fully interpretable + modelled\\n  - idealized\\n  - probablistic\\n- ~mechanistic - somewhere in between\\n- descriptive - based on reality\\n  - empirical\\n\\n### exaggerated claims\\n\\n- [video](https://www.youtube.com/watch?time_continue=4&v=PwCyvSDkUCY&feature=emb_logo) by Rob Kass\\n- concepts are ambiguous and have many mathematical instantiations\\n  - e.g. \"central tendency\" can be mean or median\\n  - e.g. \"information\" can be mutual info (reduction in entropy) or squared correlation (reduction in variance)\\n  - e.g. measuring socioeconomic status and controlling for it\\n- regression \"when controlling for another variable\" makes causal assumptions\\n  - must make sure that everything that could confound is controlled for\\n- Idan Segev: \"modeling is the lie that reveals the truth\"\\n  - picasso: \"art is the lie that reveals the truth\"\\n- box: \"all models are wrong but some are useful\" - statistical pragmatism\\n  - moves **from true to useful** - less emphasis on truth\\n  - \"truth\" is contingent on the purposes to which it will be put\\n- the scientific method aims to provide explanatory models (theories) by collecting and analyzing data, according to protocols, so that\\n  - the data provide info about models\\n  - replication is possible\\n  - the models become increasingly accurate\\n- scientific knowledge is always uncertain - depends on scientific method',\n",
       " \"---\\nlayout: notes\\ntitle: Testing\\ncategory: stat\\n---\\n\\n#  testing\\n\\n[wonderful summarizing blog post](https://lindeloev.github.io/tests-as-linear/)\\n\\n## basics\\n\\n- *data snooping* - decide which hypotheses to test after examining data\\n- null hypothesis $H_0$ vs alternative hypothesis $H_1$\\n- types\\n  - simple hypothesis $\\\\theta = \\\\theta_0$\\n  - composite hypothesis $\\\\theta > \\\\theta_0$ or $\\\\theta < \\\\theta_0$\\n  - two-sided test: $H_0: \\\\theta = \\\\theta_0 \\\\: vs. \\\\: H_1 \\\\theta \\\\neq \\\\theta_0$\\n  - one-sided test: $H_0: \\\\theta \\\\leq \\\\theta_0 \\\\: vs. \\\\: H_1: \\\\theta > \\\\theta_0$\\n- significance levels\\n  - *stat. significant*: p = 0.05\\n  - *highly stat. significant*: p = 0.01\\n- errors\\n  - $\\\\alpha$ - type 1 - reject $H_0$ but $H_0$ true\\n  - $\\\\beta$ - type 2 - fail to reject $H_0$ but $H_0$ false\\n- *p-value* = probability, calculated assuming that the null hypothesis is true, of obtaining a value of the test statistic at least as contradictory to $H_0$ as the value calculated from the available sample\\n- **power**: $1 - \\\\beta$\\n- adjustments\\n  - *bonferroni procedure* - we are doing 3 tests with 5% confidence, so we actually do 5/3% for each test in order to restrict everything to 5% total\\n  - *Benjamini–Hochberg procedure* - controls for false discovery rate\\n- note: ranking is often more important than actual FDR control (because we just need to know what experiments to do)\\n\\n## gaussian theory\\n\\n- normal theory: assume $\\\\epsilon_i$ ~ $N(0, \\\\sigma^2)$\\n- distributions\\n  - suppose $Z_1, ..., Z_n$ ~ iid N(0, 1)\\n  - **chi-squared**: $\\\\chi_d^2$ ~ $\\\\sum_i^d U_i^2$ w/ d degrees of freedom\\n    - $(d-1)S^2/\\\\sigma^2 \\\\text{ proportional to } \\\\chi_{d-1}^2$\\n  - *student's t*: $U_{d+1} / \\\\sqrt{d^{-1} \\\\sum_1^d U_i^2}$ w/ d degress of freedom\\n- **t-test**: test if mean is nonzero\\n  - test null $\\\\theta_k=0$ w/ $t = \\\\hat{\\\\theta}_k / \\\\hat{SE}$ where $SE = \\\\hat{\\\\sigma} \\\\cdot \\\\sqrt{\\\\Sigma_{kk}^{-1}}$\\n  - t-test: reject if \\\\|t\\\\| is large\\n  - when n-p is large, t-test is called the z-test\\n  - under null hypothesis t follows t-distr with n-p degrees of freedom\\n  - here, $\\\\hat{\\\\theta}$ has a normal distr. with mean $\\\\theta$ and cov matrix $\\\\sigma^2 (X^TX)^{-1}$\\n    - e independent of $\\\\hat{\\\\theta}$ and $\\\\|\\\\|e\\\\|\\\\|^2 ~ \\\\sigma^2 \\\\chi^2_d$ with d = n-p\\n  - *observed stat. significance level* = *P-value* - area of normal curve beyond $\\\\pm \\\\hat{\\\\theta_k} / \\\\hat{SE}$\\n  - if 2 vars are statistically significant, said to have *independent effects* on Y\\n- **f-test**: test if any of non-zero means\\n  - null hypothesis: $\\\\theta_i = 0,  i=p-p_0, ..., p$\\n  - alternative hypothesis: for at least one $ i \\\\in \\\\{p-p_0, ..., p\\\\}, \\\\: \\\\theta_i \\\\neq 0$\\n  - $F = \\\\frac{(\\\\|\\\\|X\\\\hat{\\\\theta}\\\\|\\\\|^2 - \\\\|\\\\|X\\\\hat{\\\\theta}^{(s)}\\\\|\\\\|^2) / p_0}{\\\\|\\\\|e\\\\|\\\\|^2 / (n-p)} $ where $\\\\hat{\\\\theta^{(s)}}$ has last $p_0$ entries 0\\n  - under null hypothesis, $\\\\|\\\\|X\\\\hat{\\\\theta}\\\\|\\\\|^2 - \\\\|\\\\|X\\\\hat{\\\\theta}^{(s)}\\\\|\\\\|^2$ ~ $U$, $\\\\|\\\\|e\\\\|\\\\|^2$ ~ $V$, $F$ ~ $\\\\frac{U/p_0}{V/(n-p)}$ where $ U \\\\: indep \\\\: V$, $U$ ~ $\\\\sigma^2 \\\\chi^2_{p_0}$, $V$ ~ $\\\\sigma^2 \\\\chi_{n-p}^2$\\n  - there is also a *partial f-test*\\n\\n## statistical intervals\\n\\n- interval estimates come with confidence levels\\n- $Z=\\\\frac{\\\\bar{X}-\\\\mu}{\\\\sigma / \\\\sqrt{n}}$\\n- For p not close to 0.5, use Wilson score confidence interval (has extra terms)\\n- **confidence interval** - if multiple samples of trained typists were selected and an interval constructed for each sample mean, 95 percent of these intervals contain the true preferred keyboard height\\n  - frequentist idea\\n\\n## tests on hypotheses\\n\\n- Var($\\\\bar{X}-\\\\bar{Y})=\\\\frac{\\\\sigma_1^2}{m}+\\\\frac{\\\\sigma_2^2}{n}$\\n- tail refers to the side we reject (e.g. upper-tailed=$H_a:\\\\theta>\\\\theta_0$\\n- we try to make the null hypothesis a statement of equality\\n- upper-tailed - reject large values\\n- $\\\\alpha$ is computed using the probability distribution of the test statistic when $H_0$ is true, whereas determination of b requires knowing the test statistic distribution when $H_0$ is false\\n- type 1 error usually more serious, pick $\\\\alpha$ level, then constrain $\\\\beta$\\n- can standardize values and test these instead\\n\\n## testing LR coefficients\\n\\n- confidence interval construction\\n  - confidence interval (CI) is range of values likely to include true value of a parameter of interest\\n  - confidence level (CL) - probability that the procedure used to determine CI will provide an interval that covers the value of the parameter -  if we remade it 100 times, 95 would contain the true $\\\\theta_1$\\n- $\\\\hat{\\\\beta_0} \\\\pm t_{n-2,\\\\alpha /2} * s.e.(\\\\hat{\\\\beta_0}) $\\n  - for $\\\\beta_1$\\n    - with known $\\\\sigma$\\n      - $\\\\frac{\\\\hat{\\\\beta_1}-\\\\beta_1}{\\\\sigma(\\\\hat{\\\\beta_1})} \\\\sim N(0,1)$\\n      - derive CI\\n    - with unknown $\\\\sigma$\\n      - $\\\\frac{\\\\hat{\\\\beta_1}-\\\\beta_1}{s(\\\\hat{\\\\beta_1})} \\\\sim t_{n-2}$\\n      - derive CI\\n\\n## ANOVA (analysis of variance)\\n\\n- y - called dependent, response variable\\n- x - independent, explanatory, predictor variable\\n- notation: $E(Y\\\\|x^*) = \\\\mu_{Y\\\\cdot x^*} = $ mean value of Y when x = $x^*$\\n- Y = f(x) + $\\\\epsilon$\\n- linear: $Y=\\\\beta_0+\\\\beta_1 x+\\\\epsilon$\\n- logistic: $odds = \\\\frac{p(x)}{1-p(x)}=e^{\\\\beta_0+\\\\beta_1 x+\\\\epsilon}$\\n- we minimize least squares: $SSE = \\\\sum_{i=1}^n (y_i-(b_0+b_1x_i))^2$\\n- $b_1=\\\\hat{\\\\beta_1}=\\\\frac{\\\\sum (x_i-\\\\bar{x})(y_i-\\\\bar{y})}{\\\\sum (x_i-\\\\bar{x})^2} = \\\\frac{S_{xy}}{S_{xx}}$\\n- $b_0=\\\\bar{y}-\\\\hat{\\\\beta_1}\\\\bar{x}$\\n- $S_{xy}=\\\\sum x_iy_i-\\\\frac{(\\\\sum x_i)(\\\\sum y_i)}{n}$\\n- $S_{xx}=\\\\sum x_i^2 - \\\\frac{(\\\\sum x_i)^2}{n}$\\n- residuals: $y_i-\\\\hat{y_i}$\\n- SSE = $\\\\sum y_i^2 - \\\\hat{\\\\beta}_0 \\\\sum y_i - \\\\hat{\\\\beta}_1 \\\\sum x_iy_i$\\n- SST  = total sum of squares = $S_{yy} = \\\\sum (y_i-\\\\bar{y})^2 = \\\\sum y_i^2 - (\\\\sum y_i)^2/n$\\n- $r^2 = 1-\\\\frac{SSE}{SST}=\\\\frac{SSR}{SST}$ - proportion of observed variation that can be explained by regression\\n- $\\\\hat{\\\\sigma}^2 = \\\\frac{SSE}{n-2}$\\n- $T=\\\\frac{\\\\hat{\\\\beta}_1-\\\\beta_1}{S / \\\\sqrt{S_{xx}}}$ has a t distr. with n-2 df\\n- $s_{\\\\hat{\\\\beta_1}}=\\\\frac{s}{\\\\sqrt{S_{xx}}}$\\n- $s_{\\\\hat{\\\\beta_0}+\\\\hat{\\\\beta_1}x^*} = s\\\\sqrt{\\\\frac{1}{n}+\\\\frac{(x^*-\\\\bar{x})^2}{S_{xx}}}$\\n- sample correlation coefficient $r = \\\\frac{S_{xy}}{\\\\sqrt{S_xx}\\\\sqrt{S_{yy}}}$\\n- this is a point estimate for population correlation coefficient = $\\\\frac{Cov(X,Y)}{\\\\sigma_X\\\\sigma_Y}$\\n- make fisher transformation - this test statistic also tests correlation\\n- degrees of freedom\\n- one-sample T = n-1\\n- T procedures with paired data - n-1\\n- T procedures for 2 independent populations - use formula ~= smaller of n1-1 and n2-1\\n- variance - n-2\\n- use z-test if you know the standard deviation---\",\n",
       " '---\\nlayout: notes\\ntitle: causal inference\\ncategory: stat\\n---\\n\\n#  causal inference\\n\\n*Some notes on causal inference both from introductory courses following neyman-rubin framework (+ the textbook \"[What if](https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2020/01/ci_hernanrobins_21jan20.pdf)?\") + based on Judea Pearl\\'s ladder of causality (+ \"The book of why?\"). Also includes notes from [this chapter](https://fairmlbook.org/causal.html) of the fairml book.*\\n\\n## basics\\n\\n- **confounding** = difference between groups other than the treatment which affects the response\\n  - this is the key problem when using observational (non-experimental) data to make causal inferences\\n  - problem occurs because we don\\'t get to see counterfactuals\\n  - ex from Pearl where Age is the confounder![Screen Shot 2019-04-07 at 7.01.55 PM](../assets/confounding_ex.png)\\n- study types\\n  - 3 principles of experimental design: replication, randomization, conditioning\\n  - **randomized control trial (RCT)** - controls for any possible confounders\\n  - **case-control study** - retrospective - compares \"cases\" (people with a disease) to controls\\n  - **sensitivity analysis** - instead of drawing conclusions by assuming the absence of certain causal relationships, challenge such assumptions and evaluate how strong altervnative relationships must be in order to explain the observed data\\n  - **regression-based adjustment** - if we know the confounders, can just regress on the confounders and the treatment and the coefficient for the treatment (the partial regression coefficient) will give us the average causal effect)\\n    - works only for linear models\\n    - **propensity score** - probability that a subject recieving a treatment is valid after conditioning on appropriate covariates\\n  - **instrumental variables** - variable which can be used to effectively due a RCT because it was made random by some external factor\\n    - ex. army draft, john snow\\'s cholera study\\n- background\\n  - very hard to decide what to include and what is irrelevant\\n  - **epiphenomenon** - a correlated effect (not a cause)\\n    - a secondary effect or byproduct that arises from but does not causally influence a process\\n  - **ontology** - study of being, concepts, categories\\n    - nodes in graphs must refer to stable concepts\\n    - ontologies are not always stable\\n      - world changes over time\\n      - \"looping effect\" - social categories (like race) are constantly chainging because people who putatively fall into such categories to change their behavior in possibly unexpected ways\\n  - **epistemology** - theory of knowledge\\n\\n### intuition\\n\\n- [bradford hill criteria](https://en.wikipedia.org/wiki/Bradford_Hill_criteria) - some simple criteria for establishing causality (e.g. strength, consistency, specificity)\\n  - association is circumstantial evidence for causation\\n- *no causation without manipulation* (Holland, 1986)\\n  - in this manner, something like causal effect of race/gender doesn\\'t make sense\\n  - can partially get around this by changing *race* $\\\\to$ *perceived race*\\n  - weaker view (e.g. of Pearl) is that we only need to be able to understand how entities interact (e.g. write an SEM)\\n- different levels\\n  - **experiment**: experiment, RCT, natural experiment, observation\\n  - **evidence**: marginal correlation, regression, invariance, causal\\n  - **inference** (pearl\\'s ladder of causality): prediction/association, intervention, counterfactuals\\n    - kosuke imai\\'s levels of inference: descriptive, predictive, causal\\n\\n### common examples\\n\\n- HIP trial of mammography - want to do whole treatment group v. whole control group\\n- John Snow on cholera - water\\n- causes of poverty - Yul\\'s model, changes with lots of things\\n- liver transplant\\n- monty hall problem: why you should switch\\n```mermaid\\ngraph LR\\nA(Your Door) -->B(Door Opened)\\nC(Location of Car) --> B\\n```\\n- berkson\\'s paradox - diseases in hospitals are correlated even when they are not in the general population\\n  - possible explanation - only having both diseases together is strong enough to put you in the hospital\\n- **simpson\\'s paradox** - trend appears in several different groups but disappears/reverses when groups are combined\\n  - e.g. overall men seemed to have higher acceptance rates, but in each dept. women seemed to have higher acceptance rates - explanation is that women selectively apply to harder depts.\\n\\n## frameworks\\n\\n### potential outcome framework (neyman-rubin)\\n\\n- advantages over DAGs: easy to express some common assumptions, such as monotonicity / convexity\\n- 3 frameworks\\n  1. neyman-rubin model: $Y_i = T_i a_i + (1-T_i) b_i$\\n    - $\\\\widehat{ATE} = \\\\hat{a}_A - \\\\hat{b}_B$\\n    - $\\\\widehat{ATE}_{adj} = [\\\\bar{a}_A - (\\\\bar{x}_A - \\\\bar{x})^T \\\\hat{\\\\theta}_A] - [\\\\bar{b}_B - (\\\\bar{x}_B - \\\\bar{x})^T \\\\hat{\\\\theta}_B]$\\n      - $\\\\hat{\\\\theta}_A = argmin \\\\sum_{i \\\\in A} (a_i - \\\\bar{a}_A - (x_i - \\\\bar{x}_A)^T \\\\theta)^2$\\n  2. neyman-pearson\\n    - null + alternative hypothesis\\n      - null is favored unless there is strong evidence to refute it\\n  3. fisherian testing framework\\n    - small p-values evidence against null hypothesis\\n    - null hypothesis\\n- action = intervention, exposure, treatments\\n- action $A$ and outcome $Y$\\n- ![Screen Shot 2020-05-05 at 10.50.28 AM](../assets/counterfactuals.png)\\n- **potential outcomes** = **counterfactual outcomes** $Y^{a=1}, Y^{a=0}$ \\n- **average treatment effect ATE**: $E[Y^{a=1} - Y^{a=0}]$\\n- key assumptions: SUTVA, consistency, ignorability\\n\\n### DAGs (pearl et al.)\\n\\n\\n- comparison to potential outcomes\\n  - easy to make clear exactly what is independent, particularly when there are many variables\\n  - do-calculus allows for answering some specific questions easily\\n  - often difficult to come up with proper causal graph\\n- [blog post on causal ladder](http://smithamilli.com/blog/causal-ladder/)\\n- [intro to do-calculus post](https://www.inference.vc/untitled/) and subsequent posts\\n\\n#### causal ladder (different levels of inference)\\n\\n1. **prediction/association** - just need to have the joint distr. of all the variables\\n\\t- basically just $p(y|x)$\\n2. **intervention** - we can change things and get conditionals based on *evidence after intervention*\\n  - $p(y|do(x))$ - which represents the conditional distr. we would get if we were to manipulate $x$ in a randomized trial\\n    - to get this, we assume the causal structure (can still kind of test it based on conditional distrs.)\\n    - having assumed the structure, we delete all edges going into a do operator and set the value of $x$\\n    - then, do-calculus yields a formula to estimate $p(y|do(x))$ assuming this causal structure\\n    - see introductory paper [here](https://arxiv.org/pdf/1305.5506.pdf), more detailed paper [here](https://ftp.cs.ucla.edu/pub/stat_ser/r416-reprint.pdf) (pearl 2013)\\n  - by assuming structure, we learn how large impacts are\\n3. **counterfactuals** - we can change things and get conditionals based on *evidence before intervention*\\n  - instead of intervention $p(y|do(x))$ we get $p(y^*|x^*, z=z)$ where z represents fixing all the other variables and $y^*$ and $x^*$ are not observed\\n    - averaging over all data points, we\\'d expect to get something similar to the intervention $p(y|do(x))$\\n  - probabalistic answer to a \"what would have happened if\" question\\n    - e.g. \"Given that Hillary lost and didn\\'t visit Michigan, would she win if she had visited Michigan?\"\\n    - e.g. “What fraction of patients who are treated and died would have survived if they were not treated?”\\n    - this allows for our intervention to contradict something we condition on \\n    - simple matching is often not sufficient (need a very good model for how to match, hopefully a causal one)\\n  - key difference with standard intervention is that we incorporate available evidence into our calculation\\n    - available evidence influences exogenous variables\\n    - this is for a specific data point, not a randomly sampled data point like an intervention would be\\n    - requires SEM, not just causal graph\\n\\n### sem (structured equation model)\\n\\n- gives a set of variables $X_1, ... X_i$ and and assignments of the form $X_i := f_i(X_{parents(i)}, \\\\epsilon_i)$, which tell how to compute value of each node given parents\\n\\n  - $\\\\epsilon_i$ = noise variables = **exogenous nodes** - node in the network that represents all the data not collected\\n  - parent nodes = *direct causes*\\n- again, fix value of $x$ (and values of $\\\\epsilon$ seend in the data) and use SEM to set all downstream variables\\n- ex. ![sem](../assets/sem.png)\\n\\n  - in this ex, W and H are usually correlated, so conditional distrs. are similar, but do operator of changing W has no effect on H (and vice versa)\\n  - notation: $P(H|do(W:=1))$ or $P_{M[W:=1]}(h)$\\n  - ATE of $W$ on $H$ would be $P(H|do(W:=1)) - P(H|do(W:=0))$\\n\\n#### causal graphs\\n\\n- common graphs\\n\\t- absence of edges often corresponds to qualitative judgements\\n| forks                                                        | mediators                                                    | colliders                                                    |\\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\\n| ![Screen Shot 2020-10-21 at 11.57.13 PM](../assets/explaining_away.png) | ![Screen Shot 2020-10-21 at 11.57.17 PM](../assets/confounder_in_way.png) | ![Screen Shot 2020-10-21 at 11.57.22 PM](../assets/confounding_triple.png) |\\n| confounder $z$, can be adjusted for                          | confounder can vary causal effect                            | conditioning on confounder z can explain away a cause        |\\n\\n-   **controlling** for a variable  (when we have a causal graph):\\n- $P(Y=y|do(X:=x)) = \\\\sum_z \\\\underbrace{P(Y=y|X=x, X_{parents}=z)}_{\\\\text{effect for slice}} \\\\underbrace{P(X_{parents}=z)}_{\\\\text{weight for slice}}$\\n-   **counterfactual** - given structural causal model *M*, observed event *E*, action *X:=x*, target variable *Y*, define counterfactual $Y_{X:=x}(E)$ in 3 steps:\\n    - **abduction** - adjust noise variables to be consistent with observation\\n    - **action** - perform do-intervention\\n    - **prediction** - compute target counterfactual\\n    - counterfactual can be a random variable or deterministic\\n- **back-door criterion** - establishes if 2 variables X, Y are confounded\\n  - more details: http://bayes.cs.ucla.edu/BOOK-2K/ch3-3.pdf\\n  - ensure that there is no path which points to X which allows dependence between X and Y ( paths which point to X are non-causal, representing confounders )\\n  - remember, in DAG junctions conditioning makes things independent unless its at a V junction\\n- **front-door criterion** - want to deconfound treatment from outcome, even without info on the confounder\\n  - only really need to know about treatment, M, and outcome\\n\\n```mermaid\\ngraph LR\\nC(Confounder) -->Y(Outcome)\\nC --> X(Treatment)\\nX --> M\\nM --> Y\\n```\\n- **mediation analysis** - identify a mechanism through which a cause has an effect\\n  - if there are multiple possible paths by which a variable can exert influence, can figure out which path does what, even with just observational data\\n\\n## assumptions\\n\\n- **stable unit treatment value assumption (SUTVA)** - treatment one unit receives doesn\\'t change effect of action for any other unit\\n  - **exchangeability** = exogeneity: $\\\\color{orange}{Y^{a}} \\\\perp \\\\!\\\\!\\\\! \\\\perp A$ for all $a$ - $\\\\textcolor{orange}{\\\\text{the value of the counterfactuals}}$ doesn\\'t change based on the choice of the action\\n- **consistency**: $Y=Y^{a=0}(1-A) + Y^{a=1}A$ - outcome agrees with the potential outcome corresponding to the treatment indicator\\n- **ignorability** - potential outcomes are conditionally independent of treatment given some deconfounding varibles\\n  - very hard to check!\\n\\n## modeling approaches\\n\\n### ladder of evidence\\n\\n- RCT\\n- natural experiment\\n- instrumental variable\\n- discontinuity analysis - look for points near a threshold treatment assignment\\n\\n### matching\\n\\n- [Matching methods for causal inference: A review and a look forward](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2943670/pdf/nihms200640.pdf) (stuart 2010)\\n  - matching methods choose try to to equate (or ``balance\\'\\') the distribution of covariates in the treated and control groups \\n    - they do this by picking well-matched samples of the original treated and control groups\\n    - this may involve 1:1 matching, weighting, or subclassification\\n    - linear regression adjustment (so noto matching) can actually increase bias in the estimated treatment effect when the true relationship between the covariate and outcome is even moderately non-linear, especially when there are large differences in the means and variances of the covariates in the treated and control groups\\n  - matching distance measures\\n    - propensity scores summarize all of the covariates into one scalar: the probability of being treated\\n      - defined as the probability of being treated given the observed covariates\\n      - propensity scores are balancing scores: At each value of the propensity score, the distribution of the covariates X defining the propensity score is the same in the treated and control groups -- usually this is logistic regresion\\n      - if treatment assignment is ignorable given the covariates, then treatment assignment is also ignorable given the propensity score: ![propensity](../assets/propensity_matching.png)\\n    - hard constraints are called \"exact matching\" - can be combined with other methods\\n    - mahalanabois distance\\n  - matching methods\\n    - *stratification* = *cross-tabulation* - only compare samples when confounding variables have same value\\n    - nearest neighbor matching - we discard many samples this way (but samples are more similar, so still helpful)\\n      - optimal matching - consider all potential matches at once, rather than one at a time\\n      - ratio matching - could match many to one (especially for a rare group), although picking the number of matches can be tricky\\n      - with/without replacement - with seems to have less bias, but more practical issues\\n    - subclassification/weighting: use **all the data** - this is nice because we have more samples, but we also get some really poor matches\\n      - subclassification - stratify score, like propensity score, into groups and measure effects among the groups\\n      - full matching - automatically picks the number of groups\\n      - weighting - use propensity score as weight in calculating ATE (also know as inverse probability of treatment weighting)\\n    - common support - want to look at points which are similar, and need to be careful with how we treat points that violate similarity\\n    - genetic matching - find the set of matches which minimize the discrepancy between the distribution of potential confounders\\n  - diagnosing matches - are covariates balanced after matching?\\n    - ideally we would look at all multi-dimensional histograms, but since we have few points we end up looking at 1-d summaries\\n    - one standard metric is difference in means of each covariate, divided by its stddev in the whole dataset\\n  - analysis of the outcome - can still use regression adjustment after doing the matching to clean up residual covariances\\n    - unclear how to propagate variance from matching to outcome analysis\\n- [Multivariate and Propensity Score Matching Software with Automated Balance Optimization: The Matching Package for R](http://sekhon.berkeley.edu/papers/MatchingJSS.pdf) (sekhon 2011)\\n\\n### weighting / regression adjustments\\n\\n- regression adjustments use models like a linear model to account for confounders\\n- requires *unconfoundedness* = *omitted variable bias*\\n- if there are no confounders, correlation is causation\\n\\n## studies\\n\\n### classic studies\\n\\n- [Who Gets a Swiss Passport? A Natural Experiment in Immigrant Discrimination](http://www.hangartner.net/files/passportapsr.pdf) (Hainmueller & Hangartner 2013)\\n  - naturalization decisions vary with immigrants\\' attributes\\n  - is there immigration against immigrants based on country of origin?\\n  - citizenship requires voting by municipality\\n- [When Natural Experiments Are Neither Natural nor Experiments](http://sekhon.berkeley.edu/papers/SekhonTitiunik.pdf) (sekhon & titunik 2012)\\n  - even when natural interventions are randomly as- signed, some of the treatment–control comparisons made available by natural experiments may not be valid\\n- [Descriptive Representation and Judicial Outcomes in Multiethnic Societies](https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12187) (Grossman et al. 2016)\\n  - judicial outcomes of arabs depended on whether there was an Arab judge on the panel\\n- [Identification of Causal Effects Using Instrumental Variables](https://www.jstor.org/stable/2291629?seq=1#metadata_info_tab_contents) (angrist, imbens, & rubin 1996)\\n  - bridges the literature of instrumental variables in econometrics and the literature of causal inference in statistics\\n  - applied paper with delicate statistics\\n  - carefully discuss the assumptions\\n  - instrumental variables - regression w/ constant treatment effects\\n  - effect of veteran status on mortality, using lottery number as instrument\\n- [Sex Bias in Graduate Admissions: Data from Berkeley](https://homepage.stat.uiowa.edu/~mbognar/1030/Bickel-Berkeley.pdf) (bickel et al. 1975)\\n  - simpson\\'s paradox example\\n- [Using Maimonides\\' Rule to Estimate the Effect of Class Size on Scholastic Achievement](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.554.9675) (angrist & lavy 1999)\\n  - reducing class size induces a signi\\U0010fc1ccant and substantial increase in test scores for fourth and \\U0010fc1cfth graders, although not for third graders.\\n- [Smoking and Lung Cancer: Recent Evidence and a Discussion of Some Questions](https://academic.oup.com/jnci/article/22/1/173/912572) (cornfield et al. 1959)\\n  - not a traditional statistics paper\\n  - most of it is a review of various scientific evidence about smoking and cancer\\n  - small methodology section that describes an early version of sensitivity analysis\\n  - describes one of the most important contributions causal inference has made to science\\n- [Matching and thick description in an observational study of mortality after surgery.](https://www.ncbi.nlm.nih.gov/pubmed/12933551) (rosenbaum & silber 2001)\\n  - spends a lot of time discussing links between quantitative and qualitative analyses\\n  - takes the process of checking assumptions very seriously, and it deals with an important scientific problem\\n- [Attributing Effects to a Cluster-Randomized Get-Out-the-Vote Campaign](https://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap06589) (hansen & bowers 2009)\\n  - about a randomized experiment\\n  - proved complex to analyze and led to some controversy in political science\\n  - resolves that controversy using well-chosen statistical tools.\\n  - Because randomization is present in the design I think the assumptions are much less of a stretch than in many settings (this is also the case in the Angrist, Imbens, Rubin paper)\\n\\n### stability / invariance\\n\\n- [Invariance, Causality and Robustness](https://arxiv.org/abs/1812.08233) (buhlmann 18)\\n  - predict $Y^e$ given $X^e$ such that the prediction “works well” or is “robust” for all $e ∈ \\\\mathcal F$ based on data from much fewer environments $e \\\\in \\\\mathcal E$\\n    - assumption: ideally $e$ changes only the distr. of $X^e$ ( so doesn\\'t act directly on $Y^e$ or change the mechanism between $X^e$ and $Y^e$)\\n    - assumption (invariance): there exists a subset of \"causal\" covariates - when conditioning on these covariates, the loss is the same across all environments $e$\\n    - when these assumptions are satisfied, then minimizing a worst-case risk over environments $e$ yields a causal parameter\\n  - identifiability issue: we typically can\\'t identify the causal variables without very many perturbations $e$\\n    - **Invariant Causal Prediction (ICP)** only identifies variables as causal if they appear in all invariant sets\\n  - anchor regression model helps to relax assumptions\\n- [Invariant Risk Minimization](https://arxiv.org/abs/1907.02893) (arjovsky, bottou, gulrajani, & lopez-paz 2019)\\n  - random splitting causes problems with our data\\n  - what to perform well under different distributions of X, Y\\n  - can\\'t be solved via robust optimization\\n  - a correlation is spurious when we do not expect it to hold in the future in the same manner as it held in the past\\n    - i.e. spurious correlations are unstable\\n  - assume we have infinite data, and know what kinds of changes our distribution for the problem might have (e.g. variance of features might change)\\n    - make a model which has the minimum test error regardless of the distribution of the problem\\n  - adds a penalty inspired by invariance (which can be viewed as a stability criterion)\\n- [The Hierarchy of Stable Distributions and Operators to Trade Off Stability and Performance](https://arxiv.org/abs/1905.11374) (subbaswamy, chen, & saria 2019)\\n  - different predictors learn different things\\n  - only pick the stable parts of what they learn (in a graph representation)\\n  - there is a tradeoff between stability to all shifts and average performance on the shifts we expect to see\\n  - different types of methods\\n    - *transfer learning* - given unlabelled test data, match training/testing representations\\n    - *proactive methods* - make assumptions about possible set of target distrs.\\n    - *data-driven methods* - assume independence of cause and mechanism, like ICP, and use data from different shifts to find invariant subsets\\n    - *explicit graph methods* - assume explicit knowledge of graph representing the data-generating process\\n  - hierarchy\\n    - level 1 - invariant conditional distrs. of the form $P(Y|\\\\mathbf Z)$\\n    - level 2 - conditional interventional distrs. of the form $P(Y|do(\\\\mathbf W), \\\\mathbf Z)$\\n    - level 3 - distributions corresponding to counterfactuals\\n\\n### reviews\\n\\n- [Causality for Machine Learning](https://arxiv.org/abs/1911.10500) (scholkopf 19)\\n  - most of ml is built on the iid assumption and fails when it is violated (e.g. cow on a beach)\\n- [Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics](https://arxiv.org/abs/1907.07271) (imbens 2020)\\n\\n### recent\\n\\n- [Incremental causal effects](https://arxiv.org/abs/1907.13258) (rothenhausler & yu, 2019)\\n  - instead of considering a treatment, consider an infinitesimal change in a continuous treatment\\n  - use assumption of local independence and can prove some nice things\\n    - local ignorability assumption states that potential outcomes are independent of the current treatment assignment in a neighborhood of observations\\n\\n- [link to iclr talk](https://www.technologyreview.com/s/613502/deep-learning-could-reveal-why-the-world-works-the-way-it-does/?fbclid=IwAR3LF2dc_3EvWXzEHhtrsqtH9Vs-4pjPALfuqKCOma9_gqLXMKDeCWrcdrQ) (bottou 2019)\\n- [The Blessings of Multiple Causes](https://arxiv.org/abs/1805.06826) (wang & blei, 2019) - having multiple causes can help construct / find all the confounders\\n\\n## different problems\\n\\n### heterogenous treatment effects\\n\\n*Heterogenous treatment effects refer to effects which differ for different subgroups / individuals in a population and requires more refined modeling.*\\n\\n- **conditional average treatment effect (CATE)** - get treatment effect for each individual conditioned on its covariates\\n  - meta-learners - break down CATE into regression subproblems\\n    - e.g. T-learner (foster et al. 2011, simplest) - fit one model for conditional expectation of each potential outcome and then subtract\\n    - e.g. X-learner (kunzel et al. 19)\\n    - e.g. R-learner (nie-wager, 20)\\n    - e.g. S-learner (hill 11)\\n  - tree-based methods\\n    - e.g. causal tree (athey-imbens, 16) - like decision tree, but change splitting criterion for differentiating 2 outcomes\\n    - e.g. causal forest (wager-athery, 18)\\n    - e.g. BART (hill, 12)\\n- **subgroup analysis** - identify subgroups with treatment effects far from the average\\n  - generally easier than CATE\\n- [staDISC](https://arxiv.org/pdf/2008.10109.pdf) (dwivedi, tan et al. 2020) - learn stable / interpretable subgroups for causal inference\\n  - CATE - estimate with a bunch of different models\\n    - meta-learners: T/X/R/S-learners\\n    - tree-based methods: causal tree/forest, BART\\n    - **calibration** to evaluate subgroup CATEs\\n      - main difficulty: hard to do model selection / validation (especially with imbalanced data)\\n        - often use some kind of proxy loss function\\n      - solution: compare average CATE within a bin to CATE on test data in bin\\n        - actual CATE doesn\\'t seem to generalize\\n        - but ordering of groups seems pretty preserved\\n      - stability: check stability of this with many CATE estimators\\n  - subgroup analysis\\n    - use CATE as a stepping stone to finding subgroups\\n    - easier, but still linked to real downstream tasks (e.g. identify which subgroup to treat)\\n    - main difficulty: can quickly overfit\\n    - **cell-search** - sequential\\n      - first prune features using feature importance\\n      - target: maximize a cell\\'s true positive - false positive (subject to using as few features as possible)\\n      - sequentially find cell which maximizes target\\n        - find all cells which perform close to as good as this cell\\n        - remove all cells contained in another cell\\n        - pick one randomly, remove all points in this cell, then continue\\n    - stability: rerun search multiple times and look for stable cells / stable cell coverage\\n\\n### causal discovery\\n\\n- overview\\n  - **goal of causal discovery is to identify the causal relationships** (sometimes under some smoothness / independence assumptions)\\n    - basics: conditional indep. checks can only determine graphs up to markov equivalence\\n  - 2 approaches\\n    - test noise distr. of relationships in different directions\\n    - check variables which reduce entropy the most\\n- [Discovering Causal Signals in Images](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lopez-Paz_Discovering_Causal_Signals_CVPR_2017_paper.pdf) (lopez-paz et al. 2017)\\n  - C(A, B) - count number of images in which B would disappear if A was removed\\n  - we say A *causes* B when C(A, B) is (sufficiently) greater than the converse C(B, A)\\n  - basics\\n    - given joint distr. of (A, B), we want to know if A -> B, B-> A\\n      - with no assumptions, this is nonidentifiable\\n    - requires 2 assumptions\\n      - ICM: independence between cause and mechanism (i.e. the function doesn\\'t change based on distr. of X) - this usually gets violated in anticausal direction\\n      - causal sufficiency - we aren\\'t missing any vars\\n    - ex. ![Screen Shot 2019-05-20 at 10.04.03 PM](../assets/learning_causal_pattern.png)\\n      - here noise is indep. from x (causal direction), but can\\'t be independent from y (non-causal direction)\\n      - in (c), function changes based on input\\n    - can turn this into binary classification and learn w/ network: given X, Y, does X->Y or Y-X?\\n  - on images, they get scores for different objects (w/ bounding boxes)\\n    - eval - when one thing is erased, does the other also get erased?\\n- [Visual Causal Feature Learning](https://arxiv.org/abs/1412.2309) (chalupka, perona, & eberhardt, 2015)\\n  - assume the behavior $T$ is a function of some hidden causes $H_i$ and the image\\n    - ![Screen Shot 2020-02-03 at 2.27.27 PM](../assets/hidden_graphical_node.png)\\n  - **Causal Coarsening Theorem** - causal partition is coarser version of the observational partition\\n    - observational partition - divide images into partition where each partition has constant prediction $P(T|I)$\\n    - causal partition - divide images into partition where each partition has constant $P(T|man(I))$\\n      - $man(I)$ does visual manipulation which changes $I$, while keeping all $H_i$ fixed and $T$ fixed\\n        - ex. turn a digit into a 7 (or turn a 7 into not a 7)\\n  - can further simplify the problem into $P(T|I) = P(T|C, S)$\\n    - $C$ are the causes and $S$ are the spurious correlates\\n    - any other variable $X$ such that $P(T|I) = P(T|X)$ has Shannon entropy $H(X) \\\\geq H(C, S)$ - these are the simplest descriptions of $P(T|I$)\\n  - causal effect prediction\\n    - first, create causal dataset of $P(T|man(I))$ and train, so the model can\\'t learn spurious correlations\\n    - then train on this - very similar to adversarial training\\n- [Visual Physics: Discovering Physical Laws from Videos](https://arxiv.org/abs/1911.11893)\\n  - 3 steps\\n    - Mask R-CNN finds bounding box of object and center of bounding box is taken to be location\\n    - $\\\\beta-VAE$ compresses the trajectory to some latent repr. (while also being able to predict held-out points of the trajectory)\\n    - **Eureqa** package does eq. discovery on latent repr + trajectory\\n      - includes all basic operations, such as addition, mult., sine function\\n      - R-squared value measures goodness of fit\\n  - see also SciNet -  [Discovering physical concepts with neural networks](https://arxiv.org/abs/1807.10300) (iten et al. 2020)\\n  - see also the field of symbolic regression\\n    - genetic programming is the most pervalent method here\\n    - alternatives: sparse regression, dimensional function synthesis\\n',\n",
       " '---\\n\\nlayout: notes\\ntitle: info Theory\\ncategory: stat\\n---\\n\\n#  info theory\\n\\n- *material from* Cover \"Elements of Information Theory\"\\n\\n## overview\\n\\n- with small number of points, estimated mutual information is too high\\n- founded with claude shannon 1948\\n- set of principles that govern flow + transmission of info\\n\\n- X is a R.V. on (finite) discrete alphabet ~ won\\'t cover much continuous\\n\\n## entropy, relative entropy, and mutual info\\n\\n### entropy\\n\\n- $H(X) = - \\\\sum p(x) \\\\:\\\\log p(x) = E[h(p)]$\\n  - $h(p)= - \\\\log(p)$\\n  - $H(p)$ implies p is binary\\n  - usually for discrete variables only\\n  - assume 0 log 0 = 0\\n\\n- intuition\\n  - higher entropy $\\\\implies$ more uniform\\n  - lower entropy $\\\\implies$ more pure\\n  1. expectation of variable $W=W(X)$, which assumes the value $-log(p_i)$ with probability $p_i$\\n  2. minimum, average number of binary questions (like is X=1?) required to determine value is between $H(X)$ and $H(X)+1$\\n  3. related to asymptotic behavior of sequence of i.i.d. random variables\\n\\n- properties\\n\\n  - $H(X) \\\\geq 0$ since $p(x) \\\\in [0, 1]$\\n  - funtion of distr. only, not the specific values the RV takes (the support of the RV)\\n  - gaussian has max entropy s.t. variance constraint\\n\\n- $H(Y\\\\|X)=\\\\sum p(x) H(Y\\\\|X=x) =- \\\\sum_x p(x) \\\\sum_y  p(y|x) \\\\log \\\\: p(y|x)$\\n\\n  - $H(X,Y)=H(X)+H(Y\\\\|X) =H(Y)+H(X\\\\|Y)$\\n\\n### relative entropy / mutual info\\n\\n- *relative entropy* = *KL divergence* - measures distance between 2 distributions\\n  - $$D(p\\\\|\\\\|q) = \\\\sum_x p(x) log \\\\frac{p(x)}{q(x)} = \\\\mathbb E_p log \\\\frac{p(X)}{q(X)} = \\\\mathbb E_p[-\\\\log q(X)] - H(p)\\t$$\\n  - if we knew the true distribution p of the random variable, we could construct a code with average description length H(p). \\n  - If, instead, we used the code for a distribution q, we would need H(p) + D(p\\\\|\\\\|q) bits on average to describe the random variable.\\n  - $D(p\\\\|\\\\|q) \\\\neq D(q\\\\|\\\\|p)$\\n  - properties\\n    - nonnegative\\n    - not symmetric\\n\\n- *mutual info I(X; Y)*: how much you can predict about one given the other\\n  \\n  - $I(X; Y) = \\\\sum_X \\\\sum_y p(x,y) log \\\\frac{p(x,y)}{p(x) p(y)} = D(p(x,y)\\\\|\\\\|p(x) p(y))$\\n  - $I(X; Y) =  -H(X,Y) + H(X) + H(Y))$\\n    - $=I(Y|X)$\\n  - $I(X; X) = H(X)$ so entropy sometimes called *self-information*\\n  \\n\\n![entropy-venn-diagram](../assets/entropy-venn-diagram.png)\\n\\n- cross-entropy: $H_q(p) = -\\\\sum_x p(x) \\\\: log \\\\: q(x)$\\n  \\n  ![Screen Shot 2018-07-02 at 11.26.42 AM](../assets/cross_entropy.png)\\n\\n### chain rules\\n\\n- *entropy* - $H(X_1, ..., X_n) = \\\\sum_i H(X_i \\\\| X_{i-1}, ..., X_1) = H(X_n \\\\| X_{n-1}, ..., X_1) + ... + H(X_1)$\\n- *conditional mutual info* $I(X; Y\\\\|Z) = H(X\\\\|Z) - H(X\\\\|Y,Z)$\\n  - $I(X_1, ..., X_n; Y) = \\\\sum_i I(X_i; Y\\\\|X_{i-1}, ... , X_1)$\\n- *conditional relative entropy* $D(p(y\\\\|x) \\\\|\\\\| q(y\\\\|x)) = \\\\sum_x p(x) \\\\sum_y p(y\\\\|x) log \\\\frac{p(y\\\\|x)}{q(y\\\\|x)}$\\n  - $D(p(x, y)\\\\|\\\\|q(x, y)) = D(p(x)\\\\|\\\\|q(x)) + D(p(y\\\\|x)\\\\|\\\\|q(y\\\\|x))$\\n\\n### axiomatic approach\\n\\n- *fundamental theorem of information theory* - it is possible to transmit information through a noisy channel at any rate less than channel capacity with an arbitrarily small probability of error\\n  - to achieve arbitrarily high reliability, it is necessary to reduce the transmission rate to the *channel capacity*\\n- uncertainty measure axioms\\n  1. H(1/M,...,1/M)=f(M) is a montonically increasing function of M\\n  2. f(ML) = f(M)+f(L) where M,L $\\\\in \\\\mathbb{Z}^+$\\n  3. *grouping axiom*\\n  4. H(p,1-p) is continuous function of p\\n- $H(p_1,...,p_M) = - \\\\sum p_i log p_i = E[h(p_i)]$\\n  - $h(p_i)= - log(p_i)$\\n  - only solution satisfying above axioms\\n  - H(p,1-p) has max at 1/2\\n- *lemma* - Let $p_1,...,p_M$ and $q_1,...,q_M$ be arbitrary positive numbers with $\\\\sum p_i = \\\\sum q_i = 1$. Then $-\\\\sum p_i log p_i \\\\leq - \\\\sum p_i log q_i$. Only equal if $p_i = q_i \\\\: \\\\forall i$\\n  - intuitively, $\\\\sum p_i log q_i$ is maximized when $p_i=q_i$, like a dot product\\n- $H(p_1,...,p_M) \\\\leq log M$ with equality iff  all $p_i = 1/M$\\n- $H(X,Y) \\\\leq H(X) + H(Y)$ with equality iff X and Y are independent\\n- $I(X,Y)=H(Y)-H(Y\\\\|X)$\\n- sometimes allow p=0 by saying 0log0 = 0\\n- information $I(x)=log_2 \\\\frac{1}{p(x)}=-log_2p(x)$\\n- reduction in uncertainty (amount of surprise in the outcome)\\n- if the probability of event happening is small and it happens the info is large\\n    - entropy $H(X)=E[I(X)]=\\\\sum_i p(x_i)I(x_i)=-\\\\sum_i p(x_i)log_2 p(x_i)$\\n- information gain $IG(X,Y)=H(Y)-H(Y\\\\|X)$\\n\\n    - $=-\\\\sum_j p(x_j) \\\\sum_i p(y_i\\\\|x_j) log_2 p(y_i\\\\|x_j)$\\n- parts\\n  1. random variable X taking on $x_1,...,x_M$ with probabilities $p_1,...,p_M$\\n  2. code alphabet = set $a_1,...,a_D$ . Each symbol $x_i$ is assigned to finite sequence of code characters called *code word* associated with $x_i$\\n  3. *objective* - minimize the average word length $\\\\sum p_i n_i$ where $n_i$ is average word length of $x_i$\\n- code is *uniquely decipherable* if every finite sequence of code characters corresponds to at most one message\\n  - *instantaneous code* - no code word is a prefix of another code word\\n\\n## basic inequalities\\n\\n### jensen\\'s inequality\\n\\n- *convex* - function lies below any chord\\n  - has positive 2nd deriv\\n  - linear functions are both convex and concave\\n- *Jensen\\'s inequality* - if f is a convex function and X is an R.V., $f(E[X]) \\\\leq E[f(X)]$\\n  - if f strictly convex, equality $\\\\implies X=E[X]$\\n- implications\\n  - *information inequality* $D(p\\\\|\\\\|q) \\\\geq 0$ with equality iff p(x)=q(x) for all x\\n  - $H(X) \\\\leq log \\\\|X\\\\|$ where \\\\|X\\\\| denotes the number of elements in the range of X, with equality if and only X has a uniform distr\\n  - $H(X\\\\|Y) \\\\leq H(X)$ - information can\\'t hurt\\n  - $H(X_1, ..., X_n) \\\\leq \\\\sum_i H(X_i)$\\n\\n\\n\\n## [mdl](https://homepages.cwi.nl/~pdg/ftp/mdlintro.pdf)\\n\\n- chapter 1: overview\\n  - explain data given limited observations\\n  - benefits\\n    - occam\\'s razor\\n    - no overfitting (can pick both form of model and params), without need for ad hoc penalties\\n    - bayesian interpretation\\n    - no need for underlying truth\\n  - description - in terms of some description method \\n    - e.g. a python program which prints a sequence then halts = Kolmogorov complexity\\n      - invariance thm - as long as sequence is long enough, choice of programming language doesn\\'t matter) - (kolmogorov 1965, chaitin 1969, solomonoff 1964)\\n      - not computable in general\\n      - for small samples in practice, depends on choice of programming language\\n    - in practice, we don\\'t use general programming languages but rather select a description method which we know we can get the length of the shortest description in that class (e.g. linear models)\\n      - trade-off: we may fail to minimally compress some sequences which have regularity\\n    - knowing data-generating process can help compress (e.g. recording times for something to fall from a height, generating digits of $\\\\pi$ via taylor expansion, compressing natural language based on correct grammar)\\n  - simplest version - let $\\\\theta$ be the model and $X$ be the data\\n    - 2-part MDL: minimize $L(\\\\theta) + L(X|\\\\theta)$\\n      - $L(X|\\\\theta) = - \\\\log P(X|\\\\theta)$  - Shannon code\\n      - $L(\\\\theta)$ - hard to get this, basic problem with 2-part codes\\n        - have to do this for each model, not model-class (e.g. different linear models with same number of parameters would have different $L(\\\\theta)$\\n    - stochastic complexity (\"refined mdl\"): $\\\\bar{L}(X|\\\\theta)$ - only construct one code\\n      - ex. $\\\\bar L(X|\\\\theta) = |\\\\theta|_0 + L(X|\\\\theta)$ - like 2-part code but breaks up $\\\\theta$ space into different sets (e.g. same number of parameters) and assigns them equal codelength\\n    - normalized maximum likelihood - most recent version - select from among a set of codes\\n  \\n- chapter 2.2.1 background\\n  - in mdl, we only work with prefix codes (i.e. no codeword is a prefix of any other codeword)\\n    - these are uniquely decodable\\n    - in fact, any uniquely decodable code can be rewritten as a prefix code which achieves the same code length\\n  \\n- chapter 2.2.2: **probability mass functions correspond to codelength functions**\\n\\n  - coding: $x \\\\sim P(X)$, codelengths $\\\\ell(x)$\\n    - **Kraft inequality**: $\\\\sum_x 2^{-\\\\ell(x)} \\\\leq 1$\\n  - given a code $C$ and a prob distr. $P$, we can construct a code so short codewords get high probs and vice versa\\n    - given $P$, $\\\\exists C, \\\\forall z \\\\: L_C(z) \\\\leq \\\\lceil -\\\\log P(z) \\\\rceil$\\n    - given $C\\'$, $\\\\exists P\\' \\\\: \\\\forall z -\\\\log P(z) = L_{C\\'}(z)$\\n  \\n- we redefine codelength so it doesn\\'t require actual integer lengths\\n  - we don\\'t care about the actual encodings, only the codelengths\\n  - given a sample space $\\\\mathcal Z$, the set of all codelength functions $L_\\\\mathcal Z$ is the set of functions $L$ on $\\\\mathcal Z$ where $\\\\exists \\\\,Q$ (distr), such that $\\\\sum_z Q(z) \\\\leq 1$ and $\\\\forall z,\\\\; L(z) = -\\\\log Q(z)$\\n  \\n  - uniform distr. - every codeword just has same length (fixed-length)\\n  - we usually assume we are encoding a sequence $x^n$ which is large, so the rounding becomes negligible\\n  \\n  - ex. encoding integers: send $\\\\log k$ zeros, then add a 1, then uniform code from 0 to $2^{\\\\log k}$\\n  - Given $P(X)$, the codelength function $L(X) = -\\\\log P(X)$ minimizes expected code length for the variable $X$\\n  \\n- chapter 2.2.3 - **the information inequality**: $E_P[-\\\\log Q(X)] > E_P[-\\\\log P(X)]$ \\n\\n  - if $X$ was generated by $P$, then codes with length $-\\\\log P(X)$ give the shortest encodings on average\\n  - not surprising - in a large sample, X will occur with frequency proportial to $P(X)$, so we want to give it a short codelength $-\\\\log P(X)$\\n  - consequently, ideal mean length = $H(X)$\\n\\n- chapter 2.4: crude mdl ex. pick order of markov chain by minimizing $L(H) + L(D|H)$ where $D$ is data, $H$ is hypothesis\\n  - we get to pick codes for $L(H)$ and $L(D|H)$\\n  - let $L(x^n|H) = -\\\\log P(x^n|H)$ (length of data is just negative log-likelihood)\\n  - for $L(H)$, describe length of chain $k$ with code for integers, then $k$ parameters between 0 and 1 by describing the counts generated by the params in n samples - this tends to be harder to do well\\n  \\n- chapter 2.5: universal codes + models\\n\\n  - note: these codes are only universal relative to the set of considered codes $\\\\mathcal L$\\n\\n  - *universal model* - prob. distr corresponding to universal codes \\n\\n    - (different from how we use model in statistics)\\n\\n  - given a set of codes $\\\\mathcal L = \\\\{ L_1, L_2, ... \\\\}$, given a squences $x^n$, one of the codes $L \\\\in \\\\mathcal L$ has the shortest length for that sequence $\\\\bar L(x^n)$\\n\\n    - however, we have to pick one $L$, before we see $x^n$\\n\\n  - **universal code** - one code such that no matter which $x^n$ arrives, length is not much longer than the shortest length among all considered codes\\n  \\n    - ex. 2-part codes: first send bits to pick among codes, then use the selected code to encode $x^n$ - overhead is small because it doesn\\'t depend on $n$\\n    - among countably infinite codes, can still send $k$ to index the code, although $k$ can get very large\\n    - ex. bayesian universal model - assign prior distr to codes\\n  - ex. **nml** is an optimal (minimizes worst-case regret) universal model\\n      - $\\\\bar P_{\\\\text{nml}} (x^n) = \\\\frac{P(x^n | \\\\hat \\\\theta (x^n))}{\\\\sum_{z^n \\\\in \\\\mathcal X^n} P(z^n | \\\\hat \\\\theta (z^n))}$\\n      - literally a normalized likelihood\\n      - **regret** of $\\\\bar P$ relative to $M$:  $−\\\\log \\\\bar P(x^n)− \\\\min_{P \\\\in M} -\\\\log P(x^n )$\\n        - measures the performance of universal models relative to a set of candidate sources M \\n        - $\\\\bar P$ is a probability distribution on $\\\\mathcal X^n$ ($P$ is not necessarily in $M$)\\n        - when evaluating a code, we may look at the worst regret over all $x^n$, or the average\\n  \\n  ',\n",
       " '---\\nlayout: notes\\ntitle: linear models\\ncategory: stat\\n---\\n\\n#  linear models\\n\\n- *Material from \"Statistical Models Theory and Practice\" - David Freedman*\\n\\n\\n## introduction\\n\\n- $Y = X \\\\beta + \\\\epsilon$\\n\\n  | type of linear regression | X            | Y                |\\n  | ------------------------- | ------------ | ---------------- |\\n  | simple                    | univariate   | univariate       |\\n  | multiple                  | multivariate | univariate       |\\n  | multivariate              | either       | multivariate     |\\n  | generalized               | either       | error not normal |\\n\\n\\n\\n- minimize: $L(\\\\theta) = \\\\|\\\\|Y-X\\\\beta\\\\|\\\\|_2^2 \\\\implies \\\\hat{\\\\theta} = (X^TX)^{-1} X^TY$\\n\\n- 2 proofs\\n\\n  1. set deriv and solve\\n  2. use projection matrix H to show HY is proj of Y onto R(X)\\n     1. the projection matrix maps the responses to the predictions: $\\\\hat{y} = Hy$\\n\\n  - define projection (hat) matrix $H = X(X^TX)^{-1} X^T$\\n    - show $\\\\|\\\\|Y-X \\\\theta\\\\|\\\\|^2 \\\\geq \\\\|\\\\|Y - HY\\\\|\\\\|^2$\\n    - key idea: subtract and add HY\\n\\n\\n- interpretation\\n  - if feature correlated, weights aren\\'t stable / can\\'t be interpreted\\n  - curvature inverse $(X^TX)^{-1}$ - dictates stability\\n  - importance: weight * feature value\\n- LS doesn\\'t work when p >> n because of colinearity of X columns\\n- assumptions\\n  - $\\\\epsilon \\\\sim N(X\\\\beta,\\\\sigma^2)$\\n  - *homoscedasticity*: $var(Y_i\\\\|X)$ is the same for all i\\n    - opposite of *heteroscedasticity*\\n- *multicollinearity* - predictors highly correlated\\n  - *variance inflation factor (VIF)* - measure how much the variances of the estimated regression coefficients are inflated as compared to when the predictors are not linearly related\\n- normal linear regression\\n  - variance MLE $\\\\hat{\\\\sigma}^2 = \\\\sum (y_i - \\\\hat{\\\\theta}^T x_i)^2 / n$\\n    - in unbiased estimator, we divide by n-p\\n  - LS has a distr. $N(\\\\theta, \\\\sigma^2(X^TX)^{-1})$\\n- linear regression model\\n  - when n is large, LS estimator ahs approx normal distr provided that X^TX /n is approx. PSD\\n- *weighted LS*: minimize $\\\\sum [w_i (y_i - x_i^T \\\\theta)]^2$\\n  - $\\\\hat{\\\\theta} = (X^TWX)^{-1} X^T W Y$\\n  - heteroscedastic normal lin reg model: erorrs ~ N(0, 1/w_i)\\n- *leverage scores* - measure how much each $x_i$ influences the LS fit\\n  - for data point i, $H_{ii}$ is the leverage score\\n- *LAD (least absolute deviation)* fit\\n  - MLE estimator when error is Laplacian\\n\\n## recent notes\\n\\n### regularization\\n\\n- when $(X^T X)$ isn\\'t invertible can\\'t use normal equations and gradient descent is likely unstable\\n  - X is nxp, usually n >> p and X almost always has rank p\\n  - problems when n < p\\n- intuitive way to fix this problem is to reduce p by getting rid of features\\n- a lot of papers assume your data is already zero-centered\\n  - conventionally don\\'t regularize the intercept term\\n\\n1. *ridge* regression (L2)\\n  - if (X^T X) not invertible, add a small element to diagonal\\n  - then it becomes invertible\\n  - small lambda -> numerical solution is unstable\\n  - proof of why it\\'s invertible is difficult\\n  - argmin $\\\\sum_i (y_i - \\\\hat{y_i})^2+ \\\\lambda \\\\vert \\\\vert \\\\beta\\\\vert \\\\vert _2^2 $\\n  - equivalent to minimizing $\\\\sum_i (y_i - \\\\hat{y_i})^2$ s.t. $\\\\sum_j \\\\beta_j^2 \\\\leq t$\\n  - solution is $\\\\hat{\\\\beta_\\\\lambda} = (X^TX+\\\\lambda I)^{-1} X^T y$\\n  - for small $\\\\lambda$ numerical solution is unstable\\n  - When $X^TX=I$, $\\\\beta _{Ridge} = \\\\frac{1}{1+\\\\lambda} \\\\beta_{Least Squares}$\\n2. *lasso* regression (L1)\\n  - $\\\\sum_i (y_i - \\\\hat{y_i})^2+\\\\lambda  \\\\vert \\\\vert \\\\beta\\\\vert \\\\vert _1 $ \\n  - equivalent to minimizing $\\\\sum_i (y_i - \\\\hat{y_i})^2$ s.t. $\\\\sum_j \\\\vert \\\\beta_j\\\\vert  \\\\leq t$\\n  - \"least absolute shrinkage and selection operator\"\\n  - lasso - least absolute shrinkage and selection operator - L1\\n  - acts in a nonlinear manner on the outcome y\\n  - keep the same SSE loss function, but add constraint of L1 norm\\n  - doesn\\'t have closed form for Beta\\n    - because of the absolute value, gradient doesn\\'t exist\\n    - can use directional derivatives\\n    - best solver is *LARS* - least angle regression\\n  - if tuning parameter is chose well, will set lots of coordinates to 0\\n  - convex functions / convex sets (like circle) are easier to solve\\n  - disadvantages\\n    - if p>n, lasso selects at most n variables\\n    - if pairwise correlations are very high, lasso only selects one variable\\n3. *elastic net* - hybrid of the other two\\n  - $\\\\beta_{Naive ENet} = \\\\sum_i (y_i - \\\\hat{y_i})^2+\\\\lambda_1 \\\\vert \\\\vert \\\\beta\\\\vert \\\\vert _1 + \\\\lambda_2  \\\\vert \\\\vert \\\\beta\\\\vert \\\\vert _2^2$ \\n  - l1 part generates sparse model\\n  - l2 part encourages grouping effect, stabilizes l1 regularization path\\n    - grouping effect - group of highly correlated features should all be selected\\n  - naive elastic net has too much shrinkage so we scale $\\\\beta_{ENet} = (1+\\\\lambda_2) \\\\beta_{NaiveENet}$\\n  - to solve, fix l2 and solve lasso\\n\\n### the regression line (freedman ch 2)\\n\\n- regression line\\n  - goes through $(\\\\bar{x}, \\\\bar{y})$\\n  - slope: $r s_y / s_x$\\n  - intercept: $\\\\bar{y} - slope \\\\cdot \\\\bar{x}$\\n  - basically fits graph of averages (minimizes MSE)\\n- SD line\\n  - same except slope: $sign(r) s_y / s_x$\\n  - intercept changes accordingly\\n- for regression, MSE = $(1-r^2) Var(Y)$\\n\\n### multiple regression (freedman ch 4)\\n\\n- assumptions\\n  1. assume $n > p$ and X has full rank (rank p - columns are linearly independent)\\n  2. $\\\\epsilon_i$ are iid, mean 0, variance $\\\\sigma^2$\\n  3. $\\\\epsilon$ independent of $X$\\n    - $e_i$ still orthogonal to $X$\\n- OLS is conditionally unbiased\\n  \\n  - $E[\\\\hat{\\\\theta} \\\\| X] = \\\\theta$\\n- $Cov(\\\\hat{\\\\theta}\\\\|X) = \\\\sigma^2 (X^TX)^{-1}$\\n  - $\\\\hat{\\\\sigma^2} = \\\\frac{1}{n-p} \\\\sum_i e_i^2$\\n    - this is unbiased - just dividing by n is too small since we have minimized $e_i$ so their variance is lower than var of $\\\\epsilon_i$\\n- *random errors* $\\\\epsilon$\\n- *residuals* $e$\\n- $H = X(X^TX)^{-1} X^T$\\n  1. e = (I-H)Y = $(I-H) \\\\epsilon$\\n  2. H is symmetric\\n  3. $H^2 = H, (I-H)^2 = I-H$\\n  4. HX = X\\n  5. $e \\\\perp X$\\n- basically H projects Y int R(X)\\n- $E[\\\\hat{\\\\sigma^2}\\\\|X] = \\\\sigma^2$\\n- random errs don\\'t need to be normal\\n- variance\\n  - $var(Y) = var(X \\\\hat{\\\\theta}) + var(e)$\\n    - $var(X \\\\hat{\\\\theta})$ is the *explained variance*\\n    - *fraction of variance explained*: $R^2 = var(X \\\\hat{\\\\theta}) / var(Y)$\\n    - like summing squares by projecting\\n  - if there is no intercept in a regression eq, $R^2 = \\\\|\\\\|\\\\hat{Y}\\\\|\\\\|^2 / \\\\|\\\\|Y\\\\|\\\\|^2$\\n\\n## advanced topics\\n\\n### BLUE\\n\\n- drop assumption: $\\\\epsilon$ independent of $X$\\n  - instead: $E[\\\\epsilon\\\\|X]=0, cov[\\\\epsilon\\\\|X] = \\\\sigma^2 I$\\n  - can rewrite: $E[\\\\epsilon]=0, cov[\\\\epsilon] = \\\\sigma^2 I$ fixing X\\n- *Gauss-markov thm* - assume linear model and assumption above: when X is fixed, OLS estimator is *BLUE* = best linear unbiased estimator\\n  - has smallest variance.\\n  - ***prove this***\\n\\n### GLS = GLM\\n\\n- GLMs roughly solve the problem where outcomes are non-Gaussian\\n  - mean is related to $w^tx$ through a link function (ex. logistic reg assumes sigmoid)\\n  - also assume different prob distr on Y (ex. logistic reg assumes Bernoulli)\\n- *generalized least squares regression model*: instead of above assumption, use $E[\\\\epsilon\\\\|X]=0, cov[\\\\epsilon\\\\|X] = G, \\\\: G \\\\in S^K_{++}$\\n  - covariance formula changes: $cov(\\\\hat{\\\\theta}_{OLS}\\\\|X) = (X^TX)^{-1} X^TGX(X^TX)^{-1}$\\n  - estimator is the same, but is no longer BLUE - can correct for this:\\n    $(G^{-1/2}Y) = (G^{-1/2}X)\\\\theta + (G^{-1/2}\\\\epsilon)$\\n- *feasible GLS*=*Aitken estimator* - use $\\\\hat{G}$\\n- examples\\n  - simple\\n  - iteratively reweighted\\n- 3 assumptions can break down:\\n  1. if $E[\\\\epsilon\\\\|X] \\\\neq 0$ - GLS estimator is biased\\n  2. else if $cov(\\\\epsilon\\\\|X) \\\\neq G$ - GLS unbiased, but covariance formula breaks down\\n  3. if G from data, but violates estimation procedure, estimator will be misealding estimate of cov\\n\\n### path models\\n\\n- *path model* - graphical way to represent a regression equation\\n- making causal inferences by regression requires a *response schedule*\\n\\n### simultaneous equations\\n\\n- *simultaneous-equation* models - use *instrumental variables / two-stage least squares*\\n  - these techniques avoid *simultaneity bias = endogeneity bias*\\n\\n### binary variables\\n\\n- indicator variables take on the value 0 or 1\\n  - *dummy coding* - matrix is singular so we drop the last indicator variable - called *reference* class / *baseline* class\\n  - effect coding\\n    - one vector is all -1s\\n    - B_0 should be weighted average of the class averages\\n  - orthogonal coding\\n- *additive effects* assume that each predictor’s effect on the response does not depend on the value of the other predictor (as long as the other one was fixed\\n  - assume they have the same slope\\n- *interaction effects* allow the effect of one predictor on the response to depend on the values of other predictors.\\n  - $y_i = β_0 + β_1x_{i1} + β_2x_{i2} + β_3x_{i1}x_{i2} + ε_i$\\n\\n### LR with non-linear basis functions\\n\\n- can have nonlinear basis functions (ex. polynomial regression)\\n- radial basis function - ex. kernel function (Gaussian RBF)\\n  - $exp(-(x-r)^2 /  (2 \\\\lambda ^2))$\\n- non-parametric algorithm - don\\'t get any parameters theta; must keep data\\n\\n### locally weighted LR (lowess)\\n\\n- recompute model for each target point\\n- instead of minimizing SSE, we minimize SSE weighted by each observation\\'s closeness to the sample we want to query\\n\\n### kernel regression\\n\\n- nonparametric method\\n\\n- $\\\\operatorname{E}(Y | X=x) = \\\\int y f(y|x) dy = \\\\int y \\\\frac{f(x,y)}{f(x)} dy$\\n\\n  Using the [[kernel density estimation]] for the joint distribution \\'\\'f(x,y)\\'\\' and \\'\\'f(x)\\'\\' with a kernel \\'\\'\\'\\'\\'K\\'\\'\\'\\'\\',\\n\\n  $\\\\hat{f}(x,y) = \\\\frac{1}{n}\\\\sum_{i=1}^{n} K_h\\\\left(x-x_i\\\\right) K_h\\\\left(y-y_i\\\\right)$\\n  $\\\\hat{f}(x) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} K_h\\\\left(x-x_i\\\\right)$\\n  \\n\\n  we get\\n\\n  $\\\\begin{align} \\\\operatorname{\\\\hat E}(Y | X=x) &= \\\\int \\\\frac{y \\\\sum_{i=1}^{n} K_h\\\\left(x-x_i\\\\right) K_h\\\\left(y-y_i\\\\right)}{\\\\sum_{j=1}^{n} K_h\\\\left(x-x_j\\\\right)} dy,\\\\\\\\ &= \\\\frac{\\\\sum_{i=1}^{n} K_h\\\\left(x-x_i\\\\right) \\\\int y \\\\, K_h\\\\left(y-y_i\\\\right) dy}{\\\\sum_{j=1}^{n} K_h\\\\left(x-x_j\\\\right)},\\\\\\\\ &= \\\\frac{\\\\sum_{i=1}^{n} K_h\\\\left(x-x_i\\\\right) y_i}{\\\\sum_{j=1}^{n} K_h\\\\left(x-x_j\\\\right)},\\\\end{align}$\\n\\n### GAM = generalized additive model\\n\\n- generalized additive models: assume mean output is sum of functions of individual variables (no interactions)\\n  - learn individual functions using splines\\n- $g(\\\\mu) = b + f(x_0) + f(x_1) + f(x_2) + ...$\\n- can also add some interaction terms (e.g. $f(x_0, x_1)$)\\n\\n## sums interpretation\\n\\n- SST - total sum of squares - measure of total variation in response variable\\n  - $\\\\sum(y_i-\\\\bar{y})^2$\\n- SSR - regression sum of squares - measure of variation explained by predictors\\n  - $\\\\sum(\\\\hat{y_i}-\\\\bar{y})^2$\\n- SSE - measure of variation not explained by predictors\\n  - $\\\\sum(y_i-\\\\hat{y_i})^2$\\n- SST = SSR + SSE\\n- $R^2 = \\\\frac{SSR}{SST}$ - coefficient of determination\\n  - measures the proportion of variation in Y that is explained by the predictor\\n\\n## geometry - J. 6\\n\\n- *LMS* = *least mean squares* (p-dimensional geometries)\\n\\n  - ![](../assets/j6_1.png)\\n  - $y_n = \\\\theta^T x_n + \\\\epsilon_n$\\n  - $\\\\theta^{(t+1)}=\\\\theta^{(t)} + \\\\alpha (y_n - \\\\theta^{(t)T} x_n) x_n$\\n    - converges if $0 < \\\\alpha < 2/||x_n||^2$\\n    - ![](../assets/j6_2.png)\\n    - ![](../assets/j6_3.png)\\n  - if N=p and all $x^{(i)}$ are lin. indepedent, then there exists exact solution $\\\\theta$\\n\\n- solving requires finding *orthogonal projection of y on column space of X* (n-dimensional geometries)\\n\\n  - ![](../assets/j6_4.png)\\n\\n  1. 3 Pfs\\n     1. geometry - $y-X\\\\theta^*$ must be orthogonal to columns of X: $X^T(y-X\\\\theta)=0$\\n     2. minimize least square cost function and differentiate\\n     3. show HY projects Y onto col(X)\\n\\n  - either of these approaches yield the *normal eqns*: $X^TX \\\\theta^* = X^Ty$\\n\\n- SGD\\n\\n  - SGD converges to normal eqn\\n\\n- ***convergence analysis***: requires $0 < \\\\rho < 2/\\\\lambda_{max} [X^TX]$\\n\\n  - algebraic analysis: expand $\\\\theta^{(t+1)}$ and take $t \\\\to \\\\infty$\\n  - geometric convergence analysis: consider contours of loss function\\n\\n- weighted least squares: $J(\\\\theta)=\\\\frac{1}{2}\\\\sum_n w_n (y_n - \\\\theta^T x_n)^2$\\n\\n  - yields $X^T WX \\\\theta^* = X^T Wy$\\n\\n- probabilistic interpretation\\n\\n  - $p(y\\\\|x, \\\\theta) = \\\\frac{1}{(2\\\\pi\\\\sigma^2)^{N/2}} exp \\\\left( \\\\frac{-1}{2\\\\sigma^2} \\\\sum_{n=1}^N (y_n - \\\\theta^T x_n)^2 \\\\right)$\\n  - $l(\\\\theta; x,y) = - \\\\frac{1}{2\\\\sigma^2} \\\\sum_{n=1}^N (y_n - \\\\theta^T x_n)^2$\\n    - log-likelihood is equivalent to least-squares cost function\\n\\n## likelihood calcs\\n\\n### normal equation\\n\\n- $L(\\\\theta) = \\\\frac{1}{2} \\\\sum_{i=1}^n (\\\\hat{y}_i-y_i)^2$\\n- $L(\\\\theta) = 1/2 (X \\\\theta - y)^T (X \\\\theta -y)$\\n- $L(\\\\theta) = 1/2 (\\\\theta^T X^T - y^T) (X \\\\theta -y)$ \\n- $L(\\\\theta) = 1/2 (\\\\theta^T X^T X \\\\theta - 2 \\\\theta^T X^T y +y^T y)$ \\n- $0=\\\\frac{\\\\partial L}{\\\\partial \\\\theta} = 2X^TX\\\\theta - 2X^T y$\\n- $\\\\theta = (X^TX)^{-1} X^Ty$\\n\\n### ridge regression\\n\\n- $L(\\\\theta) = \\\\sum_{i=1}^n (\\\\hat{y}_i-y_i)^2+ \\\\lambda \\\\vert \\\\vert \\\\theta\\\\vert \\\\vert _2^2$ \\n- $L(\\\\theta) = (X \\\\theta - y)^T (X \\\\theta -y)+ \\\\lambda \\\\theta^T \\\\theta$\\n- $L(\\\\theta) = \\\\theta^T X^T X \\\\theta - 2 \\\\theta^T X^T y +y^T y +  \\\\lambda \\\\theta^T \\\\theta$ \\n- $0=\\\\frac{\\\\partial L}{\\\\partial \\\\theta} = 2X^TX\\\\theta - 2X^T y+2\\\\lambda \\\\theta$\\n- $\\\\theta = (X^TX+\\\\lambda I)^{-1} X^T y$',\n",
       " \"---\\nlayout: notes\\ntitle: time series\\ncategory: stat\\n---\\n\\n#  time series\\n\\n## high-level\\n\\n### basics\\n\\n- usually assume points are equally spaced\\n- modeling - for understanding underlying process or predicting\\n- [nice blog](https://algorithmia.com/blog/introduction-to-time-series), [nice tutorial](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc442.htm), [Time Series for scikit-learn People](https://www.ethanrosenthal.com/2018/01/28/time-series-for-scikit-learn-people-part1/)\\n- *noise*, *seasonality* (regular / predictable fluctuations), *trend*, *cycle*\\n- multiplicative models: time series = trend * seasonality * noise\\n- additive model: time series = trend + seasonality + noise\\n- stationarity - mean, variance, and autocorrelation structure do not change over time\\n- **endogenous variable = x** = independent variable\\n- **exogenous variable = y** = dependent variable\\n\\n### libraries\\n\\n- [pandas has some great time-series functionality](https://tomaugspurger.github.io/modern-7-timeseries)\\n- [skits library](https://github.com/EthanRosenthal/skits) for forecasting\\n\\n### high-level modelling\\n\\n- common methods\\n  - decomposition - identify each of these components given a time-series\\n    - ex. loess, exponential smoothing\\n  - frequency-based methods - e.g. look at spectral plot\\n  - (AR) autoregressive models - linear regression of current value of one or more prior values of the series\\n  - (MA) moving-average models - require fitting the noise terms\\n  - (ARMA) box-jenkins approach \\n- moving averages\\n  - simple moving average - just average over a window\\n  - cumulative moving average - mean is calculated using previous mean\\n  - exponential moving average - exponentially weights up more recent points\\n- prediction (forecasting) models\\n  - autoregressive integrated moving average (arima)\\n    - assumptions: stationary model\\n\\n## similarity measures\\n\\n- [An Empirical Evaluation of Similarity Measures for Time Series Classification](https://arxiv.org/pdf/1401.3973.pdf) (serra et al. 2014)\\n  - lock-step measures (Euclidean distance, or any norm)\\n    - can resample to make them same length\\n  - feature-based measures (Fourier coefficients)\\n    - euclidean distance over all coefs is same as over time-series, but we usually filter out high-freq coefs\\n    - can also use wavelets\\n  - model-based measures (auto-regressive)\\n    - compare coefs of an AR (or ARMA) model\\n  - elastic measures\\n    - dynamic time warping = DTW - optimallt aligns in temporal domaub ti nubunuze accumulated cost\\n      - can also enforce some local window around points\\n      - Every index from the first sequence must be matched with one or more indices from the other sequence and vice versa\\n      - The first index from the first sequence must be matched with the first index from the other sequence (but it does not have to be its only match)\\n      - The last index from the first sequence must be matched with the last index from the other sequence (but it does not have to be its only match)\\n      - The mapping of the indices from the first sequence to indices from the other sequence must be monotonically increasing, and vice versa, i.e. if `j > i` are indices from the first sequence, then there must not be two indices `l > k` in the other sequence, such that index `i` is matched with index `l` and index `j` is matched with index `k` , and vice versa\\n    - edit distance EDR\\n    - time-warped edit distance - TWED\\n    - minimum jump cost - MJC\\n\\n## [book1](https://www.stat.tamu.edu/~suhasini/teaching673/time_series.pdf) (A course in Time Series Analysis) + [book2](http://home.iitj.ac.in/~parmod/document/introduction%20time%20series.pdf) (Intro to Time Series and Forecasting)\\n\\n### ch 1\\n\\n- when errors are dependent, very hard to distinguish noise from signal\\n- usually in time-series analysis, we begin by de-trending the data and analyzing the residuals\\n  - ex. assume linear trend or quadratic trend and subtract that fit (or could include sin / cos for seasonal behavior)\\n  - ex. look at the differences instead of the points (nth order difference removes nth order polynomial trend). However, taking differences can introduce dependencies in the data\\n  - ex. remove trend using sliding window (maybe with exponential weighting)\\n- periodogram - in FFT, this looks at the magnitude of the coefficients (but loses the phase information)\\n\\n### ch 2 - stationary time series\\n\\n- in time series, we never get iid data\\n- instead we make assumptions\\n  - ex. the process has a constant mean (a type of stationarity)\\n  - ex. the dependencies in the time-series are short-term\\n- autocorrelation plots: plot correlation of series vs series offset by different lags\\n- formal definitions of stationarity for time series $\\\\{X_t\\\\}$\\n  - **strict stationarity** - the distribution is the same across time\\n  - **second-order / weak stationarity** -  mean is constant for all t and, for any t and k, the covariance between $X_t$ and $X_{t+k}$ only depends on the lag difference k\\n    - In other words, there exists a function $c: \\\\mathbb Z \\\\to \\\\mathbb R$ such that for all t and k we have $c(k) = \\\\text{cov} (X_t, X_{t+k})$\\n    - strict stationary and $E|X_T^2| < \\\\infty \\\\implies$ second-order stationary\\n  - **ergodic** - stronger condition, says samples approach the expectation of functions on the time series: for any function $g$ and shift $\\\\tau_1, ... \\\\tau_k$:\\n    - $\\\\frac 1 n \\\\sum_t g(X_t, ... X_{t+\\\\tau_k}) \\\\to \\\\mathbb E [g(X_0, ..., X_{t+\\\\tau_k} )]$\\n- **causal** - can predict given only past values (for Gaussian processes no difference)\\n\\n### ch 3 - linear time series\\n\\n**note: can just assume all have 0 mean (otherwise add a constant)**\\n\\n- **AR model** $AR(p)$:  $$ X_t = \\\\sum_{i=1}^p \\\\phi_i X_{t-i}+ \\\\varepsilon_t $$\\n  - $\\\\phi_1, \\\\ldots, \\\\phi_p$ are parameters\\n  - $\\\\varepsilon_t$ is white noise\\n  - stationary assumption places constraints on param values (e.g. processes in the $AR(1)$ model with $|\\\\phi_1| \\\\ge 1$ are not stationary)\\n  - looks just like linear regression, but is more complex\\n    - if we don't account for issues, things can go wrong\\n      - model will not be stationary\\n      - model may be misspecified\\n      - $E(\\\\epsilon_t|X_{t-p}) \\\\neq 0$\\n    - this represents a set of difference equations, and as such, must have a solution\\n  - ex. $AR(1)$ model - if $|\\\\phi| < 0$, then soln is in terms of past values of {$\\\\epsilon_t$}, otherwise it is in terms of future values\\n    - ex. simulating - if we know $\\\\phi$ and $\\\\{\\\\epsilon_t\\\\}$, we still need to use the backshift operator to solve for  $\\\\{ X_t \\\\}$\\n  - ex. $AR(p)$ model - if $\\\\sum_j |\\\\phi_j|$< 1, and $\\\\mathbb E |\\\\epsilon_t| < \\\\infty$, then will have a causal stationary solution\\n  - **backshift operator** $B^kX_t=X_{t-k}$\\n    - solving requires using the backshift operator, because we need to solve for what all the residuals are\\n  - **characteristic polynomial** $\\\\phi(a) = 1 - \\\\sum_{j=1}^p \\\\phi_j a^j$\\n    - $\\\\phi(B) X_t = \\\\epsilon_t$\\n    - $X_t=\\\\phi(B)^{-1} \\\\epsilon_t$\\n  - can represent $AR(p)$ as a vector $AR(1)$ using the vector $\\\\bar X_t = (X_t, ..., X_{t-p+1})$\\n  - note: can reparametrize in terms of frequencies\\n- **MA model** $MA(q)$: $ X_t = \\\\sum_{i=1}^q \\\\theta_i \\\\varepsilon_{t-i} + \\\\varepsilon_t$\\n  - $\\\\theta_1 ... \\\\theta_q$ are params\\n  - $\\\\varepsilon_t$, $\\\\varepsilon_{t-1}$ are white noise error terms\\n  - harder to fit, because the lagged error terms are not visible (also means can't make preds on new time-series)\\n  - $E[\\\\epsilon_t] = 0$, $Var[\\\\epsilon_t] = 1$\\n  - much harder to estimate these parameters\\n  - $X_t = \\\\theta (B) \\\\epsilon_t$ (assuming $\\\\theta_0=1$)\\n- **ARMA model**: $ARMA(p, q)$: $X_t = \\\\sum_{i=1}^p \\\\phi_i X_{t-i} + \\\\sum_{i=1}^q \\\\theta_i \\\\varepsilon_{t-i} + \\\\varepsilon_t$\\n  - $\\\\{X_t\\\\}$ is stationary\\n  - $\\\\phi (B) X_t = \\\\theta(B) \\\\varepsilon_t$\\n  - $\\\\phi(B) = 1 - \\\\sum_{j=1}^p \\\\phi_j B^j$\\n  - $\\\\theta(B) = 1 + \\\\sum_{j=1}^{q}\\\\theta_jz^j$\\n  - **causal** if $\\\\exists \\\\{ \\\\psi_j \\\\}$ such that $X_t = \\\\sum_{j=0}^\\\\infty \\\\psi_j Z_{t-j}$ for all t\\n\\n- **ARIMA model**: $ARIMA(p, d, q)$: - generalizes ARMA model to non-stationarity (using differencing)\\n\\n### ch 4 + 8 - the autocovariance function + parameter estimation\\n\\n- estimation\\n  - pure autoregressive\\n    - Yule-walker\\n    - Burg estimation - minimizing sums of squares of forward and backward one-step prediction errors with respect to the coefficients\\n  - when $q > 0$\\n    - innovations algorithm\\n    - hannan-rissanen algorithm\\n- autocovariance function: {$\\\\gamma(k): k \\\\in \\\\mathbb Z$} where $\\\\gamma(k) = \\\\text{Cov}(X_{t+h}. X_t) =  \\\\mathbb E (X_0 X_k)$ (assuming mean 0)\\n- ![Screen Shot 2020-01-11 at 5.29.15 PM](../assets/durbin-levinson.png)\\n- **Yule-Walker equations** (assuming AR(p) process): $\\\\mathbb E (X_t X_{t-k}) = \\\\sum_{j=1}^p \\\\phi_j \\\\mathbb E (X_{t-j} X_{t-k}) + \\\\underbrace{\\\\mathbb E (\\\\epsilon_tX_{t-k})}_{=0} = \\\\sum_{j=1}^p \\\\phi_j \\\\mathbb E (X_{t-j} X_{t-k})$\\n  - ex. MA covariance becomes 0 with lag > num params\\n- can rewrite the Yule-Walker equations\\n\\n  - $\\\\gamma(i) = \\\\sum_{j=1}^p \\\\phi_j \\\\gamma(i -j)$\\n  - $\\\\underline\\\\gamma_p = \\\\Gamma_p \\\\underline \\\\phi_p$\\n    - $(\\\\Gamma_p)_{i, j} = \\\\gamma(i - j)$\\n    - $\\\\hat{\\\\Gamma}_p$ is nonegative definite (and nonsingular if there is at least one nonzero $Y_i$)\\n  - $\\\\underline \\\\gamma_p = [\\\\gamma(1), ..., \\\\gamma(p)]$\\n  - $\\\\underline \\\\phi_p = (\\\\phi_1, ..., \\\\phi_p)$\\n    - this minimizes the mse $\\\\mathbb E [X_{t+1} - \\\\sum_{j=1}^p \\\\phi_j X_{t+1-j}]^2$\\n- use estimates to solve: $\\\\hat{\\\\underline \\\\phi}_p = \\\\hat \\\\Sigma_p^{-1} \\\\hat{\\\\underline r}_p $\\n- the innovations algorithm\\n  - set $\\\\hat X_1 = 0$\\n  - **innovations** = one-step prediction errors $U_n = X_n - \\\\hat X _n$\\n- **mle** (ch 5.2)\\n  - eq. 5.2.9: Gaussian likelihood for an ARMA process\\n  - $r_n = \\\\mathbb E[(W_{n+1} - \\\\hat W_{n+1})^2]$\\n\\n\\n\\n## multivariate time-series ch 7\\n\\n- vector-valued time-series has dependencies between variables across time\\n  - just modeling as univariate fails to take into account possible dependencies between the series\",\n",
       " '---\\nlayout: notes\\ntitle: game theory\\ncategory: stat\\n---\\n\\n#  game theory\\n\\n- **Backward Induction** – start at the ends and consider the last moves for each player until the beginning is reached\\n- **Dominating strategy** – always gives a better payoff regardless of other player\\n\\t- Weakly dominating strategy – always at least as good\\n- **Mixed strategy** – active randomization with given probabilities that determines players decision\\n- **Nash equilibrium** - no player can unilaterally (without the other player changing) change his strategy and get a better payoff\\n- Prisoner’s Dilllema has dominant strategy of both complying\\n\\t- in a repeated game, this inefficiency can be fixed\\n- Quality choice game – provider and buyer can each offer high or low bandwidth\\n\\t- Two Nash equilibria\\n\\t- In an evolutionary game, which equilibrium is picked is based on what percentage of each the provider expects\\n\\t- Over time, the buyer will mimic whatever the provider is providing\\n- Mixed strategies – compliance inspections\\n\\t- The percentage of the time that the inspector should inspect is based on the incentive / penalty that the buyer will cheat\\n\\t- It has a mixed equilibrium, based on probabilities\\n\\t- Player’s willingness for risk and other factors are considered when determining numbers for utility\\n- Many games have first-mover advantage: such as firms determining how much of a product to produce\\n- There are games with imperfect information- ex. Deciding whether to announce or cede\\n- Bidding in auctions – one of the main uses of game theory\\n\\t- You should bid how much you are willing to pay\\n\\t- Winner’s curse – if you win an auction for something with common value, you probably overvalued it\\n\\n## interesting studies\\n- [Chimpanzee choice rates in competitive games match equilibrium game theory predictions](https://www.nature.com/articles/srep05182)\\n',\n",
       " '---\\nlayout: notes\\ntitle: Computer Vision\\ncategory: ml\\n---\\n\\n#  computer vision\\n\\n## what\\'s in an image?\\n\\n- vision doesn\\'t exist in isolation - movement\\n- three R\\'s: recognition, reconstruction, reorganization\\n\\n### fundamentals of image formation\\n\\n#### projections\\n\\n- image I(x,y) projects scene(X, Y, Z)\\n  - lower case for image, upper case for scene\\n  - ![](../assets/pinhole.png)\\n    - f is a fixed dist. not a function\\n    - box with pinhole=*center of projection*, which lets light go through\\n    - Z axis points out of box, X and Y aligned w/ image plane (x, y)\\n- perspective projection - maps 3d points to 2d points through holes\\n  - ![](../assets/perspective.png)\\n  - perspective projection works for spherical imaging surface - what\\'s important is 1-1 mapping between rays and pixels\\n  - natural measure of image size is visual angle\\n- **orthographic projection** - appproximation to perspective when object is relatively far\\n  - define constant $s = f/Z_0$\\n  - transform $x = sX, y = sY$\\n\\n#### phenomena from perspective projection\\n\\n- parallel lines converge to vanishing point (each family has its own vanishing point)\\n  - pf: point on a ray $[x, y, z] = [A_x, A_y, A_z] + \\\\lambda [D_x, D_y, D_z]$\\n  - $x = \\\\frac{fX}{Z} = \\\\frac{f \\\\cdot (A_x+\\\\lambda D_X)}{A_z + \\\\lambda D_z}$\\n  - $\\\\lambda \\\\to \\\\infty \\\\implies \\\\frac{f \\\\cdot \\\\lambda D_x}{\\\\lambda D_z} = \\\\frac{f \\\\cdot D_x}{D_z}$\\n  - $\\\\implies$ vanishing point coordinates are $fD_x / D_z , f D_y / D_z$\\n  - not true when $D_z = 0$\\n  - all vanishing points lie on horizon\\n- nearer objects are lower in the image\\n  - let ground plane be $Y = -h$ (where h is your height)\\n  - point on ground plane $y = -fh / Z$\\n- nearer objects look bigger\\n- *foreshortening* - objects slanted w.r.t line of sight become smaller w/ scaling factor cos $\\\\sigma$ ~ $\\\\sigma$ is angle between line of sight and the surface normal\\n\\n### radiometry\\n\\n- *irradiance* - how much light (photons) are captured in some time interval\\n  - radiant power / unit area ($W/m^2$)\\n  - *radiance* - power in given direction / unit area / unit solid angle\\n    - L = directional quantity (measured perpendicular to direction of travel)\\n    - $L = Power / (dA cos \\\\theta \\\\cdot d\\\\Omega)$  where $d\\\\Omega$ is a solid angle (in steradians)\\n- irradiance $\\\\propto$ radiance in direction of the camera\\n- outgoing radiance of a patch has 3 factors\\n  - incoming radiance from light source\\n  - angle between light / camera\\n  - reflectance properties of patch\\n- 2 special cases\\n  - *specular surfaces* - outgoing radiance direction obeys angle of incidence\\n  - *lambertian surfaces* - outgoing radiance same in all directions\\n    - albedo * radiance of light * cos(angle)\\n  - model reflectance as a combination of Lambertian term and specular term\\n- also illuminated by reflections of other objects (ray tracing / radiosity)\\n- *shape-from-shading* (SFS) goes from irradiance $\\\\to$ geometry, reflectances, illumination\\n\\n### frequencies and colors\\n\\n- contrast sensitivity depends on frequency + color\\n- band-pass filtering - use gaussian pyramid\\n  - pyramid blending\\n- eye\\n  - *iris* - colored annulus w/ radial muscles\\n  - *pupil* - hole (aperture) whose size controlled by iris\\n  - *retina*: ![](../assets/retina.png)\\n- colors are what is reflected\\n- cones (short = blue, medium = green, long = red)\\n- *metamer* - 2 different but indistinguishable spectra\\n- color spaces\\n  - rgb - easy for devices\\n    - chips tend to be more green\\n  - hsv (hue, saturation, value)\\n  - lab (perceptually uniform color space)\\n- *color constancy* - ability to perceive invariant color despite ecological variations\\n- camera white balancing (when entire photo is too yellow or something)\\n  - manual - choose color-neutral object and normalize\\n  - automatic (AWB)\\n    - grey world - force average color to grey\\n    - white world - force brightest object to white\\n\\n## image processing\\n\\n### transformations\\n\\n- 2 object properties\\n    - *pose* - position and orientation of object w.r.t. the camera (6 numbers - 3 translation, 3 rotation)\\n    - *shape* - relative distances of points on the object\\n      - nonrigid objects can change shape\\n\\n| Transform (most general on top)                    | Constraints   | Invariants|                         2d params | 3d params |\\n| ---------------------------------------------------- | --------------------------------------- | --------- | --------- | --------- |\\n| Projective = homography (contains perspective proj.) | Ax + t, A nonsingular, homogenous coords | parallel -> intersecting | 8 (-1 for scale) |15 (-1 for scale)|\\n| Affine                                               | Ax + t, A nonsingular                   | parallelism, midpoints, intersection | 6=4+2 |12=9+3|\\n| Euclidean = Isometry                                 | Ax + t, A orthogonal                    | length, angles, area | 3=1+2 |6=3+3|\\n| Orthogonal (rotation when det = 1 / reflection when det = -1) | Ax, A orthogonal                        |           | 1 |3|\\n\\n- **projective transformation** = **homography**\\n\\n  - **homogenous coordinates** - use n + 1 coordinates for n-dim space to help us represent points at $\\\\infty$\\n    - $[x, y] \\\\to [x_1, x_2, x_3]$ with $x = x_1/x_3, y=x_2/x_3$\\n      - $[x_1, x_2] = \\\\lambda [x_1, x_2]  \\\\quad \\\\forall \\\\lambda \\\\neq 0$ - each points is like a line through origin in n + 1 dimensional space\\n      - even though we added a coordinate, didn\\'t add a dimension\\n    - standardize - make third coordinate 1 (then top 2 coordinates are euclidean coordinates)\\n      - when third coordinate is 0, other points are infinity\\n      - all 0 disallowed\\n    - Euclidean line $a_1x + a_2y + a_3=0$ $\\\\iff$ homogenous line $a_1 x_1 + a_2x_2 + a_3 x_3 = 0$\\n  - perspective maps parallel lines to lines that intersect\\n  - incidence of points on lines\\n    - when does a point $[x_1, x_2, x_3]$ lie on a line $[a_1, a_2, a_3]$ (homogenous coordinates)\\n    - when $\\\\mathbf{x} \\\\cdot \\\\mathbf{a} = 0$\\n  - cross product gives intersection of any 2 lines\\n  - representing affine transformations: $\\\\begin{bmatrix}X\\'\\\\\\\\Y\\'\\\\\\\\W\\'\\\\end{bmatrix} = \\\\begin{bmatrix}a_{11} & a_{12}  & t_x\\\\\\\\ a_{21} & a_{22} & t_y \\\\\\\\ 0 & 0 & 1\\\\end{bmatrix}\\\\begin{bmatrix}X\\\\\\\\Y\\\\\\\\1\\\\end{bmatrix}$\\n  - representing **perspective projection**: $\\\\begin{bmatrix}1 & 0& 0 & 0\\\\\\\\ 0 & 1 & 0 & 0 \\\\\\\\ 0 & 0 & 1/f & 0 \\\\end{bmatrix} \\\\begin{bmatrix}X\\\\\\\\Y\\\\\\\\Z \\\\\\\\ 1\\\\end{bmatrix} = \\\\begin{bmatrix}X\\\\\\\\Y\\\\\\\\Z/f\\\\end{bmatrix} = \\\\begin{bmatrix}fX/Z\\\\\\\\fY/Z\\\\\\\\1\\\\end{bmatrix}$\\n- **affine transformations**\\n  - affine transformations are a a group\\n  - examples\\n    - anisotropic scaling - ex. $\\\\begin{bmatrix}2 & 0 \\\\\\\\ 0 & 1 \\\\end{bmatrix}$\\n    - shear\\n- **euclidean transformations** = *isometries* = *rigid body transform*\\n  - preserves distances between pairs of points: $||\\\\psi(a) - \\\\psi(b)|| = ||a-b||$\\n    - ex. translation $\\\\psi(a) = a+t$\\n  - composition of 2 isometries is an isometry - they are a group\\n- **orthogonal transformations** - preserves inner products $\\\\forall a,b \\\\: a \\\\cdot b =a^T A^TA b$\\n  - $\\\\implies A^TA = I \\\\implies A^T = A^{-1}$\\n  - $\\\\implies det(A) = \\\\pm 1$\\n    - 2D\\n      - really only 1 parameter $ \\\\theta$ (also for the +t)\\n      - $A = \\\\begin{bmatrix}cos \\\\theta & - sin \\\\theta \\\\\\\\ sin \\\\theta & cos \\\\theta \\\\end{bmatrix}$ - rotation, det = +1\\n      - $A = \\\\begin{bmatrix}cos \\\\theta & sin \\\\theta \\\\\\\\ sin \\\\theta & - cos \\\\theta \\\\end{bmatrix}$ - reflection, det = -1\\n    - 3D\\n      - really only 3 parameters\\n      - ex. $A = \\\\begin{bmatrix}cos \\\\theta & - sin \\\\theta  & 0 \\\\\\\\ sin \\\\theta & cos \\\\theta & 0 \\\\\\\\ 0 & 0 & 1\\\\end{bmatrix}$ - rotation, det rotate about z-axis (like before)\\n\\n\\n- **rotation** - orthogonal transformations with det = +1 \\n  - 2D: $\\\\begin{bmatrix}cos \\\\theta & - sin \\\\theta \\\\\\\\ sin \\\\theta & cos \\\\theta \\\\end{bmatrix}$\\n  - 3D: $ \\\\begin{bmatrix}cos \\\\theta & - sin \\\\theta  & 0 \\\\\\\\ sin \\\\theta & cos \\\\theta & 0 \\\\\\\\ 0 & 0 & 1\\\\end{bmatrix}$ (rotate around z-axis)\\n- lots of ways to specify angles\\n  - axis plus amount of rotation - we will use this\\n  - euler angles\\n  - quaternions (generalize complex numbers)\\n- **Roderigues formula** - converts: $R = e^{\\\\phi \\\\hat{s}} = I + sin [\\\\phi] \\\\: \\\\hat{s} + (1 - cos \\\\phi) \\\\hat{s}^2$\\n  - $s$ is a unit vector along $w$ and $\\\\phi=||w||t$ is total amount of rotation\\n  - rotation matrix\\n    - can replace cross product with matrix multiplication with a skew symmetric $(B^T = -B)$ matrix: \\n    - $\\\\begin{bmatrix} t_1 \\\\\\\\ t_2 \\\\\\\\ t_3\\\\end{bmatrix}$ ^ $\\\\begin{bmatrix} x_1 \\\\\\\\ x_2 \\\\\\\\ x_3 \\\\end{bmatrix} = \\\\begin{bmatrix} t_2 x_3 - t_3 x_2 \\\\\\\\ t_3 x_1 - t_1 x_3 \\\\\\\\ t_1 x_2 - t_2 x_1\\\\end{bmatrix}$\\n      - $\\\\hat{t} = [t_\\\\times] = \\\\begin{bmatrix}  0 & -t_3 & t_2 \\\\\\\\ t_3 & 0 & -t_1 \\\\\\\\ -t_2 & t_1 & 0\\\\end{bmatrix}$\\n  - proof\\n    - $\\\\dot{q(t)} = \\\\hat{w} q(t)$\\n    - $\\\\implies q(t) = e^{\\\\hat{w}t}q(0)$\\n    - where $e^{\\\\hat{w}t} = I + \\\\hat{w} t + (\\\\hat{w}t)^2 / w! + ...$\\n      - can rewrite in terms above\\n\\n### image preprocessing\\n\\n- image is a function from $R^2 \\\\to R$\\n  - f(x,y) = reflectance(x,y) * illumination(x,y)\\n- image histograms - treat each pixel independently\\n  - better to look at CDF\\n  - use CDF as mapping to normalize a histogram\\n  - histogram matching - try to get histograms of all pixels to be same\\n- need to map high dynamic range (HDR) to 0-255 by ignoring lots of values\\n  - do this with long exposure\\n  - *point processing* does this transformation independent of position x, y\\n- can enhance photos with different functions\\n  - negative - inverts\\n  - log - can bring out details if range was too large\\n  - contrast stretching - stretch the value within a certain range (high contrast has wide histogram of values)\\n- sampling\\n  - sample and write function\\'s value at many points\\n  - reconstruction - make samples back into continuous function\\n  - ex. audio -> digital -> audio\\n  - undersampling loses information\\n  - **aliasing** - signals traveling in disguise as other frequencies\\n    - ![](../assets/sin1.png)\\n    - ![](../assets/sin2.png)\\n  - antialiasing\\n    - can sample more often\\n    - make signal less wiggly by removing high frequencies first\\n- filtering\\n  - *lowpass filter* - removes high frequencies\\n  - *linear filtering* - can be modeled by convolution\\n  - *cross correlation* - what cnns do, dot product between kernel and neighborhood\\n    - *sobel* filter is edge detector\\n  - *gaussian filter* - blur, better than just box blur\\n    - rule of thumb - set filter width to about 6 $\\\\sigma$\\n    - removes high-frequency components\\n  - **convolution** - cross-correlation where filter is flipped horizontally and vertically\\n    - commutative and associative\\n    - **convolution theorem**: $F[g*h] = F[g]F[h]$ where F is Fourier, * is convolution\\n      - convolution in spatial domain = multiplication in frequency domain\\n- resizing\\n  - Gaussian (lowpass) then subsample to avoid aliasing\\n  - image pyramid - called pyramid because you can subsample after you blur each time\\n    - whole pyramid isn\\'t much bigger than original image\\n    - *collapse* pyramid - keep upsampling and adding\\n    - good for template matching, search over translations\\n- sharpening - add back the high frequencies you remove by blurring (laplacian pyramid): ![](../assets/laplacian.png)\\n\\n\\n### edges + templates\\n\\n- **edge** - place of rapid change in the image intensity function\\n- solns\\n\\n  - smooth first, then take gradient\\n  - gradient first then smooth gives same results (linear operations are interchangeable)\\n- *derivative theorem of convolution* - differentiation can also be though of as convolution\\n\\n  - can convolve with deriv of gaussian\\n  - can give orientation of edges\\n- tradeoff between smoothing (denoising) and good edge localization (not getting blurry edges)\\n- image gradient looks like edges\\n- canny edge detector\\n\\n  1. filter image w/ deriv of Gaussian\\n  2. find magnitude + orientation of gradient\\n  3. **non-maximum suppression** - does thinning, check if pixel is local maxima\\n      - anything that\\'s not a local maximum is set to 0\\n      - on line direction, require a weighted average to interpolate between points (bilinear interpolation = average on edges, then average those points)\\n  4. **hysteresis thresholding** - high threshold to start edge curves then low threshold to continue them\\n- [Scale-space and edge detection using anisotropic diffusion](https://ieeexplore.ieee.org/abstract/document/56205) (perona & malik 1990)\\n  - introduces **anisotropic diffusion** (see [wiki page](https://en.wikipedia.org/wiki/Anisotropic_diffusion)) - removes image noise without removing content\\n  - produces series of images, similar to repeatedly convolving with Gaussian\\n- filter review\\n  - smoothing\\n    - no negative values\\n    - should sum to 1 (constant response on constant)\\n  - derivative\\n    - must have negative values\\n    - should sum to 0 (0 response on constant)\\n      - intuitive to have positive sum to +1, negative sum to -1\\n- matching with filters (increasing accuracy, but slower)\\n  - ex. zero-mean filter subtract mean of patch from patch (otherwise might just match brightest regions)\\n  - ex. SSD - L2 norm with filter\\n    - doesn\\'t deal well with intensities\\n  - ex. normalized cross-correlation\\n- recognition\\n  - instance - \"find me this particular chair\"\\n    - simple template matching can work\\n  - category - \"find me all chairs\"\\n\\n### texture\\n\\n- *texture* - non-countable stuff\\n  - related to material, but different\\n- texture analysis - compare 2 things, see if they\\'re made of same stuff\\n  - pioneered by bela julesz\\n  - random dot stereograms - eyes can find subtle differences in randomness if fed to different eyes\\n  - human vision sensitive to some difference types, but not others\\n- easy to classify textures based on v1 gabor-like features\\n- can make histogram of **filter response histograms** - convolve filter with image and then treat each pixel independently\\n- heeger & bergen siggraph 95 - given texture, want to make more of that texture\\n  - start with noise\\n  - match histograms of noise with each of your filter responses\\n  - combine them back together to make an image\\n  - repeat this iteratively\\n- simoncelli + portilla 98 - also match 2nd order statistics (match filters pairwise)\\n  - much harder, but works better\\n- **texton histogram matching**  - classify images\\n  - use \"computational version of textons\" - histograms of joint responses\\n    - like bag of words but with \"visual words\"\\n    - won\\'t get patches with exact same distribution, so need to extract good \"words\"\\n    - define words as k-means of features from 10x10 patches \\n      - features could be raw pixels\\n      - gabor representation ~10 dimensional vector\\n      - SIFT features: histogram set of oriented filters within each box of grid\\n      - HOG features\\n      - usually cluster over a bunch of images\\n      - invariance - ex. blur signal\\n  - each image patch -> a k-means cluster so image -> histogram\\n  - then just do nearest neighbor on this histogram (chi-squared test is good metric)\\n- object recognition is really texture recognition\\n- all methods follow these steps\\n  - compute low-level features\\n  - aggregate features - k-means, pool histogram\\n  - use as visual representation\\n- why these filters - sparse coding (data driven find filters)\\n\\n### optical flow\\n\\n- simplifying assumption - world doesn\\'t move, camera moves\\n  - lets us always use projection relationship $x, y = -Xf/Z, -Yf/Z$\\n- **optical flow** - movement in the image plane\\n  - square of points moves out as you get closer\\n    - as you move towards something, the center doesn\\'t change\\n    - things closer to you change faster\\n  - if you move left / right points move in opposite direction\\n    - rotations also appear to move opposite to way you turn your head\\n- equations: relate optical flow in image to world coords\\n  - optical flow at $(u, v) = (\\\\Delta x / \\\\Delta t,  \\\\Delta y/ \\\\Delta t)$ in time $\\\\Delta t$\\n    - function in image space (produces vector field)\\n  - $\\\\begin{bmatrix} \\\\dot{X}\\\\\\\\ \\\\dot{Y} \\\\\\\\ \\\\dot{Z} \\\\end{bmatrix} = -t -\\\\omega \\\\times \\\\begin{bmatrix} X \\\\\\\\ Y \\\\\\\\ Z\\\\end{bmatrix} \\\\implies \\\\begin{bmatrix} \\\\dot{x}\\\\\\\\ \\\\dot{y}\\\\end{bmatrix}= \\\\frac{1}{Z} \\\\begin{bmatrix}  -1 & 0 & x\\\\\\\\ 0 & 1 & y\\\\end{bmatrix} \\\\begin{bmatrix} t_x \\\\\\\\ t_y \\\\\\\\ t_z \\\\end{bmatrix}+ \\\\begin{bmatrix} xy & -(1+x^2) & y \\\\\\\\ 1+y^2 & -xy & -x\\\\end{bmatrix}\\\\begin{bmatrix} \\\\omega_x \\\\\\\\ \\\\omega_y \\\\\\\\ \\\\omega_z\\\\end{bmatrix}$\\n  - decomposed into translation component + rotation component\\n  - $t_z / Z$ is time to impact for a point\\n- translational component of flow fields is more important - tells us $Z(x, y)$ and translation $t$\\n- we can compute the time to contact\\n\\n\\n## cogsci / neuro\\n\\n### psychophysics\\n\\n- julesz search experiment\\n  - \"pop-out\" effect of certain shapes (e.g. triangles but not others)\\n  - axiom 1: human vision has 2 modes\\n    - **preattentive vision** - parallel, instantaneous (~100-200 ms)\\n      - large visual field, no scrutiny\\n      - surprisingly large amount of what we do\\n      - ex. sensitive to size/width, orientation changes\\n    - **attentive vision** - serial search with small focal attention in 50 ms steps\\n  - axiom 2: textons are the fundamental elements in preattentive vision\\n    - texton is invariant in preattentive vision \\n    - ex. elongated blobs (rectangles, ellipses, line segments w/ orientation/width/length)\\n    - ex. terminators - ends of line segments\\n    - crossing of line segments\\n- **julesz conjecture** (not quite true) - textures can\\'t be spontaneously discriminated if they have same first-order + second-order statistics (ex. density)\\n- humans can saccade to correct place in object detection really fast (150 ms - Kirchner & Thorpe, 2006)\\n  - still in preattentive regime\\n  - can also do object detection after seeing image for only 40 ms\\n\\n### neurophysiology\\n\\n- on-center off-surround - looks like Laplacian of a Gaussian\\n  - horizontal cell \"like convolution\"\\n- LGN does quick processing\\n- hubel & wiesel - single-cell recording from visual cortex in v1\\n- 3 v1 cell classes\\n  - *simple cells* - sensitive to oriented lines\\n    - oriented Gaussian derivatives\\n    - some were end-stopped\\n  - *complex cells* - simple cells with some shift invariance (oriented lines but with shifts)\\n    - could do this with maxpool on simple cells\\n  - *hypercomplex cells* (less common) - complex cell, but only lines of certain length\\n- *retinotopy* - radiation stain on retina maintained radiation image\\n- *hypercolumn* - cells of different orientations, scales grouped close together for a location\\n\\n### perceptual organization\\n\\n- max werthermian - we perceive things not numbers\\n- principles: grouping, element connectedness\\n- figure-ground organization: surroundedness, size, orientation, contrast, symmetry, convexity\\n- gestalt - we see based on context\\n\\n## correspondence + applications (steropsis, optical flow, sfm)\\n\\n### binocular steropsis\\n\\n- **stereopsis** - perception of depth\\n- **disparity** - difference in image between eyes\\n  - this signals depth (0 disparity at infinity)\\n  - measured in pixels (in retina plane) or angle in degrees\\n  - sign doesn\\'t really matter\\n- **active stereopsis** - one projector and one camera vs **passive (ex. eyes)**\\n  - active uses more energy\\n  - ex. kinect - measure / triangulate\\n    - worse outside\\n  - ex. lidar - time of light - see how long it takes for light to bounce back\\n- 3 types of 2-camera configurations: single point, parallel axes, general case\\n\\n#### single point of fixation (common in eyes)\\n\\n- fixation point has 0 disparity\\n  - humans do this to put things in the fovea\\n- use coordinates of *cyclopean eye*\\n- *vergence* movement - look at close / far point on same line\\n  - change angle of convergence (goes to 0 at $\\\\infty$)\\n    - *disparity* = $ 2 \\\\delta \\\\theta = b \\\\cdot \\\\delta Z / Z^2$ where b - distance between eyes, $\\\\delta Z$ - change in depth, Z - depth\\n    - ![epth_disparit](../assets/depth_disparity.png)\\n      - b - distance between eyes, $\\\\delta$ - change in depth, Z - depth\\n- *version* movement - change direction of gaze\\n  - forms Vieth-Muller circle - points lie on same circle with eyes\\n    - cyclopean eye isn\\'t on circle, but close enough\\n    - disparity of P\\' = $\\\\alpha - \\\\beta = 0$ on Vieth-Muller circle\\n    - ![ieth_mulle](../assets/vieth_muller.png)\\n\\n#### optical axes parallel (common in robots)\\n\\n- ![isparity_paralle](../assets/disparity_parallel.png)\\n- *disparity* $d = x_l - x_r = bf/Z$\\n- *error* $|\\\\delta Z| = \\\\frac{Z^2 |\\\\delta d|}{bf}$\\n- **parallax** - effect where near objects move when you move but far don\\'t\\n\\n#### general case (ex. reconstruct from lots of photos)\\n\\n- given n point correspondences, estimate rotation matrix R, translation t, and depths at the n points\\n\\n  - more difficult - don\\'t know coordinates / rotations of different cameras\\n\\n- **epipolar plane** - contains cameras, point of fixation\\n\\n  - different epipolar planes, but all contain line between cameras\\n  - $\\\\vec{c_1 c_2}$ is on all epipolar planes\\n  - each image plane has corresponding **epipolar line** - intersection of epipolar plane with image plane\\n    - **epipole** - intersection of $\\\\vec{c_1 c_2}$ and image plane\\n      - ![pipolar](../assets/epipolar1.png)\\n\\n- **structure from motion** problem: given n corresponding projections $(x_i, y_i)$ in both cameras, find $(X_i, Y_i, Z_i)$ by estimating R, t:  **Longuet-Higgins 8-point algorithm** - overall minimizing *re-projection error* (basically minimizes least squares = bundle adjustment)\\n\\n  - find $n (\\\\geq 8)$ corresponding points\\n  - estimate **essential matrix** $E = \\\\hat{T} R$ (converts between points in normalized image coords - origin at optical center)\\n    - **fundamental matrix** F corresponds between points in pixel coordinates (more degrees of freedom, coordinates not calibrated )\\n    - **essential matrix constraint**: $x_1, x_2$ homogoneous coordinates of $M_1, M_2 \\\\implies x_2^T \\\\hat{T} R x_1 = 0$\\n      - 6 or 5 dof; 3 dof for rotation, 3 dof for translation. up to a scale, so 1 dof is removed\\n      - $t = c_2 - c_1, x_2$ in second camera coords, $Rx_1$ moves 1st camera coords to second camera coords\\n    - need at least 8 pairs of corresponding points to estimate E (since E has 8 entries up to scale)\\n      - if they\\'re all coplanar, etc doesn\\'t always work (need them to be independent)\\n  - extract (R, t)\\n  - triangulation\\n\\n\\n### solving for stereo correspondence\\n\\n- **stereo correspondence** = stereo matching: given point in one image, find corresponding point in 2nd image\\n- **basic stereo matching algorithm**\\n  - **stereo image rectification** - transform images so that image planes are parallel\\n    - now, epipolar lines are horizontal scan lines\\n    - do this by using a few points to estimate R, t\\n  - for each pixel in 1st image \\n    - find corresponding epipolar line in 2nd image\\n    - correspondence search: search this line and pick best match\\n  - simple ex. parallel optical axes = assume cameras at same height, same focal lengths $\\\\implies$ epipolar lines are horizontal scan lines\\n    - ![aralle](../assets/parallel.png)\\n- **correspondence search algorithms** (simplest to most complex)\\n  - assume photo consistency - same points in space will give same brightness of pixels\\n  - take a window and use metric\\n    - larger window smoother, less detail\\n    - metrics\\n      - minimize L2 norm (SSD)\\n      - maximize dot product (NCC - normalized cross correlation) - this works a little better because calibration issues could be different\\n  - failures\\n    - textureless surfaces\\n    - occlusions - have to extrapolate the disparity\\n      - half-occlusion - can\\'t see from one eye\\n      - full-occlusion - can\\'t see from either eye\\n    - repetitions\\n    - non-lambertian surfacies, specularities - mirror has different brightness from different angles\\n\\n#### optical flow II\\n\\n- related to stereo disparity except moving one camera over time\\n- *aperture problem* - looking through certain hole can change perception (ex. see movement in wrong directions)\\n- measure correspondence over time\\n  - for point (x, y, t), optical flow is (u,v) = $(\\\\Delta x / \\\\Delta t, \\\\Delta y / \\\\Delta t)$\\n- *optical flow constraint equation*: $I_x u + I_y v + I_t = 0$\\n  - assume everything is Lambertian - brightness of any given point will stay the same\\n  - also add **brightness constancy assumption** - assume brightness of given point remains constant over short period $I(x_1, y_1, t_1) = I(x_1 + \\\\Delta x, y_1 + \\\\Delta x, t_1 + \\\\Delta t)$\\n  - here, $I_x = \\\\partial I / \\\\partial x$\\n\\n\\n  - local constancy of optical flow - assume u and v are same for n points in neighborhood of a pixel\\n  - rewrite for n points(left matrix is A): $\\\\begin{bmatrix} I_x^1 & I_y^1\\\\\\\\  I_x^2 & I_y^2\\\\\\\\ \\\\vdots & \\\\vdots \\\\\\\\ I_x^n & I_y^n\\\\\\\\ \\\\end{bmatrix}\\\\begin{bmatrix} u \\\\\\\\ v\\\\end{bmatrix} = - \\\\begin{bmatrix} I_t^1\\\\\\\\ I_t^2\\\\\\\\ \\\\vdots \\\\\\\\ I_t^n\\\\\\\\\\\\end{bmatrix}$\\n    - then solve with least squares $\\\\begin{bmatrix} u \\\\\\\\ v\\\\end{bmatrix}=-(A^TA^{-1} A^Tb)$\\n    - *second moment matrix* $A^TA$ - need this to be high enough rank\\n\\n### general correspondence + interest points\\n\\n- more general **correspondence** - matching points, patches, edges, or regions across images (not in the same basic image)\\n  - most important problem - used in steropsis, optical flow, structure from motion\\n- 2 ways of finding correspondences\\n  - align and search - not really used\\n  - keypoint matching - find keypoint that matches and use everything else\\n- 3 steps to kepoint matching: detection, description, matching\\n\\n#### detection - identify key points\\n\\n- find \\'corners\\' with Harris corner detector\\n- shift small window and look for large intensity change in multiple directions\\n  - edge - only changes in one direction\\n  - compare auto-correlation of window (L2 norm of pixelwise differences)\\n- very slow naively - instead look at gradient (Taylor series expansion - second moment matrix M)\\n  - if gradient isn\\'t flat, then it\\'s a corner\\n- look at eigenvalues of M\\n  - eigenvalues tell you about magnitude of change in different directions\\n  - if same, then circular otherwise elliptical\\n  - corner - 2 large eigenvalues, similar values\\n    - edge - 1 eigenvalue larger than other\\n  - simple way to compute this: $det(M) - \\\\alpha \\\\: trace(M)^2$\\n- apply max filter to get rid of noise\\n  - adaptive - want to distribute points across image\\n- invariance properties\\n  - ignores affine intensity change (only uses derivs)\\n  - ignores translation/rotation\\n  - does not ignore scale (can fix this by considering multiple scales and taking max)\\n\\n#### description - extract vector feature for each key point\\n\\n- lots of ways - ex. SIFT, image patches wrt gradient\\n- simpler: MOPS\\n  - take point (x, y), scale (s), and orientation from gradients\\n  - take downsampled rectangle around this point in proper orientation\\n- invariant to things like shape / lighting changes\\n\\n#### matching - determine correspondence between 2 views\\n\\n- not all key points will match - only match above some threshold\\n  - ex. criteria: symmetry - only use if a is b\\'s nearest neighbor and b is a\\'s nearest neighbor\\n  - better: David Lowe trick -  how much better is 1-NN than 2-NN (e.g. threshold on 1-NN / 2-NN)\\n- problem: outliers will destroy fit\\n- **RANSAC** algorithm (random sample consensus) - vote for best transformation\\n  - repeat this lots of times, pick the match that had the most inliers\\n    - select n feature pairs at random (n = minimum needed to compute transformation - 4 for homography, 8 for rotation/translation)\\n    - compute transformation T (exact for homography, or use 8-point algorithm)\\n    - count *inliers* (how many things agree with this match)\\n      - 8-point algorithm / homography check \\n      - $x^TEx < \\\\epsilon $ for 8-point algorithm or $x^THx < \\\\epsilon$ for homography\\n  - ate end could recompute least squares H or F on all inliers\\n\\n### correspondence for sfm / instance retrieval\\n\\n- sfm (structure for motion) - given many images, simultaneously do 2 things\\n\\n  - calibration - find camera parameters\\n  - triangulation - find 3d points from 2d points\\n- **structure for motion** system (ex. photo tourism 2006 paper)\\n  - **camera calibration**: determine camera parameters from known 3d points\\n    - parameters\\n      1. internal parameters - ex. focal length, optical center, aspect ratio\\n      2. external parameters - where is the camera\\n         - only makes sense for multiple cameras\\n    - approach 1 - solve for projection matrix (which contains all parameters)\\n      - requires knowing the correspondences between image and 3d points (can use calibration object)\\n      - least squares to find points from 3x4 **projection matrix** which projects  (X, Y, Z, 1) -> (x, y, 1)\\n    - approach 2 - solve for parameters\\n      - translation T, rotation R, focal length f, principle point (xc, yc), pixel size (sx, sy) \\n        - can\\'t use homography because there are translations with changing depth\\n        - sometimes camera will just list focal length\\n      - decompose projection matrix into a matrix dependent on these things\\n      - nonlinear optimization\\n  - **triangulation** - predict 3d points $(X_i, Y_i, Z_i)$ given pixels in multiple cameras $(x_i, y_i)$ and camera parameters $R, t$\\n    - minimize reprojection error (bundle adjustment): $\\\\sum_i \\\\sum_j \\\\underbrace{w_{ij}}_{\\\\text{indicator var}}\\\\cdot || \\\\underbrace{P(x_i, R_j, t_j)}_{\\\\text{pred. im location}} - \\\\underbrace{\\\\begin{bmatrix} u_{i, j}\\\\\\\\v_{i, j}\\\\end{bmatrix}}_{\\\\text{observed m location}}||^2$\\n      - solve for matrix that projects points into 3d coords\\n- incremental sfm: start with 2 cameras\\n  - initial pair should have lots of matches, big baseline (shouldn\\'t just be a homography)\\n  - solve with essential matrix\\n  - then iteratively add cameras and recompute\\n  - good idea: ignore lots of data since data is cheap in computer vision\\n- search for similar images - want to establish correspondence despite lots of changes\\n  - see how many keypoint matches we get\\n  - search with inverted file index\\n    - ex. visual words - cluster the feature descriptors and use these as keys to a dictionary\\n    - inverted file indexing\\n    - should be sparse\\n  - *spatial verification* - don\\'t just use visual words, use structure of where the words are\\n    - want visual words to give similar transformation - RANSAC with some constraint\\n\\n## deep learning\\n\\n### cnns\\n\\n- *object recognition* - visual similarity via labels\\n- classification\\n  - linear boundary -> nearest neighbors\\n- neural nets\\n  - don\\'t need feature extraction step\\n  - high capacity (like nearest neighbors)\\n  - still very fast test time\\n  - good at high dimensional noisy inputs (vision + audio)\\n- pooling - kind of robust to exact locations\\n  - a lot like blurring / downsampling\\n  - everyone now uses maxpooling\\n- history: lenet 1998 \\n  - neurocognition (fukushima 1980) - unsupervised\\n  - convolutional neural nets (lecun et al) - supervised\\n  - alexnet 2012\\n    - used norm layers (still common?)\\n  - resnet 2015\\n    - 152 layers\\n    - 3x3s with skip layers\\n- like nonparametric - number of params is close to number of data points\\n- network representations learn a lot\\n  - zeiler-fergus - supercategories are learned to be separated, even though only given single class lavels\\n  - nearest neighbors in embedding spaces learn things like pose\\n  - can be used for transfer learning\\n- fancy architectures - not just a classifier\\n  - siamese nets\\n    - ex. want to compare two things (ex. surveillance) - check if 2 people are the same (even w/ sunglasses)\\n    - ex. connect pictures to amazon pictures\\n      - embed things and make loss function distance between real pics and amazon pics + make different things farther up to some margin\\n    - ex. searching across categories\\n  - multi-modal\\n    - ex. could look at repr. between image and caption\\n  - semi-supervised\\n    - context as supervision - from word predict neighbors\\n    - predict neighboring patch from 8 patches in image\\n  - multi-task\\n    - many tasks / many losses at once - everything will get better at once\\n  - differentiable programming - any nets that form a DAG\\n    - if there are cycles (RNN), unroll it\\n  - fully convolutional\\n    - works on different sizes \\n    - this lets us have things per pixel, not per image (ex. semantic segmentation, colorization)\\n    - usually use skip connections\\n\\n### image segmentation\\n\\n- **consistency** - 2 segmentations consistent when they can be explained by same segmentation tree\\n  - *percept tree* - describe what\\'s in an image using a tree\\n- evaluation - how to correspond boundaries?\\n  - min-cost assignment on **bipartite graph=bigraph** - connections only between groundtruth, signal: ![bigraph](../assets/bigraph.png)\\n- ex. for each pixel predict if it\\'s on a boundary by looking at window around it\\n  - proximity cue\\n  - boundary cues: brightness gradient, color gradient, texture gradient (gabor responses)\\n    - look for sharp change in the property\\n  - region cue - patch similarity\\n    - proximity\\n    - graph partitioning\\n  - learn cue combination by fitting linear combination of cues and outputting whether 2 pixels are in same segmentation\\n- graph partitioning approach: generate affinity graph from local cues above (with lots of neighbors)\\n  - *normalized cuts* - partition so within-group similarity is large and between-group similarity is small\\n- deep semantic segmentation - fully convolutional\\n  - upsampling\\n    - unpooling - can fill all, always put at top-left\\n    - *max-unpooling* - use positions from pooling layer\\n    - *learnable upsampling* = deconvolution = upconvolution = fractionally strided convolution = backward strided convolution - transpose the convolution\\n\\n### classification + localization\\n\\n- goal: coords (x, y, w, h) for each object + class\\n- simple: sliding window and use classifier each time - computationally expensive!\\n- region proposals - find blobby image regions likely to contain objects and run (fast)\\n- R-CNN - run each region of interest, warped to some size, through CNN\\n- Fast R-CNN - get ROIs from last conv layer, so everything is faster / no warping\\n  - to maintain size, fix number of bins instead of filter sizes (then these bins are adaptively sized) - spatial pyramid pooling layer\\n- Faster R-CNN - use region proposal network within network to do region proposals as well\\n  - train with 4 losses for all things needed\\n  - region proposal network uses multi-scale anchors and predicts relative to convolution\\n\\n\\n- instance segmentation\\n  - mask-rcnn - keypoint detection then segmentation\\n\\n###  learning detection\\n\\n- countour detection - predict contour after every conv (at different scales) then interpolate up to get one final output (ICCV 2015)\\n  - deep supervision helps to aggregate multiscale info\\n- semantic segmentation - sliding window\\n- classification + localization\\n  - need to output a bounding box + classify what\\'s in the box\\n  - bounding box: regression problem to output box\\n    - use locations of features\\n- feature map\\n  - location of a feature in a feature map is where it is in the image (with finer localization info accross channels)\\n  - response of a feature - what it is\\n\\n#### modeling figure-ground\\n\\n- figure is closer, ground background - affects perception\\n- figure/ground datasets\\n- local cues\\n  - edge features: shapemes - prototypical local shapes\\n  - junction features: line labelling - contour directions with convex/concave images\\n  - lots of principles\\n    - surroundedness, size, orientation, constrast, symmetry, convexity, parallelism, lower region, meaningfulness, occlusion, cast shadows, shading\\n  - global cues\\n    -  want consistency with CRF\\n      - spectral graph segmentation\\n    - embedding approach - satisfy pairwise affinities\\n\\n### single-view 3d construction\\n\\n- useful for planning, depth, etc.\\n- different levels of output (increasing complexity)\\n  - image depth map\\n  - scene layout - predict simple shapes of things\\n  - volumetric 3d - predict 3d binary voxels for which voxels are occupied\\n    - could approximate these with CAD models, deformable shape models\\n- need to use priors of the world\\n- (explicit) single-view modeling - assume a model and fit it\\n  - many classes are very difficult to model explicitly\\n  - ex. use dominant edges in a few directions to calculate vanishing points and then align things\\n- (implicit) single-view prediction - learn model of world data-driven\\n  - collect data + labels (ex. sensors)\\n  - train + predict\\n  - supervision from annotation can be wrong',\n",
       " \"---\\nlayout: notes\\ntitle: structure learning\\ncategory: ml\\n---\\n\\n#  structure learning\\n\\n## introduction\\n\\n1. *structured prediction* - have multiple independent output variables\\n  - output assignments are evaluated jointly\\n  - requires joint (global) inference\\n  - can't use classifier because output space is combinatorially large\\n  - three steps\\n    1. model - pick a model\\n    2. learning = training\\n    3. inference = testing\\n2. *representation learning* - picking features\\n  - usually use domain knowledge\\n  1. combinatorial - ex. map words to higher dimensions\\n  2. hierarchical - ex. first layers of CNN\\n\\n\\n## structure\\n\\n- structured output can be represented as a graph\\n- outputs y\\n- inputs x\\n- two types of info are useful\\n  - relationships between x and y\\n  - relationships betwen y and y\\n- complexities\\n  1. modeling - how to model?\\n  2. train - can't train separate weight vector for each inference outcome\\n  3. inference - can't enumerate all possible structures\\n- need to score nodes and edges\\n  - could score nodes and edges independently\\n  - could score each node and its edges together\\n\\n## sequential models\\n\\n![](../assets/hmm_vs_condition.png) \\n\\n### sequence models\\n- goal: learn distribution $P(x_1,...,x_n)$ for sequences $x_1,...,x_n$\\n  - ex. text generation\\n- *discrete Markov model*\\n  - $P(x_1,...,x_n) = \\\\prod_i P(x_i \\\\vert  x_{i-1})$\\n  - requires \\n    1. initial probabilites\\n    2. transition matrix\\n- *mth order Markov model* - keeps history of previous m states\\n- each state is an observation\\n\\n### conditional models and local classifiers - discriminative model\\n- conditional models = discriminative models\\n  - goal: model $P(Y\\\\vert X)$\\n  - learns the decision boundary only\\n  - ignores how data is generated (like generative models)\\n- ex. *log-linear models*\\n  - $P(\\\\mathbf{y\\\\vert x,w}) = \\\\frac{exp(w^T \\\\phi (x,y))}{\\\\sum_y' exp(w^T \\\\phi (x,y'))}$\\n  - training: $w = \\\\underset{w}{argmin} \\\\sum log \\\\: P(y_i\\\\vert x_i,w)$\\n- ex. *next-state model*\\n  - $P(\\\\mathbf{y}\\\\vert \\\\mathbf{x})=\\\\prod_i P(y_i\\\\vert y_{i-1},x_i)$\\n- ex. *maximum entropy markov model*\\n  - $P(y_i\\\\vert y_{i-1},x) \\\\propto exp( w^T \\\\phi(x,i,y_i,y_{i-1}))$\\n    - adds more things into the feature representation than HMM via $\\\\phi$\\n  - has *label bias* problem\\n    - if state has fewer next states they get high probability\\n      - effectively ignores x if $P(y_i\\\\vert y_{i-1})$ is too high\\n- ex. *conditional random fields=CRF* \\n  - a global, undirected graphical model\\n    - divide into *factors*\\n  - $P(Y\\\\vert x) = \\\\frac{1}{Z} \\\\prod_i exp(w^T \\\\phi (x,y_i,y_{i-1}))$\\n    - $Z = \\\\sum_{\\\\hat{y}} \\\\prod_i exp(w^T \\\\phi (x,\\\\hat{y_i},\\\\hat{y}_{i-1}))$\\n    - $\\\\phi (x,y) = \\\\sum_i \\\\phi (x,y_i,y_{i-1})$\\n  - prediction via Viterbi (with sum instead of product)\\n  - training\\n    - maximize log-likelihood $\\\\underset{W}{max} -\\\\frac{\\\\lambda}{2} w^T w + \\\\sum log \\\\: P(y_I\\\\vert x_I,w)$\\n    - requires inference\\n  - *linear-chain CRF* - only looks at current and previous labels\\n- ex. *structured perceptron*\\n  - HMM is a linear classifier\\n\\n## constrained conditional models\\n\\n### consistency of outputs and the value of inference\\n- ex. POS tagging - sentence shouldn't have more than 1 verb\\n- *inference*\\n  - a global decision comprising of multiple local decisions and their inter-dependencies\\n  1. local classifiers\\n  2. constraints\\n- *learning*\\n  \\n  - global - learn with inference (computationally difficult)\\n\\n### hard constraints and integer programs\\n\\n- ![](../assets/8_hard_constraints.png) \\n\\n### soft constraints\\n- ![](../assets/8_1.png) \\n\\n## inference\\n\\n- inference constructs the output given the model\\n- goal: find highest scoring state sequence\\n  \\n  - $argmax_y \\\\: score(y) = argmax_y w^T \\\\phi(x,y)$\\n- naive: score all and pick max - terribly slow\\n- *viterbi* - decompose scores over edges\\n- questions\\n  1. exact v. approximate inference\\n    - exact - search, DP, ILP\\n    - approximate = *heuristic* - Gibbs sampling, belief propagation, beam search, linear programming relaxations\\n  2. randomized v. deterministic\\n    - if run twice, do you get same answer\\n- *ILP* - integer linear programs\\n  - combinatorial problems can be written as integer linear programs\\n  - many commercial solvers and specialized solvers\\n  - NP-hard in general\\n  - special case of *linear programming* - minimizing/maximizing a linear objective function subject to a finite number of linear constraints (equality or inequality)\\n    - in general, $ c = \\\\underset{c}{argmax}\\\\: c^Tx $ subject to $Ax \\\\leq b$\\n    - maybe more constraints like $x \\\\geq 0$\\n    - the constraint matrix defines a polytype\\n    - only the vertices or faces of the polytope can be solutions\\n    - $\\\\implies$ can be solved in polynomial time\\n  - in ILP, each $x_i$ is an integer\\n  - *LP-relaxation* - drop the integer constraints and hope for the best\\n  - 0-1 ILP - $\\\\mathbf{x} \\\\in \\\\{0,1\\\\}^n$\\n  - decision variables for each label $z_A = 1$ if output=A, 0 otherwise\\n  - don't solve multiclass classification with an ILP solver (makes it harder)\\n- *belief propagation*\\n  - variable elimination\\n    1. fix an ordering of the variables\\n    2. iteratively, find the best value given previous neighbors\\n      - use DP\\n    - ex. Viterbi is max-product variable elimination\\n  - when there are loops, require approximate solution\\n    - uses *message passing* to determine marginal probabilities of each variable\\n      - message $m_{ij}(x_j)$ high means node i believes $P(x_j)$ is high\\n    - use *beam search* - keep size-limited priority queue of states\\n\\n## learning protocols\\n\\n### structural svm\\n- $\\\\underset{w}{min} \\\\: \\\\frac{1}{2} w^T w + C \\\\sum_i \\\\underset{y}{max} (w^T \\\\phi (x_i,y)+ \\\\Delta(y,y_i) - w^T \\\\phi(x_i,y_i) )$\\n\\n### empirical risk minimization\\n- subgradients\\n  - ex. $f(x) = max ( f_1(x), f_2(x))$, solve the max then compute gradient of whichever function is argmax\\n\\n### sgd for structural svm\\n- highest scoring assignment to some of the output random variables for a given input?\\n- *loss-augmented inference* - which structure most violates the margin for a given scoring function?\\n- *adagrad* - frequently updated features should get smaller learning rates\",\n",
       " '## semi-supervised learning and more\\n\\n- blog: https://dawn.cs.stanford.edu/2017/07/16/weak-supervision/\\n  - training data is hard\\n- related paper: https://www.biorxiv.org/content/early/2018/06/16/339630\\n\\n ![semi](../assets/semi.png)\\n\\n- missing self-supervised?\\n\\n- 2 keshavan papers\\n- lengerich interp paper\\n  - resample to get important neurons',\n",
       " \"---\\nlayout: notes\\ntitle: Classification\\ncategory: ml\\n---\\n\\n#  classification\\n\\n## overview\\n\\n- regressor doesn't classify well\\n  - even in binary case, outliers skew fit\\n- asymptotic classifier - assumes infinite data\\n- linear classifer $\\\\implies$ boundaries are hyperplanes\\n- *discriminative* - model $P(Y\\\\vert X)$ directly ![](../assets/j7_10.png)\\n  - usually lower bias $\\\\implies$smaller asymptotic error\\n  - slow convergence ~ $O(p)$\\n- *generative* - model $P(X, Y) = P(X\\\\vert Y) P(Y)$ ![](../assets/j7_4.png)\\n  - usually higher bias $\\\\implies$ can handle missing data\\n    - this is because we assume some underlying X\\n  - fast convergence ~ $O[\\\\log(p)]$\\n- *decision theory* - models don't require finding $p(y\\\\|x)$ at all\\n\\n\\n## binary classification\\n\\n- $\\\\hat{y} = \\\\text{sign}(\\\\theta^T x)$\\n- usually $\\\\theta^Tx$ includes b term, but generally we don't want to regularize b\\n\\n| Model               | $\\\\mathbf{\\\\hat{\\\\theta}}$ objective (minimize)                 |\\n| ------------------- | ------------------------------------------------------------ |\\n| Perceptron          | $\\\\sum_i \\\\max(0,  -y_i \\\\cdot \\\\theta^T x_i)$                   |\\n| Linear SVM          | $\\\\theta^T\\\\theta + C \\\\sum_i \\\\max(0,1-y_i \\\\cdot \\\\theta^T x_i)$ |\\n| Logistic regression | $\\\\theta^T\\\\theta + C \\\\sum_i \\\\log[1+\\\\exp(-y_i \\\\cdot \\\\theta^T x_i)]$ |\\n\\n\\n- svm, perceptron use +1/-1, logistic use 1/0\\n- *perceptron* - tries to find separating hyperplane\\n  - whenever misclassified, update w\\n  - can add in delta term to maximize margin\\n- ![](../assets/losses.png)\\n\\n## multiclass classification\\n\\n- reducing multiclass (K categories) to binary\\n  - *one-against-all*\\n    - train K binary classifiers\\n    - class i = positive otherwise negative\\n    - take max of predictions\\n  - *one-vs-one* = *all-vs-all*\\n    - train $C(K, 2)$ binary classifiers\\n    - labels are class i and class j\\n    - inference - any class can get up to k-1 votes, must decide how to break ties\\n  - flaws - learning only optimizes *local correctness*\\n- single classifier - **one hot vector encoding**\\n  - *multiclass perceptron* (Kesler)\\n    - if label=i, want $\\\\theta_i ^Tx > \\\\theta_j^T x \\\\quad \\\\forall j$\\n    - if not, update $\\\\theta_i$ and $\\\\theta_j$* accordingly\\n    - *kessler construction*\\n      - $\\\\theta =  [\\\\theta_1 ... \\\\theta_k] $\\n      - want $\\\\theta_i^T x > \\\\theta_j^T x \\\\quad \\\\forall j$\\n      - rewrite $\\\\theta^T \\\\phi (x,i) > \\\\theta^T \\\\phi (x,j) \\\\quad \\\\forall j$\\n        - here $\\\\phi (x, i)$ puts x in the ith spot and zeros elsewhere\\n        - $\\\\phi$ is often used for feature representation\\n      - define margin: \\n        $\\\\Delta (y,y') = \\\\begin{cases} \\\\delta& if \\\\: y \\\\neq y' \\\\\\\\ 0& if \\\\: y=y'\\\\end{cases}$\\n      - check if $y=\\\\text{argmax}_{y'} \\\\theta^T \\\\phi(x,y') + \\\\delta (y,y')$\\n  - multiclass SVMs (Crammer & Singer)\\n    - minimize total norm of weights s.t. true label score is at least 1 more than second best label\\n  - multinomial logistic regression = multi-class *log-linear* model (softmax on outputs)\\n    - we control the peakedness of this by dividing by stddev\\n\\n## discriminative\\n\\n### logistic regression\\n\\n- $p(Y=1|x, \\\\theta) = \\\\text{logistic}(\\\\theta^Tx)$\\n  1. assume Y ~ $\\\\text{Bernoulli}(p)$ with $p=\\\\text{logistic}(\\\\theta^Tx$)\\n  2. can solve this online with GD of ***likelihood***\\n  3. better to solve with iteratively reweighted least squares\\n- $Logit(p) = \\\\log[p / (1-p)] = \\\\theta^Tx$\\n- multiway logistic classification\\n  - assume $P(Y^k=1|x, \\\\theta) = \\\\frac{e^{\\\\theta_k^Tx}}{\\\\sum_i e^{\\\\theta_i^Tx}}$, just as arises from class-conditional exponential family distributions\\n- logistic weight change represents change in odds\\n- fitting requires penalty on weights, otherwise they might not converge (i.e. go to infinity)\\n\\n### binary models\\n\\n- probit (binary) regression\\n  - $p(Y=1|x, \\\\theta) = \\\\phi(\\\\theta^Tx)$ where $\\\\phi$ is the Gaussian CDF\\n  - pretty similar to logistic\\n- noise-OR (binary) model\\n  - consider $Y = X_1 \\\\lor X_2 \\\\lor … X_m$ where each has a probability of failing\\n  - define $\\\\theta$ to be the failure probabilities\\n  - $p(Y=1|x, \\\\theta) = 1-e^{-\\\\theta^Tx}$\\n- other (binary) exponential models\\n  - $p(Y=1|x, \\\\theta) = 1-e^{-\\\\theta^Tx}$ but x doesn't have to be binary\\n  - *complementary log-log model*: $p(Y=1|x, \\\\theta) = 1-\\\\text{exp}[e^{-\\\\theta^Tx}]$\\n\\n### decision trees / rfs - R&N 18.3; HTF 9.2.1-9.2.3\\n\\n- **importance scores**\\n  - dataset-level\\n    - for all splits where the feature was used, measure how much variance reduced (either summed or averaged over splits)\\n    - the sum of importances is scaled to 1\\n  - prediction-level: go through the splits and add up the changes (one change per each split) for each features\\n    - note: this bakes in interactions of other variables\\n    - ours: only apply rules based on this variable (all else constant)\\n    - why not perturbation based?\\n  - trees group things, which can be nice\\n  - trees are unstsable\\n- follow rules: predict based on prob distr. of points in same leaf you end up in\\n- *inductive bias*\\n  - prefer small trees\\n  - prefer tres with high IG near root\\n- good for certain types of problems\\n  - instances are attribute-value pairs\\n  - target function has discrete output values\\n  - disjunctive descriptions may be required\\n  - training data may have errors\\n  - training data may have missing attributes\\n- greedy - use statistical test to figure out which attribute is best\\n  - split on this attribute then repeat\\n- growing algorithm\\n  1. *information gain* - decrease in entropy\\n     - weight resulting branches by their probs\\n     - biased towards attributes with many values\\n     - use *GainRatio* = Gain/SplitInformation\\n       - can incorporate *SplitInformation* - discourages selection of attributes with many uniformly distributed values\\n       - sometimes SplitInformation is very low (when almost all attributes are in one category)\\n         - might want to filter using Gain then use GainRatio\\n  2. regression tree\\n     - must decide when to stop splitting and start applying linear regression\\n     - must *minimize SSE* \\n- can get stuck in local optima\\n- avoid overfitting \\n  - don't grow too deep\\n  - early stopping doesn't see combinations of useful attributes\\n  - overfit then prune - proven more succesful\\n    - *reduced-error pruning* - prune only if doesn't decrease error on validation set\\n    - *$\\\\chi^2$ pruning* - test if each split is statistically significant with $\\\\chi^2$ test\\n    - *rule post-pruning* = *cost-complexity pruning*\\n      1. infer the decision tree from the training set, growing the tree until the training data is fit as well as possible and allowing overfitting to occur.\\n      2. convert the learned tree into an equivalent set of rules by creating one rule for each path from the root node to a leaf node.\\n         - these rules are easier to work with, have no structure\\n      3. prune (generalize) each rule by removing any preconditions that result in improving its estimated accuracy.\\n      4. sort the pruned rules by their estimated accuracy, and consider them in this sequence when classifying subsequent instances.\\n- incorporating continuous-valued attributes\\n  - choose candidate thresholds which separate examples that differ in their target classification\\n  - just evaluate them all\\n- missing values\\n  - could just fill in most common value\\n  - also could assign values probabilistically\\n- differing costs\\n  - can bias the tree to favor low-cost attributes\\n    - ex. divide gain by the cost of the attribute\\n- high variance - instability - small changes in data yield changes to tree\\n- many trees\\n  - *bagging* = bootstrap aggregation - an ensemble method\\n    - *bootstrap* - resampling with replacement\\n    - training multiple models by randomly drawing new training data\\n    - bootstrap with replacement can keep the sampling size the same as the original size\\n  - *random forest* - for each split of each tree, choose from only m of the p possible features\\n    - smaller m decorrelates trees, reduces variance\\n    - RF with m=p $\\\\implies$ bagging\\n  - voting\\n    - consensus: take the majority vote\\n    - average: take average of distribution of votes\\n      - reduces variance, better for improving more variable (unstable) models\\n    - *adaboost* - weight models based on their performance\\n- [optimal classification trees](https://link.springer.com/content/pdf/10.1007%2Fs10994-017-5633-9.pdf) - simultaneously optimize all splits, not one at a time\\n\\n### svms\\n\\n- svm benefits\\n  1. *maximum margin separator* generalizes well\\n  2. *kernel trick* makes it very nonlinear\\n  3. nonparametric - can retain training examples, although often get rid of many\\n     1. at test time, can't just store w - have to store support vectors\\n- ![](../assets/svm_margin.png)\\n- $\\\\hat{y} =\\\\begin{cases}   1 &\\\\text{if } w^Tx +b \\\\geq 0 \\\\\\\\ -1 &\\\\text{otherwise}\\\\end{cases}$\\n- $\\\\hat{\\\\theta} = \\\\text{argmin} \\\\:\\\\frac{1}{2} \\\\vert \\\\vert \\\\theta\\\\vert \\\\vert ^2 \\\\\\\\s.t. \\\\: y^{(i)}(\\\\theta^Tx^{(i)}+b)\\\\geq1, i = 1,...,m$\\n  - *functional margin* $\\\\gamma^{(i)} = y^{(i)} (\\\\theta^T x +b)$\\n    - limit the size of $(\\\\theta, b)$ so we can't arbitrarily increase functional margin\\n    - function margin $\\\\hat{\\\\gamma}$ is smallest functional margin in a training set\\n  - *geometric margin* = functional margin / $\\\\vert \\\\vert \\\\theta \\\\vert \\\\vert $\\n    - if $\\\\vert \\\\vert \\\\theta \\\\vert \\\\vert =1$, then same as functional margin\\n    - invariant to scaling of w\\n  - derived from maximizing margin: $$\\\\max \\\\: \\\\gamma \\\\\\\\\\\\: s.t. \\\\: y^{(i)} (\\\\theta^T x^{(i)} + b) \\\\geq \\\\gamma, i=1,..,m\\\\\\\\ \\\\vert \\\\vert \\\\theta\\\\vert \\\\vert =1$$\\n    - difficult to solve, especially because of $\\\\vert \\\\vert w\\\\vert \\\\vert =1$ constraint\\n    - assume $\\\\hat{\\\\gamma}=1$ ~ just a scaling factor\\n    - now we are maximizing $1/\\\\vert \\\\vert w\\\\vert \\\\vert $\\n- *soft margin* classifier - lets examples fall on wrong side of decision boundary\\n  - assigns them penalty proportional to distance required to move them back to correct side\\n  - min $\\\\frac{1}{2}||\\\\theta||^2 \\\\textcolor{blue}{ + C \\\\sum_i^n \\\\epsilon_i} \\\\\\\\s.t. y^{(i)} (\\\\theta^T x^{(i)} + b) \\\\geq 1 \\\\textcolor{blue}{- \\\\epsilon_i}, i=1:m \\\\\\\\ \\\\textcolor{blue}{\\\\epsilon_i \\\\geq0, 1:m}$\\n  - large C can lead to overfitting\\n- benefits\\n  - number of parameters remains the same (and most are set to 0)\\n  - we only care about support vectors\\n  - maximizing margin is like regularization: reduces overfitting\\n- actually solve dual formulation (which only requires calculating dot product) - QP\\n- replace dot product $x_j \\\\cdot x_k$ with *kernel function* $K(x_j, x_k)$, that computes dot product in expanded feature space\\n  - linear $K(x,z) = x^Tz$\\n  - polynomial $K (x, z) = (1+x^Tz)^d$\\n  - radial basis kernel $K (x, z) = \\\\exp(-r\\\\vert \\\\vert x-z\\\\vert \\\\vert ^2)$\\n  - transforming then computing is O($m^2$), but this is just $O(m)$\\n- practical guide\\n  - use m numbers to represent categorical features\\n  - scale before applying\\n  - fill in missing values\\n  - start with RBF\\n  - valid kernel: kernel matrix is Psd\\n\\n### decision rules\\n\\n- if-thens, rule can contain ands\\n- good rules have large **support** and high accuracy (they tradeoff)\\n- decision list is ordered, decision set is not (requires way to resolve rules)\\n- most common rules: Gini - classification, variance - regression\\n- ways to learn rules\\n  - oneR - learn from a single feature\\n  - sequential covering - iteratively learn rules and then remove data points which are covered\\n    - ex rule could be learn decision tree and only take purest node\\n  - bayesian rule lists - use pre-mined frequent patterns\\n- generally more interpretable than trees, but doesn't work well for regression\\n- features often have to be categorical\\n- **rulefit**\\n    - learns a sparse linear model with the original features and also a number of new features that are decision rules\\n    - train trees and extract rules from them - these become features in a sparse linear model\\n    - feature importance becomes a little stickier....\\n\\n## generative\\n\\n### gaussian class-conditioned classifiers\\n\\n- binary case: posterior probability $p(Y=1|x, \\\\theta)$ is a sigmoid $\\\\frac{1}{1+e^{-z}}$ where $z = \\\\beta^Tx+\\\\gamma$\\n  1. multiclass extends to *softmax function*: $\\\\frac{e^{\\\\beta_k^Tx}}{\\\\sum_i e^{\\\\beta_i^Tx}}$ - $\\\\beta$s can be used for dim reduction\\n- probabilistic interpretation\\n  - assumes classes are distributed as different Gaussians\\n  - it turns out this yields posterior probability in the form of sigmoids / softmax\\n- only a linear classifier when covariance matrices are the same (**LDA**)\\n  1. otherwise a quadratic classifier (like **QDA**) - decision boundary is quadratic\\n  2. MLE for estimates are pretty intuitive\\n  3. decision boundary are points satisfying $P(C_i\\\\vert X) = P(C_j\\\\vert X)$\\n- *regularized discriminant analysis* - shrink the separate covariance matrices towards a common matrix\\n  - $\\\\Sigma_k = \\\\alpha \\\\Sigma_k + (1-\\\\alpha) \\\\Sigma$\\n- parameter estimation: treat each feature attribute and class label as random variables\\n  - assume distributions for these\\n  - for 1D Gaussian, just set mean and var to sample mean and sample var\\n- can use directions for dimensionality reduction (class-separation)\\n\\n### naive bayes classifier\\n\\n- assume multinomial Y\\n- with clever tricks, can produce $P(Y^i=1|x, \\\\eta)$ again as a softmax\\n- let $y_1,...y_l$ be the classes of Y\\n- want Posterior $P(Y\\\\vert X) = \\\\frac{P(X\\\\vert Y)(P(Y)}{P(X)}$ \\n- MAP rule - maximum a posterior rule\\n  - use prior P(Y)\\n  - given x, predict $\\\\hat{y}=\\\\text{argmax}_y P(y\\\\vert X_1,...,X_p)=\\\\text{argmax}_y P(X_1,...,X_p\\\\vert y) P(y)$\\n    - generally ignore constant denominator\\n- naive assumption - assume that all input attributes are conditionally independent given y\\n  - $P(X_1,...,X_p\\\\vert Y) = P(X_1\\\\vert Y)\\\\cdot...\\\\cdot P(X_p\\\\vert Y) = \\\\prod_i P(X_i\\\\vert Y)$ \\n- learning\\n  - learn L distributions $P(y_1),P(y_2),...,P(y_l)$\\n  - for i in 1:$\\\\vert Y \\\\vert$\\n    - learn $P(X \\\\vert y_i)$ \\n    - for discrete case we store $P(X_j\\\\vert y_i)$, otherwise we assume a prob. distr. form\\n- naive: $\\\\vert Y\\\\vert  \\\\cdot (\\\\vert X_1\\\\vert  + \\\\vert X_2\\\\vert  + ... + \\\\vert X_p\\\\vert )$ distributions\\n- otherwise: $\\\\vert Y\\\\vert \\\\cdot (\\\\vert X_1\\\\vert  \\\\cdot \\\\vert X_2\\\\vert  \\\\cdot ... \\\\cdot \\\\vert X_p\\\\vert )$\\n- smoothing - used to fill in 0s\\n  - $P(x_i\\\\vert y_j) = \\\\frac{N(x_i, y_j) +1}{N(y_j)+\\\\vert X_i\\\\vert }$ \\n  - then, $\\\\sum_i P(x_i\\\\vert y_j) = 1$\\n\\n### exponential family class-conditioned classifiers\\n\\n1. includes Gaussian, binomial, Poisson, gamma, Dirichlet\\n2. $p(x|\\\\eta) = \\\\text{exp}[\\\\eta^Tx - a(\\\\eta)] h(x)$\\n3. for classification, anything from exponential family will result in posterior probability that is logistic function of a linear function of x\\n\\n### text classification\\n\\n- bag of words - represent text as a vector of word frequencies X\\n  - remove stopwords, stemming, collapsing multiple - NLTK package in python\\n  - assumes word order isn't important\\n  - can store n-grams\\n- multivariate Bernoulli: $P(X\\\\vert Y)=P(w_1=true,w_2=false,...\\\\vert Y)$\\n- multivariate Binomial: $P(X\\\\vert Y)=P(w_1=n_1,w_2=n_2,...\\\\vert Y)$\\n  - this is inherently naive\\n- time complexity\\n  - training O(n*average\\\\_doc\\\\_length\\\\_train+$\\\\vert c\\\\vert \\\\vert dict\\\\vert $)\\n  - testing O($\\\\vert Y\\\\vert $ average\\\\_doc\\\\_length\\\\_test)\\n- implementation\\n  - have symbol for unknown words\\n  - underflow prevention - take logs of all probabilities so we don't get 0\\n  - $y = \\\\text{argmax }log \\\\:P(y) + \\\\sum_i log \\\\: P(X_i\\\\vert y)$\\n\\n## instance-based (nearest neighbors) - really discriminative\\n\\n- also called lazy learners = nonparametric models\\n- make Voronoi diagrams\\n- can take majority vote of neighbors or weight them by distance\\n- distance can be Euclidean, cosine, or other\\n  - should scale attributes so large-valued features don't dominate\\n  - *Mahalanobois* distance metric accounts for covariance between neighbors\\n  - in higher dimensions, distances tend to be much farther, worse extrapolation\\n  - sometimes need to use *invariant metrics*\\n    - ex. rotate digits to find the most similar angle before computing pixel difference\\n      - could just augment data, but can be infeasible\\n      - computationally costly so we can approximate the curve these rotations make in pixel space with the *invariant tangent line*\\n      - stores this line for each point and then find distance as the distance between these lines\\n- finding NN with *k-d* (k-dimensional) tree\\n  - balanced binary tree over data with arbitrary dimensions\\n  - each level splits in one dimension\\n  - might have to search both branches of tree if close to split\\n- finding NN with *locality-sensitive hashing*\\n  - approximate\\n  - make multiple hash tables\\n    - each uses random subset of bit-string dimensions to project onto a line\\n    - union candidate points from all hash tables and actually check their distances\\n- comparisons\\n  - error rate of 1 NN is never more than twice that of Bayes error\\n\\n\\n## likelihood calcs\\n\\n### single Bernoulli\\n\\n- $L(p) = P$[Train | Bernoulli(p)]= $P(X_1,...,X_n\\\\vert p)=\\\\prod_i P(X_i\\\\vert p)=\\\\prod_i p^{X_i} (1-p)^{1-X_i}$\\n- $=p^x (1-p)^{n-x}$ where x = $\\\\sum x_i$\\n- $\\\\log[L(p)] = \\\\log[p^x (1-p)^{n-x}]=x \\\\log(p) + (n-x) \\\\log(1-p)$\\n- $0=\\\\frac{dL(p)}{dp} = \\\\frac{x}{p} - \\\\frac{n-x}{1-p} = \\\\frac{x-xp - np+xp}{p(1-p)}=x-np$\\n- $\\\\implies \\\\hat{p} = \\\\frac{x}{n}$\\n\\n### multinomial\\n\\n- $L(\\\\theta) = P(x_1,...,x_n\\\\vert \\\\theta_1,...,\\\\theta_p) = \\\\prod_i^n P(d_i\\\\vert \\\\theta_1,...\\\\theta_p)=\\\\prod_i^n factorials \\\\cdot \\\\theta_1^{x_1},...,\\\\theta_p^{x_p}$- ignore factorials because they are always same\\n  - require $\\\\sum \\\\theta_i = 1$\\n- $\\\\implies \\\\theta_i = \\\\frac{\\\\sum_{j=1}^n x_{ij}}{N}$ where N is total number of words in all docs\\n\\n\\n\\n## boosting\\n\\n1. adaboost\\n   1. freund & schapire\\n   2. reweight data points based on errs of previous weak learners, then train new classifiers\\n   3. classify as an ensemble\\n   \\n2. gradient boosting\\n   1. leo breiman\\n   2. actually fits the residual errors made by the previous predictors\\n   \\n3. newer algorithms for gradient boosting (speed / approximations)\\n   1. xgboost (2014 - popularized around 2016)\\n      1. implementation of gradient boosted decision trees designed for speed and performance\\n      2. things like caching, etc.\\n   2. light gbm (2017)\\n      1. can get gradient of each point wrt to loss - this is like importance for point (like weights in adaboost)\\n         1. when picking split, filter out unimportant points\\n   3. Catboost (2017)\\n   \\n4. boosting with different cost function ($y \\\\in \\\\{-1, 1\\\\}$, or for $L_2$Boost, also $y \\\\in \\\\mathbb R$)\\n\\n   | Adaboost         | LogitBoost                       | $L_2$Boost           |\\n   | ---------------- | -------------------------------- | -------------------- |\\n   | $\\\\exp(y\\\\hat{y})$ | $\\\\log_2 (1 + \\\\exp(-2y \\\\hat y ))$ | $(y - \\\\hat y)^2 / 2$ |\\n\\n   \",\n",
       " '---\\nlayout: notes\\ntitle: Unsupervised\\ncategory: ml\\ntypora-copy-images-to: ../assets\\n---\\n\\n#  unsupervised\\n\\n## clustering\\n\\n- labels are not given\\n- intra-cluster distances are minimized, inter-cluster distances are maximized\\n- distance measures\\n  - symmetric D(A,B)=D(B,A)\\n  - self-similarity D(A,A)=0\\n  - positivity separation D(A,B)=0 iff A=B\\n  - triangular inequality D(A,B) <= D(A,C)+D(B,C)\\n  - ex. Minkowski Metrics $d(x,y)=\\\\sqrt[r]{\\\\sum \\\\vert x_i-y_i\\\\vert ^r}$\\n    - r=1 Manhattan distance\\n    - r=1 when y is binary -> Hamming distance\\n    - r=2 Euclidean\\n    - r=$\\\\infty$ \"sup\" distance\\n- correlation coefficient - unit independent\\n- edit distance\\n\\n### hierarchical\\n\\n- two approaches:\\n    1. bottom-up agglomerative clustering - starts with each object in separate cluster then joins\\n    2. top-down divisive - starts with 1 cluster then separates\\n- ex. starting with each item in its own cluster, find best pair to merge into a new cluster\\n    - repeatedly do this to make a tree (dendrogram)\\n- distances between clusters defined by *linkage function*\\n  - single-link - closest members (long, skinny clusters)\\n  - complete-link - furthest members  (tight clusters)\\n  - average - most widely used\\n- ex. MST - keep linking shortest link\\n- *ultrametric distance* - tighter than triangle inequality\\n    - $d(x, y) \\\\leq \\\\max[d(x,z), d(y,z)]$\\n\\n### partitional\\n\\n- partition n objects into a set of K clusters (must be specified)\\n- globally optimal: exhaustively enumerate all partitions\\n- minimize sum of squared distances from cluster centroid\\n- evaluation w/ labels - purity - ratio between dominant class in cluster and size of cluster\\n- k-means++ - better at not getting stuck in local minima\\n  - randomly move centers apart\\n- Complexity: $O(n^2p)$ for first iteration and then can only get worse\\n\\n### statistical clustering (j 10)\\n\\n- *latent vars* - values not specified in the observed data\\n- \\n  ![](../assets/j10_1.png)\\n\\n- *K-Means*\\n  - start with random centers\\n  - E: assign everything to nearest center: $O(\\\\|\\\\text{clusters}\\\\|*np) $\\n  - M: recompute centers $O(np)$ and repeat until nothing changes\\n  - partition amounts to Voronoi diagram\\n  - can be viewed as minimizing *distortion measure* $J=\\\\sum_n \\\\sum_i z_n^i ||x_n - \\\\mu_i||^2$\\n- *GMMs*: $p(x|\\\\theta) = \\\\underset{i}{\\\\Sigma} \\\\pi_i \\\\mathcal{N}(x|\\\\mu_i, \\\\Sigma_i)$\\n\\n  - $l(\\\\theta|x) = \\\\sum_n \\\\log \\\\: p(x_n|\\\\theta) \\\\\\\\ = \\\\sum_n \\\\log \\\\sum_i \\\\pi_i \\\\mathcal{N}(x_n|\\\\mu_i, \\\\Sigma_i)$\\n  - hard to maximize bcause log acts on a sum\\n\\n  - \"soft\" version of K-means - update means as weighted sums of data instead of just normal mean\\n  - sometimes initialize K-means w/ GMMs\\n\\n### conditional mixture models - regression/classification (j 10)\\n\\n```mermaid\\ngraph LR;\\n  X-->Y;\\n  X --> Z\\n  Z --> Y\\n```\\n\\n- ex. ![](../assets/j5_16.png)\\n- latent variable Z has multinomial distr.\\n  - *mixing proportions*: $P(Z^i=1|x, \\\\xi)$\\n    - ex. $ \\\\frac{e^{\\\\xi_i^Tx}}{\\\\sum_je^{\\\\xi_j^Tx}}$\\n  - *mixture components*: $p(y|Z^i=1, x, \\\\theta_i)$ ~ different choices\\n  - ex. mixture of linear regressions\\n    - $p(y| x, \\\\theta) = \\\\sum_i \\\\underbrace{\\\\pi_i (x, \\\\xi)}_{\\\\text{mixing prop.}} \\\\cdot \\\\underbrace{\\\\mathcal{N}(y|\\\\beta_i^Tx, \\\\sigma_i^2)}_{\\\\text{mixture comp.}}$\\n  - ex. mixtures of logistic regressions\\n    - $p(y|x, \\\\theta_i) = \\\\underbrace{\\\\pi_i (x, \\\\xi)}_{\\\\text{mixing prop.}} \\\\cdot \\\\underbrace{\\\\mu(\\\\theta_i^Tx)^y\\\\cdot[1-\\\\mu(\\\\theta_i^Tx)]^{1-y}}_{\\\\text{mixture comp.}}$ where $\\\\mu$ is the logistic function\\n- also, nonlinear optimization for this (including EM)\\n\\n\\n## dim reduction\\n\\n| Method              | Analysis objective | Temporal smoothing | Explicit noise model | Notes |\\n|---------------------|--------------------|--------------------|----------------------|---------------------|\\n| PCA                 | Covariance         | No                 | No                   | orthogonality |\\n| FA                  | Covariance         | No                 | Yes                  | like PCA, but with errors (not biased by variance) |\\n| LDS/GPFA            | Dynamics           | Yes                | Yes                  |  |\\n| NLDS                | Dynamics           | Yes                | Yes                  |  |\\n| LDA                 | Classification     | No                 | No                   |  |\\n| Demixed             | Regression         | No                 | Yes/No               |  |\\n| Isomap/LLE          | Manifold discovery | No                 | No                   |  |\\n| T-SNE               | ....               | ....               | ...                  |  |\\n| UMAP                | ...                | ...                | ...                  |  |\\n\\n- NMF - $\\\\min_{D \\\\geq 0, A \\\\geq 0} \\\\|\\\\|X-DA\\\\|\\\\|_F^2$\\n  - SEQNMF\\n- ICA\\n  - remove correlations and higher order dependence\\n  - all components are equally important\\n  - like PCA, but instead of the dot product between components being 0, the mutual info between components is 0\\n  - goals\\n    - minimize statistical dependence between components\\n    - maximize information transferred in a network of non-linear units\\n    - uses information theoretic unsupervised learning rules for neural networks\\n  - problem - doesn\\'t rank features for us\\n- LDA/QDA - finds basis that separates classes\\n  - reduced to axes which separate classes (perpendicular to the boundaries)\\n- K-means - can be viewed as a linear decomposition\\n\\n### spectral clustering\\n\\n- *spectral* clustering - does dim reduction on eigenvalues (spectrum) of similarity matrix before clustering in few dims\\n  - uses adjacency matrix\\n  - basically like PCA then k-means\\n  - performs better with regularization - add small constant to the adjacency matrix\\n\\n### pca\\n\\n- want new set of axes (linearly combine original axes) in the direction of greatest variability\\n    - this is best for visualization, reduction, classification, noise reduction\\n    - assume $X$ (nxp) has zero mean\\n\\n- derivation: \\n\\n    - minimize variance of X projection onto a unit vector v\\n      - $\\\\frac{1}{n} \\\\sum (x_i^Tv)^2 = \\\\frac{1}{n}v^TX^TXv$ subject to $v^T v=1$\\n      - $\\\\implies v^T(X^TXv-\\\\lambda v)=0$: solution is achieved when $v$ is eigenvector corresponding to largest eigenvalue\\n    - like minimizing perpendicular distance between data points and subspace onto which we project\\n\\n- SVD: let $U D V^T = SVD(Cov(X))$\\n\\n    - $Cov(X) = \\\\frac{1}{n}X^TX$, where X has been demeaned\\n\\n- equivalently, eigenvalue decomposition of covariance matrix $\\\\Sigma = X^TX$\\n  - each eigenvalue represents prop. of explained variance: $\\\\sum \\\\lambda_i = tr(\\\\Sigma) = \\\\sum Var(X_i)$\\n\\n  - *screeplot*  - eigenvalues in decreasing order, look for num dims with kink\\n    - don\\'t automatically center/normalize, especially for positive data\\n\\n- SVD is easier to solve than eigenvalue decomposition, can also solve other ways\\n  1. multidimensional scaling (MDS)\\n    - based on eigenvalue decomposition\\n  2. adaptive PCA\\n    - extract components sequentially, starting with highest variance so you don\\'t have to extract them all\\t\\n\\n- good PCA code: http://cs231n.github.io/neural-networks-2/\\n```python\\nX -= np.mean(X, axis = 0) ## zero-center data (nxd)\\ncov = np.dot(X.T, X) / X.shape[0] ## get cov. matrix (dxd)\\nU, D, V = np.linalg.svd(cov) ## compute svd, (all dxd)\\nXrot_reduced = np.dot(X, U[:, :2]) ## project onto first 2 dimensions (n x 2)\\n```\\n- nonlinear pca\\n    - usually uses an auto-associative neural network\\n\\n### topic modeling\\n\\n- similar, try to discover topics in a model (which maybe can be linearly combined to produce the original document)\\n\\n- ex. LDA - generative model: posits that each document is a mixture of a **small number of topics** and that **each word\\'s presence is attributable to one of the document\\'s topics**\\n\\n### sparse coding = sparse dictionary learning\\n\\n$$\\\\underset {\\\\mathbf{D}} \\\\min \\\\underset t \\\\sum \\\\underset {\\\\mathbf{a^{(t)}}} \\\\min ||\\\\mathbf{x^{(t)}} - \\\\mathbf{Da^{(t)}}||_2^2 + \\\\lambda ||\\\\mathbf{a^{(t)}}||_1$$\\n\\n- D is like autoencoder output weight matrix\\n- $a$ is more complicated - requires solving inner minimization problem\\n- outer loop is not quite lasso - weights are not what is penalized\\n- impose norm $D$ not too big\\n- algorithms\\n  - thresholding (simplest) - do $D^Ty$ and then threshold this\\n  - basis pursuit - change $l_0$ to $l_1$\\n    - this will work under certain conditions (with theoretical guarantees)\\n  - matching purusuit - greedy, find support one at a time, then look for the next one\\n\\n### ica\\n\\n- goal: want to decompose $X$ into $z$, where we assume $X = Az$\\n  - assumptions\\n    - independence: $P(z) = \\\\prod_i P(z_i)$\\n    - non-gaussianity of $z$\\n  - 2 ways to get $z$ which matches these assumptions\\n    1. maximize non-gaussianity of $z$ - use kurtosis, negentropy\\n    2. minimize mutual info between components of $z$ - use KL, max entropy\\n       1. often equivalent\\n  - identifiability: $z$ is identifiable up to a permutation ans scaling of sources when\\n    - at most one of the sources $z_k$ is gaussian\\n    - $A$ is full-rank\\n- ICA learns components which are completely independent, whereas PCA learns orthogonal components\\n- **non-linear ica**: $X \\\\approx f(z)$, where assumptions on $s$ are the same, and $f$ can be nonlinear\\n  - to obtain identifiability, we need to restrict $f$ and/or constrain the distr of the sources $s$\\n- bell & sejnowski 1995 original formulation (slightly different)\\n  - entropy maximization - try to find a nonlinear function $g(x)$ which lets you map that distr $f(x)$ to uniform\\n    - then, that function $g(x)$ is the cdf of $f(x)$\\n  - in ICA, we do this for higher dims - want to map distr of $x_1, ..., x_p$ to $y_1, ..., y_p$ where distr over $y_i$\\'s is uniform (implying that they are independent)\\n    - additionally we want the map to be information preserving\\n  - mathematically: $\\\\underset{W} \\\\max I(x; y) = \\\\underset{W} \\\\max H(y)$ since $H(y|x)$ is zero (there is no randomness)\\n    - assume $y = \\\\sigma (W x)$ where $\\\\sigma$ is elementwise\\n    - (then S = WX, $W=A^{-1}$)\\n    - requires certain assumptions so that $p(y)$ is still a distr: $p(y) = p(x) / |J|$ where J is Jacobian\\n  - learn W via gradient ascent $\\\\Delta W \\\\propto \\\\partial / \\\\partial W (\\\\log |J|)$\\n    - there is now something faster called fast ICA\\n- topographic ICA (make nearby coefficient like each other)\\n- interestingly, some types of self-supervised learning perform ICA assuming certain data structure (e.g. time-contrastive learning (hyvarinen et al. 2016))\\n\\n### topological\\n\\n- **multidimensional scaling (MDS)**\\n  - given a a distance matrix, MDS tries to recover low-dim coordinates s.t. distances are preserved\\n  - minimizes goodness-of-fit measure called *stress* = $\\\\sqrt{\\\\sum (d_{ij} - \\\\hat{d}_{ij})^2 / \\\\sum d_{ij}^2}$\\n  - visualize in low dims the similarity between individial points in high-dim dataset\\n  - classical MDS assumes Euclidean distances and uses eigenvalues\\n    - constructing configuration of n points using distances between n objects\\n    - uses distance matrix\\n      - $d_{rr} = 0$\\n      - $d_{rs} \\\\geq 0$\\n    - solns are invariant to translation, rotation, relfection\\n    - solutions types\\n      1. non-metric methods - use rank orders of distances\\n         - invariant to uniform expansion / contraction\\n      2. metric methods - use values\\n    - D is *Euclidean* if there exists points s.t. D gives interpoint Euclidean distances\\n      - define B = HAH\\n        - D Euclidean iff B is psd\\n- **t-sne** preserves pairwise neighbors\\n  - [t-sne tutorial](https://distill.pub/2016/misread-tsne/)\\n  - t-sne tries to match pairwise distances between the original data and the latent space data: ![Screen Shot 2020-09-11 at 12.35.35 AM](../assets/tsne.png)\\n  - original data\\n    - distances are converted to probabilities by assuming points are means of Gaussians, then normalizing over all pairs\\n      - variance of each Gaussian is scaled depending on the desired perplexity\\n  - latent data\\n    - distances are calculated using some kernel function\\n      - t-SNE uses heavy-tailed Student\\'s t-distr kernel (van der Maaten & Hinton, 2008)\\n      - SNE use Gausian kernel (Hinton & Roweis, 2003)\\n    - kernels have some parameters that can be picked or learned\\n    - **perplexity** - how to balance between local/global aspects of data\\n  - optimization - for optimization purposes, this can be decomposed into attractive/repulsive forces \\n- **umap**: Uniform Manifold Approximation and Projection for Dimension Reduction\\n\\t- [umap tutorial](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html) \\n\\n## generative models\\n\\n- overview: https://blog.openai.com/generative-models/\\n- notes for [deep unsupervised learning](https://sites.google.com/view/berkeley-cs294-158-sp20/home)\\n- MLE equivalent to minimizing KL for density estimation:\\n  - $\\\\min_\\\\theta KL(p|| p_\\\\theta) =\\\\\\\\ \\\\min_\\\\theta-H(p) + \\\\mathbb E_{x\\\\sim p}[-\\\\log p_\\\\theta(x)] \\\\\\\\ \\\\max_\\\\theta E_p[\\\\log p_\\\\theta(x)]$\\n\\n### autoregressive models\\n\\n- model input based on input\\n  - $p(x_1)$ is a histogram (learned prior)\\n  - $p(x_2|x_1)$ is a distr. ouptut by a neural net (output is logits, followed by softmax)\\n  - all conditional distrs. can be given by neural net\\n- can model using an RNN: e.g. char-rnn (karpathy, 2015): $\\\\log p(x) - \\\\sum_i \\\\log p(x_i | x_{1:i-1})$, where each $x_i$ is a character\\n- can also use masks\\n  - masked autoencoder for distr. estimation - mask some weights so that autoencoder output is a factorized distr\\n    - pick an odering for the pixels to be conditioned on\\n  - ex. 1d masked convolution on wavenet (use past points to predict future points)\\n  - ex. pixelcnn - use masking for pixels to the topleft\\n  - ex. gated pixelcnn - fixes issue with blindspot\\n  - ex. pixelcnn++ - nearby pixel values are likely to cooccur\\n  - ex. pixelSNAIL - uses attention and can get wider receptive field\\n  - **attention:**$A(q, K, V) = \\\\sum_i \\\\frac{\\\\exp(q \\\\cdot k_i)}{\\\\sum_j \\\\exp (q \\\\cdot k_j)} v_i$\\n    - masked attention can be more flexible than masked convolution\\n  - can do super resolution, hierarchical autoregressive model\\n- problems\\n  - slow - have to sample each pixel (can speed up by caching activations)\\n    - can also speed up by assuming some pixels conditionally independent\\n- hard to get a latent reprsentation\\n  - can use **Fisher score** $\\\\nabla_\\\\theta \\\\log p_\\\\theta (x)$\\n\\n\\n\\n#### flow models\\n\\n- good intro to implementing invertible neural networks: https://hci.iwr.uni-heidelberg.de/vislearn/inverse-problems-invertible-neural-networks/\\n  - input / output dimension need to have same dimension\\n  - we can get around this by padding one of the dimensions with noise variables (and we might want to penalize these slightly during training)\\n- [normalizing flows](https://arxiv.org/pdf/1908.09257.pdf)\\n- ultimate goal: a likelihood-based model with\\n  - fast sampling\\n  - fast inference (evaluating the likelihood)\\n  - fast training\\n  - good samples\\n  - good compression\\n- transform some $p(x)$ to some $p(z)$\\n  - $x \\\\to z = f_\\\\theta (x)$, where $z \\\\sim p_Z(z)$\\n  - $p_\\\\theta (x) dx = p(z)dz$\\n  - $p_\\\\theta(x) = p(f_\\\\theta(x))|\\\\frac {\\\\partial f_\\\\theta (x)}{\\\\partial x}|$\\n- autoregressive flows\\n  - map $x\\\\to z$ invertible\\n    - $x \\\\to z$ is same as log-likelihood computation\\n    - $z\\\\to x$ is like sampling\\n  - end up being as deep as the number of variables\\n- realnvp (dinh et al. 2017) - can couple layers to preserve invertibility but still be tractable\\n  - downsample things and have different latents at different spatial scales\\n- other flows\\n  - flow++\\n  - glow\\n  - FFJORD - continuous time flows\\n- discrete data can be harder to model\\n  - **dequantization** - add noise (uniform) to discrete data\\n\\n\\n### vaes\\n\\n- [intuitively understanding vae](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)\\n- [VAE tutorial](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\\n  - minimize $\\\\mathbb E_{q_\\\\phi(z|x)}[\\\\log p_\\\\theta(x|z)- D_{KL}(q_\\\\phi(z|x)\\\\:||\\\\:p(z))]$\\n    - want latent $z$ to be standard normal - keeps the space smooth\\n  - hard to directly calculate $p(z|x)$, since it includes $p(x)$, so we approximate it with the variational posterior $q_\\\\phi (z|x)$, which we assume to be Gaussian\\n  - goal: $\\\\text{argmin}_\\\\phi KL(q_\\\\phi(z|x) \\\\:|| \\\\:p(z|x))$\\n    - still don\\'t have acess to $p(x)$, so rewrite $\\\\log p(x) = ELBO(\\\\phi) + KL(q_\\\\phi(z|x) \\\\: || \\\\: p(z|x))$\\n    - instead of minimizing $KL$, we can just maximize the $ELBO=\\\\mathbb E_q [\\\\log p(x, z)] - \\\\mathbb E_q[\\\\log q_\\\\phi (z|x)]$\\n  - **mean-field variational inference** - each point has its own params (e.g. different encoder DNN) vs **amortized inference** - same encoder for all points\\n- [pyro explanation](https://pyro.ai/examples/vae.html)\\n  - want large evidence $\\\\log p_\\\\theta (\\\\mathbf x)$ (means model is a good fit to the data)\\n  - want good fit to the posterior $q_\\\\phi(z|x)$\\n- just an autoencoder where the middle hidden layer is supposed to be unit gaussian\\n  - add a kl loss to measure how well it maches a unit gaussian\\n    - for calculation purposes, encoder actually produces means / vars of gaussians in hidden layer rather than the continuous values....\\n  - this kl loss is not too complicated...https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf\\n- generally less sharp than GANs\\n  - uses mse loss instead of gan loss...\\n  - intuition: vaes put mass between modes while GANs push mass towards modes\\n- constraint forces the encoder to be very efficient, creating information-rich latent variables. This improves generalization, so latent variables that we either randomly generated, or we got from encoding non-training images, will produce a nicer result when decoded.\\n\\n\\n### gans\\n\\n- evaluating gans\\n  - don\\'t have explicit objective like likelihood anymore\\n  - kernel density = parzen-window density based on samples yields likelihood\\n  - inception score $IS(\\\\mathbf x) = \\\\exp(\\\\underbrace{H(\\\\mathbf y)}_{\\\\text{want to generate diversity of classes}} - \\\\underbrace{H(\\\\mathbf y|\\\\mathbf x)}_{\\\\text{each image should be distinctly recognizable}})$\\n  - **FID** - Frechet inception score works directly on embedded features from inception v3 model\\n    - embed population of images and calculate mean + variance in embedding space\\n    - measure distance between these means / variances for real/synthetic images using Frechet distance = Wasseterstein-2 distance\\n- infogan\\n  - ![infogan](../assets/infogan.png)\\n- problems\\n  - mode collapse - pick just one mode in the distr.\\n- train network to be loss function\\n- original gan paper (2014)\\n- *generative adversarial network*\\n- goal: want G to generate distribution that follows data\\n  - ex. generate good images\\n- two models\\n  - *G* - generative\\n  - *D* - discriminative\\n- G generates adversarial sample x for D\\n  - G has prior z\\n  - D gives probability p that x comes from data, not G\\n    - like a binary classifier: 1 if from data, 0 from G\\n  - *adversarial sample* - from G, but tricks D to predicting 1\\n- training goals\\n  - G wants D(G(z)) = 1\\n  - D wants D(G(z)) = 0\\n    - D(x) = 1\\n  - converge when D(G(z)) = 1/2\\n  - G loss function: $G = argmin_G log(1-D(G(Z))$\\n  - overall $\\\\min_g \\\\max_D$ log(1-D(G(Z))\\n- training algorithm\\n  - in the beginning, since G is bad, only train  my minimizing G loss function\\n\\n### self-supervised\\n\\n- basics: predict some part of the input (e.g. present from past, bottom from top, etc.)\\n  - ex. denoising autoencoder\\n  - ex. in-painting (can use adversarial loss)\\n  - ex. colorization, split-brain autoencoder\\n    - colorization in video given first frame (helps learn tracking)\\n  - ex. relative patch prediction\\n  - ex. orientation prediction\\n  - ex. nlp\\n    - word2vec\\n    - bert - predict blank word\\n- contrastive predictive coding - translates generative modeling into classification\\n  - *contrastive loss* = *InfoNCE loss* uses cross-entropy loss to measure how well the model can classify the “future” representation amongst a set of unrelated “negative” samples\\n  - negative samples may be from other batches or other parts of the input\\n- momentum contrast - queue of previous embeddings are \"keys\"\\n  - match new embedding (query) against keys and use contrastive loss\\n  - similar idea as memory bank\\n- **SimCLR** ([Chen et al, 2020](https://arxiv.org/abs/2002.05709))\\n  - maximize agreement for different points after some augmentation (contrastive loss)\\n\\n\\n\\n### semi-supervised\\n\\n- make the classifier more confident\\n  - entropy minimization - try to minimize the entropy of output predictions (like making confident predictions labels)\\n  - pseudo labeling - just take argmax pred as if it were the label\\n- label consistency with data augmentation\\n- ensembling\\n  - temporal ensembling - ensemble multiple models at different training epochs\\n  - mean teachers - learn from exponential moving average of students\\n- unsupervised data augmentation - augment and ensure prediction is the same\\n- distribution alignment - ex. cyclegan - enforce  cycle consistency = dual learning = back translation\\n  - simpler is marginal matching\\n\\n### projecting into gan latent space (=gan inversion)\\n\\n- 2 general approaches\\n  1. learn an encoder to go image -> latent space\\n  \\t- [In-Domain GAN Inversion for Real Image Editing](https://arxiv.org/pdf/2004.00049.pdf) (zhu et al. 2020) \\n  \\t- learn encoder to project image into latent space, with regularizer to make sure it follows the right distr.\\n2. optimize latent code wrt image directly\\n     1. can also learn an encoder to initialize this optimization\\n  3. some work designing GANs that are intrinsically invertible\\n  4. stylegan-specific - some works which exploit layer-wise noises\\n       1. stylegan2 paper: optimize w along with noise maps - need to make sure noise maps don\\'t include signal\\n\\n## compression\\n\\n- simplest - fixed-length code\\n- variable-length code\\n  - could append stop char to each codeword\\n  - general prefix-free code = binary tries\\n    - codeword is path from froot to leaf\\n    - ![Screen Shot 2020-04-20 at 8.50.07 PM](../assets/trie.png)\\n  - huffman code - higher prob = shorter\\n- **arithmetic coding**\\n  - motivation: coding one symbol at a time incurs penalty of +1 per symbol - more efficient to encode groups of things\\n  - can be improved with good autoregressive model',\n",
       " '---\\nlayout: notes\\ntitle: Deep Learning\\ncategory: ml\\n---\\n\\n#  deep learning\\n\\n## neural networks\\n\\n- basic perceptron update rule\\n    - if output is 0 but should be 1: raise weights on active connections by d\\n    - if output is 1 but should be 0: lower weights on active connections by d\\n- *perceptron convergence thm* - if data is linearly separable, perceptron learning algorithm wiil converge\\n- transfer / activation functions\\n    - sigmoid(z) = $\\\\frac{1}{1+e^{-z}}$\\n    - Binary step\\n    - TanH (always preferred to sigmoid)\\n    - Rectifier = ReLU\\n         - Leaky ReLU - still has some negative slope when <0\\n         - rectifying in electronics converts analog -> digital\\n    - rare to mix and match neuron types\\n- *deep* - more than 1 hidden layer\\n- regression loss = $\\\\frac{1}{2}(y-\\\\hat{y})^2$\\n- classification loss = $-y log (\\\\hat{y}) - (1-y) log(1-\\\\hat{y})$ \\n    - can\\'t use SSE because not convex here\\n- multiclass classification loss $=-\\\\sum_j y_j ln \\\\hat{y}_j$\\n- **backpropagation** - application of *reverse mode automatic differentiation* to neural networks\\'s loss\\n  - apply the chain rule from the end of the program back towards the beginning\\n    - $\\\\frac{dL}{dx_i} = \\\\frac{dL}{dz} \\\\frac{\\\\partial z}{\\\\partial x_i}$\\n    - sum $\\\\frac{dL}{dz}$ if neuron has multiple outputs z\\n    - L is output\\n  - $\\\\frac{\\\\partial z}{\\\\partial x_i}$ is actually a Jacobian (deriv each $z_i$ wrt each $x_i$ - these are vectors)\\n    - each gate usually has some sparsity structure so you don\\'t compute whole Jacobian\\n- pipeline\\n  - initialize weights, and final derivative ($\\\\frac{dL}{dL}=1$)\\n  - for each batch\\n    - run network forward to compute outputs at each step\\n    - compute gradients at each gate with backprop\\n    - update weights with SGD\\n- ![backprop](../assets/backprop.png)\\n\\n## training\\n\\n- *vanishing gradients problem* - neurons in earlier layers learn more slowly than in later layers\\n  - happens with sigmoids\\n  - dead ReLus\\n- *exploding gradients problem* - gradients are significantly larger in earlier layers than later layers\\n  - RNNs\\n- *batch normalization* - whiten inputs to all neurons (zero mean, variance of 1)\\n  - do this for each input to the next layer\\n- *dropout* - randomly zero outputs of p fraction of the neurons during training\\n  - like learning large ensemble of models that share weights\\n  - 2 ways to compensate (pick one)\\n    1. at test time multiply all neurons\\' outputs by p\\n    2. during training divide all neurons\\' outputs by p\\n- *softmax* - takes vector z and returns vector of the same length\\n  - makes it so output sums to 1 (like probabilities of classes)\\n\\n## CNNs\\n- kernel here means filter\\n- convolution G- takes a windowed average of an image F with a filter H where the filter is flipped horizontally and vertically before being applied\\n- G = H $\\\\ast$ F\\n    - if we do a filter with just a 1 in the middle, we get the exact same image\\n    - you can basically always pad with zeros as long as you keep 1 in middle\\n    - can use these to detect edges with small convolutions\\n    - can do Guassian filters\\n- convolutions typically sum over all color channels\\n- 1x1 conv - still convolves over channels\\n- pooling - usually max - doesn\\'t pool over depth\\n  - people trying to move away from this - larger strides in conversation layers\\n  - stacking small layers is generally better\\n- most of memory impact is usually from activations from each layer kept around for backdrop\\n- visualizations\\n    - layer activations (maybe average over channels)\\n    - visualize the weights (maybe average over channels)v\\n    - feed a bunch of images and keep track of which activate a neuron most\\n    - t-SNE embedding of images\\n    - occluding\\n- weight matrices have special structure (Toeplitz or block Toeplitz)\\n- input layer is usually centered (subtract mean over training set)\\n- usually crop to fixed size (square input)\\n- receptive field - input region\\n- stride m - compute only every mth pixel\\n- downsampling\\n    - max pooling - backprop error back to neuron w/ max value\\n    - average pooling - backprop splits error equally among input neurons\\n- data augmentation - random rotations, flips, shifts, recolorings\\n- siamese networks - extract features twice with same net then put layer on top\\n    - ex. find how similar to representations are\\n- famous cnns\\n\\t- LeNet (1998)\\n\\t\\t- first, used on MNIST\\n\\t- AlexNet (2012)\\n\\t\\t- landmark (5 conv layers, some pooling/dropout)\\n\\t- ZFNet (2013)\\n\\t\\t- fine tuning and deconvnet\\n\\t- VGGNet (2014)\\n\\t\\t- 19 layers, all 3x3 conv layers and 2x2 maxpooling\\n  - GoogLeNet (2015)\\n\\t  - lots of parallel elements (called *Inception module*)\\n\\t- Msft ResNet (2015)\\n        - very deep - 152 layers\\n            - connections straight from initial layers to end\\n          - only learn \"residual\" from top to bottom\\n\\t- Region Based CNNs (R-CNN - 2013, Fast R-CNN - 2015, Faster R-CNN - 2015)\\n\\t\\t- object detection\\n\\t- Karpathy Generating image descriptions (2014)\\n\\t\\t- RNN+CNN\\n\\t- Spatial transformer networks (2015)\\n\\t\\t- transformations within the network\\n\\t- Segnet (2015)\\n\\t\\t- encoder-decoder network\\n\\t- Unet (2015)\\n\\t\\t- Ronneberger - applies to biomedical segmentation\\n\\t- Pixelnet (2017)\\n        - predicts pixel-level for different tasks with the same architecture\\n        - convolutional layers then 3 FC layers which use outputs from all convolutional layrs together\\n\\t- Squeezenet\\n\\t- Yolonet\\n\\t- Wavenet\\n\\t- Densenet\\n\\t- NASNET\\n\\t- [Efficientnet (2019)](https://arxiv.org/pdf/1905.11946.pdf)\\n\\n\\n\\n## architectural components\\n\\n- **coordconv** - break translation equivariance by passing in i, j coords as extra filters\\n- **deconvolution** = transposed convolution = fractionally-strided convolution - like upsampling\\n- **attention**\\n    - attention ovw: https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\\n    - vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with (or “*attends to*” as you may have read in many papers) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.\\n    - attention [born](https://arxiv.org/pdf/1409.0473.pdf) to solve problem of going from arbitrary length -> fixed length encoding -> arbitrary length output\\n\\n## recent trends\\n- architecture search: learning to learn\\n- *optimal brain damage* - starts with fully connected and weeds out connections (Lecun)\\n- *tiling* - train networks on the error of previous networks\\n\\n## RNNs\\n\\n- feedforward NNs have no memory so we introduce recurrent NNs\\n- able to have memory\\n- could theoretically unfold the network and train with backprop\\n- truncated - limit number of times you unfold\\n- $state_{new} = f(state_{old},input_t)$\\n- ex. $h_t = tanh(W h_{t-1}+W_2 x_t)$\\n- train with backpropagation through time (unfold through time)\\n    - truncated backprop through time - only run every k time steps\\n- error gradients vanish exponentially quickly with time lag\\n\\n### LSTMs\\n- have gates for forgetting, input, output\\n- easy to let hidden state flow through time, unchanged\\n- gate $\\\\sigma$ - pointwise multiplication\\n    - multiply by 0 - let nothing through\\n    - multiply by 1 - let everything through\\n- forget gate - conditionally discard previously remembered info\\n- input gate - conditionally remember new info\\n- output gate - conditionally output a relevant part of memory\\n- GRUs - similar, merge input / forget units into a single update unit \\\\vert \\n\\n## adversarial attacks\\n\\n- **adversarial attack** - designing an input to get the wrong result from the model\\n  - **targeted** - try to get specific wrong class\\n- fast gradient step method - keep adding gradient to maximize noise (limit amplitude of pixel\\'s channel to stay imperceptible)\\n\\n## transformers\\n\\n- transformers: https://arxiv.org/pdf/1706.03762.pdf\\n  - spatial transformers: https://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf \\n- http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\\n\\n\\n\\n## misc\\n\\n- adaptive pooling can help deal with different sizes',\n",
       " '---\\nlayout: notes\\ntitle: Feature selection\\ncategory: ml\\ntypora-copy-images-to: ../assets\\n---\\n\\n#  feature selection\\n\\n## filtering - select based on summary statistic\\n\\n- ranks features or feature subsets independently of the predictor\\n- univariate methods (consider one variable at a time)\\n  - ex. variance threshold\\n  - ex. T-test of y for each variable\\n  - ex. correlation screening: pearson correlation coefficient - this can only capture linear dependencies\\n  - mutual information - covers all dependencies\\n  - ex. chi$^2$, f anova\\n- multivariate methods\\n  - features subset selection\\n  - need a scoring function\\n  - need a strategy to search the space\\n  - sometimes used as preprocessing for other methods\\n\\n## wrapper - recursively eliminate features\\n\\n- uses a predictor to assess features of feature subsets\\n- learner is considered a black-box - use train, validate, test set\\n- forward selection - start with nothing and keep adding\\n- backward elimination - start with all and keep removing\\n- others: Beam search - keep k best path at teach step, GSFS, PTA(l,r), floating search - SFS then SBS\\n\\n## embedding - select from a model\\n\\n- uses a predictor to build a model with a subset of features that are internally selected\\n- ex. lasso, ridge regression, random forest',\n",
       " \"---\\nlayout: notes\\ntitle: Learning Theory\\ncategory: ml\\n---\\n\\n#  learning theory\\n\\n\\n*references: (1) Machine Learning - Tom Mitchell, (2) An Introduction to Computational Learning Theory - Kearns & Vazirani*\\n\\n## evolution\\n\\n- performance is correlation  $Perf_D (h,c) = \\\\sum h(x) \\\\cdot c(x) \\\\cdot P(x)$\\n  - want $P(Perf_D(h,c) < Perf_D(c,c)-\\\\epsilon) < \\\\delta$\\n\\n## sample problems\\n\\n- ex: N marbles in a bag. How many draws with replacement needed before we draw all N marbles?\\n  - write $P_i = \\\\frac{N-(i-1)}{N}$ where i is number of distinct drawn marbles\\n    - transition from i to i+1 is geometrically distributed with probability $P_i$\\n    - mean times is sum of mean of each geometric\\n  - in order to get probabilities of seeing all the marbles instead of just mean[## draws], want to use Markov's inequailty\\n- box full of 1e6 marbles\\n  - if we have 10 evenly distributed classes of marbles, what is probability we identify all 10 classes of marbles after 100 draws?\\n\\n## computational learning theory\\n- frameworks\\n  1. PAC\\n  2. mistake-bound - split into b processes which each fail with probability at most $\\\\delta / b$\\n- questions\\n  1. *sample complexity* - how many training examples needed to converge\\n  2. *computational complexity* - how much computational effort needed to converge\\n  3. *mistake bound* - how many training examples will learner misclassify before converging\\n- must define convergence based on some probability\\n\\n### PAC - probably learning an approximately correct hypothesis - Mitchell\\n- want to learn C\\n  - data X is sampled with Distribution D\\n  - learner L considers set H of possible hypotheses\\n- *true error* $err_d (h)$ of hypothesis h with respect to target concept c and distribution D is the probability that h will misclassify an instance drawn at random according to D.\\n  - $err_D(h) = \\\\underset{x\\\\in D}{Pr}[c(x) \\\\neq h(x)]$\\n- getting $err_D(h)=0$ is infeasible\\n- *PAC learnable* - consider concept class C defined over set of instances X of length n and a learner L using hypothesis space H\\n  - C is PAC-learnable by L using H if for all $c \\\\in C$, distributions D over X, $\\\\epsilon$ s.t. 0 < $\\\\epsilon$ < 1/2 $\\\\delta$ s.t. $0<\\\\delta<1/2$, learner L will with probability at least $(1-\\\\delta)$ output a hypothesis $h \\\\in H$ s.t $err_D(h) \\\\leq \\\\epsilon$\\n- *efficiently PAC learnable* - *time* that is polynomial in $1/\\\\epsilon, 1/\\\\delta, n, size(c )$\\n  - *probably* - probability of failure bounded by some constant $\\\\delta$\\n  - *approximately correct* - err bounded by some constant $\\\\epsilon$\\n  - assumes H contains hypothesis with artbitraily small error for every target concept in C\\n\\n### sample complexity for finite hypothesis space - Mitchell\\n- *sample complexity* - growth in the number of training examples required\\n- *consistent learner* - outputs hypotheses that perfectly fit training data whenever possible\\n  - outputs a hypothesis belonging to the version space\\n- consider hypothesis space H, target concept c, instance distribution $\\\\mathcal{D}$, training examples D of c. The versions space $VS_{H,D}$ is *$\\\\epsilon$-exhaused* with respect to c and $\\\\mathcal{D}$ if every hypothesis h in $VS_{H,D}$ has error less than $\\\\epsilon$ with respect to c and $\\\\mathcal{D}$: $(\\\\forall h \\\\in VS_{H,D}) err_\\\\mathcal{D} (h) < \\\\epsilon$\\n\\n### rectangle learning game - Kearns\\n- data X is sampled with Distribution D\\n- simple soln: tightest-fit rectangle\\n- define region T so prob a draw misses T is $1-\\\\epsilon /4$\\n  - then, m draws miss with $(1-\\\\epsilon /4)^m$\\n    - choose m to satisfy $4(1-\\\\epsilon/4)^m \\\\leq \\\\delta$\\n\\n### VC dimension\\n- *VC dimension* measures *capacity* of a space of functions that can be learend by a statistical classification algorithm\\n  - let H be set of sets and C be a set\\n  - $H \\\\cap C := \\\\{ h \\\\cap C \\\\: \\\\vert  h \\\\in H \\\\}$\\n  - a set C is *shattered* by H if $H \\\\cap C$ contains all subsets of C\\n  - The VC dimension of $H$ is the largest integer $D$ such that there exists a set $C$ with cardinality $D$ that is shattered by $H$\\n- VC (Vapnic-Chervonenkis) dimension - if data is mapped into sufficiently high dimension, then samples will be linearly separable (N points, N-1 dims)\\n- VC dimension 0 -> hypothesis either always returns false or always returns true\\n- *Sauer's lemma* - let $d \\\\geq 0, m \\\\geq 1$, $H$ hypothesis space, VC-dim(H) = d. Then, $\\\\Pi_H(m) \\\\leq \\\\phi (d,m)$\\n- fundamental theorem of learning theory provides bound of m that guarantees learning: $m \\\\geq [\\\\frac{4}{\\\\epsilon} \\\\cdot (d \\\\cdot ln(\\\\frac{12}{\\\\epsilon}) + ln(\\\\frac{2}{\\\\delta}))]$\\n\\n## concept learning and the general-to-specific ordering\\n- definitions\\n  - *concept learning* - acquiring the definition of a general category given a sample of positive and negative training examples of the category\\n    - concept is boolean function that returns true for specific things\\n    - can represent function as vector acceptable features, ?, or null (if any null, then entire vector is null)\\n  - *general hypothesis* - more generally true\\n    - general defines a partial ordering\\n  - a hypothesis is *consistent* with the training examples if it correctly classifies them\\n  - an example x *satisfies* a hypothesis h if h(x) = 1\\n- *find-S* - finding a maximally specific hypothesis\\n   - start with most specific possible\\n    - generalize each time it fails to cover an observed positive training example\\n    - flaws\\n       - ignores negative examples\\n    - if training data is perfect, then will get answer\\n       1. no errors\\n        2. there exists a hypothesis in H that describes target concept c\\n- *version space* - set of all hypotheses consistent with the training examples\\n  - *list-then-eliminate* - list all hypotheses and eliminate any that are inconsistent (slow)\\n  - *candidate-elimination* - represent most general (G) and specific (S) members of version space\\n    - version space representation theorem - version space can be found from most general / specific version space members\\n    - for positive examples\\n      - make S more general\\n      - fix G\\n    - for negative examples\\n      - fix S\\n      - make G more specific\\n    - in general, optimal query strategy is to generate instances that satisfy exactly half the hypotheses in the current version space\\n  - testing?\\n    - classify as positive if satisfies S\\n    - classify as negative if doesn't satisfy G\\n- inductive bias of candidate-elimination - target concept c is contained in H\",\n",
       " \"---\\nlayout: notes\\ntitle: Evaluation\\ncategory: ml\\n---\\n\\n#  evaluation\\n\\n## losses\\n\\n- define a loss function $\\\\mathcal{L}$\\n  - 0-1 loss: $\\\\vert C-f(X)\\\\vert$  - hard to minimize (combinatorial)\\n  - $L_2$ loss: $[C-f(X)[^2$\\n- *risk* = $E_{(x,y)\\\\sim D}[\\\\mathcal L(f(X), y) ]$\\n- optimal classifiers\\n  - Bayes classifier minimizes 0-1 loss: $\\\\hat{f}(X)=C_i$ if $P(C_i\\\\vert X)=max_f P(f\\\\vert X)$\\n  - KNN minimizes $L_2$ loss: $\\\\hat{f}(X)=E(Y\\\\vert X)$ \\n- classification cost functions\\n  1. misclassification error - not differentiable\\n  2. Gini index: $\\\\sum_{i != j} p_i q_j$\\n  3. cross-entropy: $-\\\\sum_x p(x)\\\\: \\\\log \\\\: \\\\hat p(x) $, where $p(x)$ are usually labels and $\\\\hat p(x)$ are softmax outputs\\n     1. only penalizes target class (others penalized implicitly because of softmax)\\n     2. for binary, $- (p \\\\log \\\\hat p + (1-p) \\\\log (1-\\\\hat p)$\\n\\n## measures\\n\\n*goodness of fit* - how well does the learned distribution represent the real distribution?\\n\\n![Screen Shot 2019-06-30 at 8.27.56 PM](../assets/eval_metrics.png)\\n\\n- accuracy-based\\n  - accuracy = (TP + TN) / (P + N)\\n    - correct classifications / total number of test cases\\n  - balanced accuracy = 1/2 (TP / P + TN / N)\\n- denominator is total pos/neg\\n  - **recall** = **sensitivity** =  **true-positive rate** = TP / P = TP / (TP + FN)\\n    - what fraction of the real positives do we return?\\n  - **specificity** = true negative rate = TN / N = TN / (TN + FP)\\n    - what fraction of the real negatives do we return?\\n  - **false positive rate** = FP / N $= 1 - \\\\text{specificity}$\\n    - what fraction of the predicted negatives are wrong?\\n- fraction is total predictions\\n  - **precision** = **positive predictive value** = TP / (TP + FP)\\n    - what fraction of the prediction positives are true positives?\\n\\n  - **negative predictive value** = TN / (FN + TN)\\n    - what fraction of predicted negatives are true negatives?\\n- **F-score** is harmonic mean of precision and recall: 2 * (prec * rec) / (prec + rec)\\n- curves - easiest is often to just plot TP vs TN or FP vs FN\\n\\n  - roc curve: true-positive rate (recall) vs. false-positive rate\\n    - perfect is recall = 1, false positive rate = 0\\n  - precision-recall curve\\n  - AUC: area under (either one) of these curves - usually roc\\n\\n## comparing two things\\n\\n- odds: p1 : not p1\\n- odds ratio is a ratio of odds\\n\\n## cv\\n\\n- *cross validation* - don't have enough data for a test set\\n  - properties\\n    - not good when n < complexity of predictor\\n    - because summands are correlated\\n    - assume data units are exchangeable\\n    - can sometimes use this to pick k for k-means\\n    - data is reused\\n  - types\\n    1. k-fold - split data into N pieces\\n      - N-1 pieces for fit model, 1 for test\\n      - cycle through all N cases\\n      - average the values we get for testing\\n    2. leave one out (LOOCV)\\n      - train on all the data and only test on one\\n      - then cycle through everything\\n    3. random split - shuffle and repeat\\n    4. *one-way CV* = *prequential analysis* - keep testing on next data point, updating model\\n    5. ESCV - penalize variance between folds\\n- *regularization path* of a regression - plot each coeff v. $\\\\lambda$\\n  \\n  - tells you which features get pushed to 0 and when\\n- for OLS (and maybe other linear models), [can compute leave-one-out CV without training separate models](https://robjhyndman.com/hyndsight/crossvalidation/)\\n\\n## stability\\n1. computational stability\\n  - randomness in the algorithm\\n  - perturbations to models\\n2. generalization stability\\n  - perturbations to data\\n  - sampling methods\\n    1. *bootstrap* - take a sample\\n      - repeatedly sample from observed sample w/ replacement\\n      - bootstrap samples has same size as observed sample\\n    2. *subsampling*\\n      - sample without replacement\\n    3. *jackknife resampling*\\n      - subsample containing all but one of the points\\n\\n## other considerations\\n\\n- computational cost\\n- interpretability\\n- model-selection criteria\\n  - *adjusted $R^2_p$* - penalty \\n  - *Mallow's $C_p$*\\n  - *$AIC_p$*\\n  - *$BIC_p$*\\n  - PRESS\",\n",
       " '---\\nlayout: notes\\ntitle: Search\\ncategory: ai\\n---\\n\\n#  search\\n\\nSome notes on search based on Berkeley\\'s CS 188 course and  \"Artificial Intelligence\" Russel & Norvig 3rd Edition.\\n\\n## Uninformed Search - R&N 3.1-3.4\\n\\n### problem-solving agents\\n\\n- *goal* - 1st step\\n  - *problem* formulation - deciding what action and states to consider given a goal\\n- *uninformed* - given no info about problem besides definition\\n  - an agent with several immediate options of unknown value can decide what to do first by examining future actions that lead to states of known value\\n- 5 components\\n  1. initial state\\n  2. actions at each state\\n  3. transition model\\n  4. goal states\\n  5. path cost function\\n\\n### problems\\n\\n- toy problems\\n  1. vacuum world\\n  2. 8-puzzle (type of sliding-block puzzle)\\n  3. 8-queens problem\\n  4. Knuth conjecture\\n- real-world problems\\n  1. route-finding\\n  2. TSP (and othe touring problems)\\n  3. VLSI layout\\n  4. robot navigation\\n  5. automatic assembly sequencing\\n\\n### searching for solutions\\n\\n- start at a node and make a search tree\\n  - *frontier* = *open list* = set of all leaf nodes available for expansion at any given point\\n  - *search strategy* determines which state to expand next\\n- want to avoid redundant paths\\n  1. TREE-SEARCH - continuously expand the frontier\\n  2. GRAPH-SEARCH - tree search but also keep track of previously visited states in *explored set* = *closed set* and don\\'t revisit\\n\\n### infrastructure\\n\\n- *node* - data structure that contains parent, state, path-cost, action\\n- metrics\\n  - *complete* - terminates in finite steps\\n  - *optimal* - finds best solution\\n  - time/space complexity\\n    - theoretical CS: $\\\\vert V\\\\vert +\\\\vert E\\\\vert $\\n    - b - *branching factor* - max number of branches of any node\\n    - d - *depth* - number of steps from the root\\n    - m - *max length* of any path in the search space\\n  - *search cost* - just time/memory\\n  - *total cost* - search cost + *path cost*\\n\\n### uninformed search = blind search\\n\\n- ![](../assets/uninformed_comparisons.png)\\n- bfs\\n- *uniform-cost search* - always expand node with lowest path cost g(n)\\n  - frontier is priority queue ordered by g\\n- dfs\\n  - *backtracking search* - dfs but only one successor is generated at a time; each partially expanded node remembers which succesor to generate next\\n    - only O(m) memory instead of O(bm)\\n  - *depth-limited search*\\n    - *diameter of state space* - longest possible distance to goal from any start\\n  - *iterative deepening dfs* - like bfs explores entire depth before moving on\\n    - *iterative lengthening search* - instead of depth limit has path-cost limit\\n- *bidirectional search* - search from start and goal and see if frontiers intersect\\n  - just because they intersect doesn\\'t mean it was the shortest path\\n  - can be difficult to search backward from goal (ex. N-queens)\\n\\n## A* Search and Heuristics - R&N 3.5-3.6\\n\\n### informed search\\n\\n- *informed search* - use path costs $g(n)$ and problem-specific heuristic $h(n)$\\n  - has *evaluation function* *f* incorporating path cost *g* and heuristic *h*\\n  - *heuristic* h = estimated cost of cheapest path from state at node n to a goal state\\n- *best-first* - choose nodes with best f\\n  - *greedy best-first search* - let f = h: keep expanding node closest to goal\\n  - when f=g, reduces to uniform-cost search\\n- $A^*$ search\\n  - $f(n) = g(n) + h(n)$ represents the estimated cost of the cheapest solution through n\\n  - $A^*$ (with tree search) is optimal and complete if h(n) is *admissible*\\n    - $h(n)$ never overestimates the cost to reach the goal \\n  - $A^*$ (with graph search) is optimal and complete if h(n) is *consistent* (stronger than admissible) = *monotonicity*\\n    - $h(n) \\\\leq cost(n \\\\to n\\') + h(n\\')$\\n    - can draw contours of f (because nondecreasing)\\n  - $A^*$ is also *optimally efficient* (guaranteed to expand fewest nodes) for any given consisten heuristic because any algorithm that that expands fewer nodes runs the risk of missing the optimal solution\\n    - for a heuristic, *absolute error* $\\\\delta := h^*-h$ and *relative error* $\\\\epsilon := \\\\delta / h^*$\\n      - here $h^*$ is actual cost of root to goal\\n    - bad when lots of solutions with small absolute error because it must try them all\\n    - bad because it must store all nodes in memory\\n- memory-bounded heuristic search\\n  - *iterative-deepening* $A^*$ - iterative deepening with cutoff f-cost\\n  - *recursive best-first search* - like standard best-first search but with linear space\\n    - each node keeps f_limit variable which is best alternative path available from any ancestor\\n    - as it unwinds, each node is replaced with *backed-up value* - best f-value of its children\\n      - decides whether it\\'s worth reexpanding subtree later\\n      - often flips between different good paths (h is usually less optimistic for nodes close to the goal)\\n  - $SMA^*$ - simplified memory-bounded A* - best-first until memory is full then forgot worst leaf node and add new leaf\\n    - store forgotten leaf node info in its parent\\n    - on hard problems, too much time switching between nodes\\n- agents can also learn to search with *metalevel learning*\\n\\n### heuristic functions\\n\\n- *effective branching factor* $b^*$ - if total nodes generated by A* is N and solution depth is d, then b* is branching factor for uniform tree of depth d for N+1 nodes: $$N+1 = 1+b^* +(b^*)^2 + ... + (b^*)^d$$\\n  - want $b^*$ close to 1\\n- generally want bigger heuristic because everything with $f(n) < C^*$ will be expanded\\n  - $h_1$ dominates $h_2$ if $h_1(n) \\\\geq h_2(n) \\\\: \\\\forall \\\\: n$\\n- *relaxed problem* - removes constraints and adds edges to the graph\\n  - solution to original problem still solves relaxed problem\\n  - cost of optimal solution to a relaxed problem is an admissible heuristic for the original problem \\n    - also is consistent\\n- when there are several good heuristics, pick $h(n) = \\\\max[h_1(n), ..., h_m(n)]$ for each node\\n- *pattern database* - heuristic stores exact solution cost for every possible subproblem instance\\n  - *disjoint pattern database* - break into independent possible subproblems\\n- can learn heuristic by solving lots of problems using useful features\\n  - aren\\'t necessarily admissible / consistent\\n\\n## Local Search - R&N 4.1-4.2\\n- *local search* looks for solution not path ~ like optimization\\n  - maintains only *current node* and its neighbors\\n\\n### discrete space\\n\\n- *hill-climbing* = *greedy local search* \\n  - also stochastic hill climbing and random-restart hill climbing\\n- *simulated annealing* - pick random move\\n  - if move better, then accept\\n  - otherwise accept with some probability p\\'roportional to how bad it is and accept less as time goes on\\n- *local beam search* - pick k starts, then choose the best k states from their neighbors\\n  - *stochastic beam search* - pick best k with prob proportional to how good they are\\n- *genetic algorithms* - population of k individuals\\n  - each scored by *fitness function*\\n  - pairs are selected for *reproduction* using *crossover point*\\n  - each location subject to random *mutation*\\n  - *schema* - substring in which some of the positions can be left unspecified (ex. $246****$)\\n    - want schema to be good representation because chunks tend to be passed on together\\n\\n### continuous space\\n\\n- hill-climbing / simulated annealing still work\\n  - could just discretize neighborhood of each state\\n- use gradient\\n  - if possible, solve $\\\\nabla f  = 0$\\n  - otherwise SGD $x = x + \\\\alpha \\\\nabla f(x)$\\n    - can estimate gradient by evaluating response to small increments\\n- *line search* - repeatedly double $\\\\alpha$ until f starts to increase again\\n- *Newton-Raphson* method\\n  - finds roots of func using 1st derive: $x_\\\\text{root} = x - g(x) / g\\'(x)$\\n  - apply this on 1st deriv to get minimum\\n    - $x = x - H_f^{-1} (x) \\\\nabla f(x)$ where H is the Hessian of 2nd derivs\\n\\n## Constraint satisfaction problems - R&N 6.1-6.5\\n\\n- CSP\\n  1. set of variables $X_1, ..., X_n$\\n  2. set of domains $D_1, ..., D_n$\\n  3. set of constraints $C$ specifying allowable values\\n- each state is an *assignment* of variables\\n  - *consistent* - doesn\\'t violate constraints\\n  - *complete* - every variable is assigned\\n- *constraint graph* - nodes are variables and links connect any 2 variables that participate in a constraint\\n  - *unary constraint* - restricts value of single variable\\n  - *binary constraint*\\n  - *global constraint* - arbitrary number of variables (doesn\\'t have to be all)\\n- converting graphs to only binary constraints\\n  - every finite-domain constraint can be reduced to set of binary constraints w/ enough auxiliary variables\\n  - *dual graph* transformation - create a new graph with one variable for each constraint in the original graph and one binary constraint for each pair of original constraints that share variables\\n- also can have *preference constraints* instead of *absolute constraints*\\n\\n### inference (prunes search space before backtracking)\\n\\n- *node consistency* - prune domains violating unary constraints\\n- *arc consistency* - satisfy binary constraints (every node is made arc-consistent with all other nodes)\\n  - uses AC-3 algorithm\\n    - set of all arcs = binary constraints\\n    - pick one and apply it\\n      - if things changed, re-add all the neighboring arcs to the set\\n    - $O(cd^3)$ where $d = \\\\vert domain\\\\vert $, c = ## arcs\\n  - variable can be *generalized arc consistent*\\n- *path consistency* - consider constraints on triplets - PC-2 algorithm\\n  - extends to *k-consistency* (although path consistency assumes binary constraint networks)\\n  - *strongly k-consistent* - also (k-1) consistent, (k-2) consistent, ... 1-consistent\\n    - implies $O(k^2d)$\\n    - establishing k-consistency time/space is exponential in k\\n- global constraints can have more efficient algorithms\\n  - ex. assign different colors to everything\\n  - *resource constraint* = *atmost constraint* - sum of variable must not exceed some limit\\n    - *bounds propagation* - make sure variables can be allotted to solve resource constraint\\n\\n### backtracking\\n\\n- CSPs are *commutative* - order of choosing states doesn\\'t matter\\n- *backtracking search* - depth-first search that chooses values for one variable at a time and backtracks when no legal values left\\n  1. variable and value ordering\\n    - *minimum-remaining-values* heuristic - assign variable with fewest choices\\n    - *degree* heuristic - pick variable involved in largest number of constraints on other unassigned variables\\n    - *least-constraining-value* heuristic - prefers value that rules out fewest choices for nieghboring variables\\n  2. interleaving search and inference\\n    - *forward checking* - when we assign a variable in search, check arc-consistency on its neighbors\\n    - *maintaining arc consistency (MAC)* - when we assign a variable, call AC-3, intializing with arcs to neighbors\\n  3. intelligent backtracking - looking backward\\n    - keep track of *conflict set* for each node (list of variable assignments that deleted things from its domain)\\n    - *backjumping*  - backtracks to most recent assignment in conflict set\\n      - too simple - forward checking makes this redundant\\n    - *conflict-directed backjumping* \\n      - let $X_j$ be current variable and $conf(X_j)$ be conflict set. If every possible value for $X_j$ fails, backjump to the most recent variable $X_i$ in $conf(X_j)$ and set $conf(X_i) = conf(X_i) \\\\cup conf(X_j) - X_i$\\n    - *constraint learning* - findining minimum set of variables/values from conflict set that causes the problem = *no-good*\\n\\n### local search for csps\\n\\n- start with some assignment to variables\\n- *min-conflicts* heuristic - change variable to minimize conflicts\\n  - can escape plateaus with *tabu search* - keep small list of visited states\\n  - could use *constraint weighting*\\n\\n### structure of problems\\n\\n- connected components of constraint graph are independent subproblems\\n- *tree* - any 2 variables are connected by only one path\\n  - *directed arc consistency* - ordered variables $X_i$, every $X_i$ is consistent with each $X_j$ for j>i\\n    - tree with n nodes can be made directed arc-consisten in $O(n)$ steps - $O(nd^2)$\\n- two ways to reduce constraint graphs to trees\\n  1. assign variables so remaining variables form a tree\\n    - assigned variables called *cycle cutset* with size c\\n    - $O[d^c \\\\cdot (n-c) d^2]$\\n    - finding smallest cutset is hard, but can use approximation called *cutset conditioning*\\n  2. *tree decomposition* - view each subproblem as a mega-variable\\n    - *tree width* w - size of largest subproblem - 1\\n    - solvable in $O(n d^{w+1})$\\n- also can look at structure in variable values\\n  - ex. *value symmetry* - can assign different colorings\\n    - use *symmetry-breaking constraint* - assign colors in alphabetical order',\n",
       " '---\\nlayout: notes\\ntitle: nlp\\ncategory: ai\\ntypora-copy-images-to: ../assets\\n---\\n\\n#  nlp\\n\\nSome notes on natural language processing, focused on modern improvements based on deep learning.\\n\\n## nlp basics\\n\\n- basics come from book \"Speech and Language Processing\"\\n- **language models** - assign probabilities to sequences of words\\n  - ex. **n-gram model** - assigns probs to shorts sequences of words, known as n-grams\\n    - for full sentence, use markov assumption\\n  - eval: **perplexity (PP)** - inverse probability of the test set, normalized by the number of words (want to minimize it)\\n    - $PP(W_{test}) = P(w_1, ..., w_N)^{-1/N}$\\n    - can think of this as the weighted average branching factor of a language\\n    - should only be compared across models w/ same vocab\\n  - vocabulary\\n    - sometimes closed, otherwise have unkown words, which we assign its own symbol\\n    - can fix training vocab, or just choose the top words and have the rest be unkown\\n- **topic models (e.g. LDA)** - apply unsupervised learning on large sets of text to learn sets of associated words\\n- **embeddings** - vectors for representing words\\n  - ex. **tf-idf** - defined as counts of nearby words (big + sparse)\\n    - pointwise mutual info - instead of counts, consider whether 2 words co-occur more than we would have expected by chance\\n  - ex. **word2vec** - short, dense vectors\\n    - intuition: train classifier on binary prediction: is word $w$ likely to show up near this word? (algorithm also called skip-gram)\\n      - the weights are the embeddings\\n    - also **GloVe**, which is based on ratios of word co-occurrence probs\\n- some tasks\\n  - tokenization\\n  - pos tagging\\n  - named entity recognition\\n    - nested entity recognition - not just names (but also Jacob\\'s brother type entity)\\n  - sentiment classification\\n  - language modeling (i.e. text generation)\\n  - machine translation\\n  - hardest: coreference resolution\\n  - question answering\\n  - [natural language inference](https://www.aclweb.org/anthology/P19-1334.pdf) - does one sentence entail another?\\n- most popular datasets\\n  - (by far) WSJ\\n  - then twitter\\n  - then Wikipedia\\n- [eli5](https://eli5.readthedocs.io/en/latest/libraries/sklearn.html#library-scikit-learn) has nice text highlighting for interp\\n\\n## dl for nlp\\n\\n- some recent topics based on [this blog](http://jalammar.github.io/)\\n- rnns\\n  - when training rnn, accumulate gradients over sequence and then update all at once\\n  - **stacked rnns** have outputs of rnns feed into another rnn\\n  - bidirectional rnn - one rnn left to right and another right to left (can concatenate, add, etc.)\\n- standard seq2seq\\n  - encoder reads input and outputs context vector (the hidden state)\\n  - decoder (rnn) takes this context vector and generates a sequence\\n- misc papers\\n  - [Deal or No Deal? End-to-End Learning for Negotiation Dialogues](https://arxiv.org/abs/1706.05125) - controversial FB paper where agents \"make up their own language\"\\n\\n\\n\\n### attention / transformers\\n\\n- self-attention layer [implementation](https://github.com/mertensu/transformer-tutorial) and [mathematics](https://homes.cs.washington.edu/~thickstn/docs/transformers.pdf)\\n\\n- **self-attention ** - layer that lets word learn its relation to other layers\\n  - for each word, want score telling how much importance to place on each other word (queries $\\\\cdot$ keys)\\n  - we get an encoding for each word\\n    - the encoding of each word returns a weighted sum of the values of the words (the current word gets the highest weight)\\n    - softmax this and use it to do weighted sum of values![Screen Shot 2019-08-17 at 2.51.53 PM](../assets/attention.png)\\n  - (optional) implementation details\\n    - **multi-headed attention** - just like having many filters, get many encodings for each word\\n      - each one can take input as the embedding from the previous attention layer\\n    - **position vector** - add this into the embedding of each word (so words know how far apart they are) - usually use sin/cos rather than actual position number\\n    - **padding mask** - add zeros to the end of the sequence\\n    - **look-ahead mask** - might want to mask to only use previous words (e.g. if our final task is decoding)\\n    - **residual + normalize** - after self-attention layer, often have residual connection to previous input, which gets added then normalized\\n  - decoder - each word only allowed to attend to previous positions\\n  - 3 components\\n    - queries\\n    - keys\\n    - values\\n- **attention**\\n  - encoder reads input and ouputs context vector after each word\\n  - decoder at each step uses a different weighted combination of these context vectors\\n    - specifically, at each step, decoder concatenates its hidden state w/ the attention vector (the weighted combination of the context vectors)\\n    - this is fed to a feedforward net to output a word![Screen Shot 2019-04-11 at 7.57.14 PM](../assets/nmt.png)\\n- **transformer**\\n  - uses many self-attention layers\\n  - many stacked layers in encoder + decoder (not rnn: self-attention + feed forward)\\n  - details\\n    - initial encoding: each word -> vector\\n    - each layer takes a list of fixed size (hyperparameter e.g. length of longest sentence) and outputs a list of that same fixed size (so one output for each word)\\n      - can easily train with a masked word to predict the word at the predicted position in the encoding\\n  - multi-headed attention has several of each of these (then just concat them)\\n- recent papers\\n  - [attention is all you need paper](<https://arxiv.org/abs/1706.03762>) - proposes transformer\\n  - [Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432) (by [Andrew Dai](https://twitter.com/iamandrewdai) and [Quoc Le](https://twitter.com/quocleix))\\n  -  [ELMo](https://arxiv.org/abs/1802.05365) (by [Matthew Peters](https://twitter.com/mattthemathman) and researchers from [AI2](https://allenai.org/) and [UW CSE](https://www.engr.washington.edu/about/bldgs/cse)) - no word embeddings - train embeddings w/ bidirectional lstm (on language modelling)\\n    - context vector is weighted sum of context vector at each word\\n  - [ULMFiT](https://arxiv.org/abs/1801.06146) (by fast.ai founder [Jeremy Howard](https://twitter.com/jeremyphoward) and [Sebastian Ruder](https://twitter.com/seb_ruder)), the\\n  - [OpenAI transformer](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (by OpenAI researchers [Radford](https://twitter.com/alecrad), [Narasimhan](https://twitter.com/karthik_r_n), [Salimans](https://twitter.com/timsalimans), and [Sutskever](https://twitter.com/ilyasut))\\n  - [BERT](BERT) - semi-supervised learning (predict masked word - this is bidirectional) + supervised finetuning\\n  - [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (small released model, full trained model, even larger model from Nvidia)\\n  - [XLNet](https://arxiv.org/abs/1906.08237)\\n  - [roberta](https://arxiv.org/abs/1907.11692)\\n- these ideas are [starting to be applied to vision cnns](https://arxiv.org/abs/1904.09925)',\n",
       " '---\\nlayout: notes\\ntitle: fairness\\ncategory: ai\\ntypora-copy-images-to: ../assets\\n---\\n\\n*Some notes on algorithm fairness and STS.**\\n\\n#  fairness\\n\\n## fairness metrics\\n\\n- good introductory [blog](https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb)\\n- causes of bias\\n  - skewed sample\\n  - tainted examples\\n  - selectively limited features\\n  - sample size disparity\\n  - proxies of sensitive attributes\\n- definitions\\n  - **unawareness** - don\\'t show sensitive attributes\\n    - flaw: other attributes can still signal for it\\n  - group fairness\\n    - **demographic parity** - mean predictions for each group should be approximately equal\\n      - flaw: means might not be equal\\n    - **equalized odds** - predictions are independent of group given label\\n      - equality of opportunity: $p(\\\\hat y=1|y=1)$ is same for both groups\\n    - **predictive rate parity** - Y is independent of group given prediction\\n  - **individual fairness** - similar individuals should be treated similarly\\n  - **counterfactual fairness** - replace attributes w/ flipped values\\n- fair algorithms\\n  - preprocessing - remove sensitive information\\n  - optimization at training time - add regularization\\n  - postprocessing - change thresholds to impose fairness\\n\\n\\n\\n## [fairness in cv (tutorial)](https://sites.google.com/view/fatecv-tutorial/schedule)\\n\\n### [Computer vision in practice: who is benefiting and who is being harmed?](https://www.youtube.com/watch?time_continue=546&v=0sBE5OyD7fk&feature=emb_logo)\\n\\n- **timnit gebru** - fairness team at google\\n  - also emily denton\\n- startups\\n  - faceception startup - profile people based on their image\\n  - hirevue startup videos - facial recognition for judging interviews\\n  - clearview ai - search all faces\\n  - police using facial recognition - harms protestors\\n  - facial recognition rarely has good uses\\n  - contributes to mass surveillance\\n  - can be used to discriminate different ethnicities (e.g. Uighurs in china)\\n- gender shades work - models for gender classification were worse for black women\\n  - datasets were biased - PPB introduced to balance things somewhat\\n- gender recognition is harmful in the first place\\n  - collecting data without consent is also harmful\\n- [letter to amazon](https://www.theverge.com/2019/4/3/18291995/amazon-facial-recognition-technology-rekognition-police-ai-researchers-ban-flawed): stop selling facial analysis technology\\n- combating this technology\\n  - fashion for fooling facial recognition\\n\\n### [data ethics](https://www.youtube.com/watch?v=v_XBJd1Fxqc&feature=emb_logo)\\n\\n- different types of harms\\n  - sometimes you need to make sure there aren\\'t disparate error rates across subgroups\\n  - sometimes the task just should not exist\\n  - sometimes the manner in which the tool is used is problematic because of who has the power\\n- technology amplifies our intent\\n- most people feel that data collection is the most important place to intervene\\n- people are denied housing based on data-driven discrimination\\n- collecting data\\n  - wild west - just collect everything\\n  - curatorial data - collect very specific data (this can help mitigate bias)\\n- datasets are value-laden, drive research agendas\\n- ex. celeba labels gender, attractiveness\\n- ex. captions use gendered language (e.g. beautiful)\\n\\n### [where do we go?](https://www.youtube.com/watch?v=vpPpwa7W93I&feature=emb_logo)\\n\\n- technology is not value-neutral -- it\\'s political\\n- model types and metrics embed values\\n- science is not *neutral, objecive, perspectiveless*\\n- be aware of your own **positionality**\\n- concrete steps\\n  - ethics-informed model evaluations (e.g. disaggregegated evaluations, counterfactual testing)\\n  - recognize limitations of technical approaches\\n  - transparent dataset documentation\\n  - think about perspectives of marginalized groups\\n\\n### misc papers\\n\\n- [Large image datasets: A pyrrhic win for computer vision?](https://openreview.net/pdf?id=s-e2zaAlG3I) - bias in imagenet / tiny images\\n- http://positivelysemidefinite.com/2020/06/160k-students.html\\n\\n## concrete harms\\n\\nTechnologies, especially world-shaping technologies like CNNs, are never objective. Their existence and adoption change the world in terms of \\n\\n- consolidation of power (e.g. facial-rec used to target Uighurs, increased rationale for amassing user data)\\n- a shift toward the quantitative (which can lead to the the type of click-bait extremization we see online)\\n- automation (low-level layoffs, which also help consolidate power to tech giants)\\n- energy usage (the exorbitant footprint of models like GPT-3)\\n- access to media (deepfakes, etc.)\\n- a lot more\\n\\n\\n\\n### pandemic\\n\\nI hope the pandemic, which has boosted the desire for tracking, does not result in a long-term arc towards more serveillance\\n\\n\\n\\n- from [here](https://www.theatlantic.com/magazine/archive/2020/09/china-ai-surveillance/614197/): City Brain would be especially useful in a pandemic. (One of Alibaba’s sister companies created the app that color-coded citizens’ disease risk, while silently sending their health and travel data to police.) As Beijing’s outbreak spread, some malls and restaurants in the city began scanning potential customers’ phones, pulling data from mobile carriers to see whether they’d recently traveled. Mobile carriers also sent municipal governments lists of people who had come to their city from Wuhan, where the coronavirus was first detected. And Chinese AI companies began making networked facial-recognition helmets for police, with built-in infrared fever detectors, capable of sending data to the government. City Brain could automate these processes, or integrate its data streams.\\n- \"The pandemic may even make people value privacy less, as one early poll in the U.S. suggests\"\\n\\n\\n\\n## ethics\\n\\n- [Moral Trade](https://www.fhi.ox.ac.uk/wp-content/uploads/moral-trade-1.pdf) (ord 2015) - **moral trade** = trade that is made possible by differences in the parties\\' moral views\\n- examples\\n  - one trading their eating meat for another donating more to a certain charity they both believe in\\n  - donating to/against political parties\\n  - donating to/against gun lobby\\n  - donating to/for pro-life lobby\\n  - paying non-profit employees\\n- benefits\\n  - can yield Pareto improvements = strict improvements where something gets better while other things remain atleast constant\\n- real-world examples\\n  - vote swapping (i.e. in congress)\\n  - vote swapping across states/regions (e.g. Nader Trader, VotePair) - ruled legal when money not involved\\n  - election campaign donation swapping - repledge.com (led by eric zolt) - was taken down due to issues w/ election financing\\n- issues\\n  - factual trust - how to ensure both sides carry through? (maybe financial penalties or audits could solve this)\\n  - counterfactual trust - would one party have given this up even if the other party hadn\\'t?\\n- minor things\\n  - fits most naturally with moral framework of consequentalism\\n  - includes indexicals (e.g. prioritizing one\\'s own family)\\n  - could have uneven pledges\\n\\n## sts\\n\\n- **social determinism** - theory that social interactions and constructs alone determine individual behavior\\n\\n- **technological determinism** - theory that assumes that a society\\'s technology determines the development of its social structure and cultural values\\n\\n- [do artifacts have politics?](https://www.cc.gatech.edu/~beki/cs4001/Winner.pdf) (winner 2009)\\n    - **politics** - arrangements of power and authority in human associations as well as the activitites that take place within those arrangements\\n    - **technology** -  smaller or larger pieces or systems of hardware of a specific kind. \\n    - examples\\n      - pushes back against social determinism - technologies have the ability to shift power\\n      - ex: nuclear power (consolidates power) vs solar power (democratizes power)\\n      - ex. tv enables mass advertising\\n      - ex. low bridges prevent buses\\n      - ex. automation removes the need for skilled labor\\n        - ex. tractors in grapes of wrath / tomato harvesters\\n      - ex. not making things handicap accessible\\n    - \"scientific knowledge, technological invention, and corporate profit reinforce each other in deeply entrenched patterns that bear the unmistakable stamp of political and economic power\"\\n      - pushback on use of things like pesticides, highways, nuclear reactors\\n    - technologies which are inherently political, regardless of use\\n      - ex. \"If man, by dint of his knowledge and inventive genius has subdued the forces of nature, the latter avenge themselves upon him by subjecting him, insofar as he employs them, to a veritable despotism independent of all social organization.\"\\n      - attempts to justify strong authority on the basis of supposedly necessary conditions of technical practice have an ancient history. \\n    \\n- [Disembodied Machine Learning: On the Illusion of Objectivity in NLP](https://openreview.net/pdf?id=fkAxTMzy3fs)\\n\\n- [less work for mother](https://www.americanheritage.com/less-work-mother) (cowan 1987) - technologies that seem like they save time rarely do (although they increase \"productivity\")\\n\\n- [The Concept of Function Creep](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547903) - “Function creep denotes an imperceptibly transformative and therewith contestable change in a data-processing system’s proper activity.”\\n\\n\\n\\n## facial rec. demographic benchmarking\\n\\n- [Face Recognition Vendor Test (FRVT) Part 3: Demographic Effects](https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8280.pdf) (grother et al. 2019), NIST\\n  - facial rec types\\n    - 1: 1 == **verification**\\n    - 1: N == **identification**\\n  - data\\n  \\t- **domestic mugshots** collected in the United States\\n  \\t- **application photographs** from a global population of applicants for immigration benefits\\n  \\t- **visa photographs** submitted in support of visa applicants\\n  \\t- **border crossing photographs** of travelers entering the United States\\n  - a common practice is to use random pairs, but as the pairs are stratified to become more similar, the false match rate increases (Fig 3)\\n  - results\\n    - biggest errors seem to be in African Americans + East Asians\\n      - impact of errors - in verification, false positives can be security threat (while false negative is mostly just a nuisance)\\n    -  In domestic mugshots, false negatives are higher in Asian and American Indian individuals, with error rates above those in white and black face\\n      - possible confounder - aging between subsequent photos\\n    - better image quality reduces false negative rates and differentials\\n    - false positives to be between 2 and 5 times higher in women than men\\n    - one to many matching usually has same biases\\n      - a few systems have been able to remove bias in these false positives\\n    - did not analyze cause and effect\\n      - don\\'t consider skin tone\\n- [Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing](https://dl.acm.org/doi/pdf/10.1145/3375627.3375820) (2020)\\n\\n## legal perspectives\\n\\n- **“the master’s tools will never dismantle the master’s house.”**\\n\\n### [ALGORITHMIC ACCOUNTABILITY: A LEGAL AND ECONOMIC FRAMEWORK](http://faculty.haas.berkeley.edu/morse/research/papers/AlgorithmicAccountability_BartlettMorseStantonWallace.pdf) (2020)\\n\\n- Title VII defines accountability under under U.S. antidiscrimination law\\n  - protected attributes: sex, religion, national origin, color\\n  - law is all about who has the burden of proof - 3 steps\\n    - **plaintiff** identifies practice that has observed statistical disparities on protected group\\n    - **defendant** demonstrates practice is (a) job-related (b) consistent with business necessity\\n    - **plaintiff** proposes alternative\\n- ex. dothard vs rawlingson: prison guards were selected based on weight/height rather than strength, so female applicants sued\\n  - legitimate target variable: strength\\n  - proxy variable: weight/height\\n  - supreme court ruled that best criterion is to assess strength, not use weight/height proxies\\n- ex. redlining - people in certain neighborhoods do not get access to credit\\n  - legitimate target: ability to pay back a loan\\n  - proxy variable: zip code (disproportionately affected minorities)\\n- 2 key questions\\n  - **legitimate target variable** - is unobservable target characteristic (e.g. strength) one that can justify hiring disparities?\\n    - disparate outcomes mus be justified by reference to a legitimate \"business necessity\" (e.g. for hiring, this would be a required job-related skill)\\n  - **biased proxy** - do proxy variables (e.g. weight/height) properly capture the legitimate target variable?\\n    - problematic \"redundant encodings\" - a proxy variable can be predictive of a legitimate target variable and membership in a protected group\\n\\n#### **input accountability test - captures these questions w/ basic statistics**\\n\\n  - intuition: exclude input variables which are potentially problematic\\n    - in this context, easier to define fairness without tradeoffs\\n    - even in unbiased approach, still need things like subsidies to address systemic issues\\n  - the test\\n    - look at correlations between proxy and legitimate target, proxy and different groups - proxy should not systematically penalize members of a protected group\\n    - **regression form**\\n      - predict legitimate target from proxy: $Height_i = \\\\alpha \\\\cdot Strength_i + \\\\epsilon_i$\\n      - measure if residuals are correlated with protected groups: $\\\\epsilon_i \\\\perp gender$\\n      - if they are correlated, exclude the feature\\n  - difficulties\\n    - target is often unobservable / has measurement err\\n    - have to define a threshold for testing residual correaltions (maybe 0.05 p-vaues)\\n    - there might exist nonlinear interactions\\n- major issues\\n  - even if features are independently okay, when you combine them in a model the outputs can be problematic\\n\\n#### **related approaches**\\n\\n- some propose balancing the outcomes\\n  - one common problem here is that balancing err rates can force different groups to be different\\n- some propose using bst predictive model alone\\n  - some have argued that a test for fairness is that there is no other algorithm that is as accurate and have less of an adverse impact (skanderson and ritter)\\n- HUD\\'s mere predictive test - only requires that prediction is good and that inputs are not subsitutes for a protected characteristic',\n",
       " '---\\nlayout: notes\\ntitle: cogsci\\ncategory: ai\\n---\\n\\n#  cogsci\\n\\nSome notes on a computational perspective on cognitive science.\\n\\n## nativism, empiricism\\n\\n- cogsci \"inverse problem\" - give world, how do we form representations\\n  \\n  - like cv, given pixels, how do we understand world?\\n- historical cognitive dev: how do we form representations?\\n  - **nativism** - plato - representation w/out learning\\n  - **empiricism** - aristotle - learning w/out representation\\n  - **constructivism** - jean piaget - cognitive development\\n    - types\\n      - assimilation - start with schema, which tells you how to act\\n      - accomodation - adjust schema based on what you see\\n      - equilibration - everything set\\n    - periods\\n      | sensorimotor  | preoperational      | concrete operational  | formal operational     |\\n      | ------------- | ------------------- | --------------------- | ---------------------- |\\n      | 0 - 18 months | 18 months - 5 years | 5 years - adolescence | adolescence and beyond |\\n    - problems\\n      - dev. is domain-specific and variable\\n      - when measured better, children show earlier competence\\n      - doesn\\'t specify learning methods\\n- contemporary theories\\n  - **nativism** - core knowledge or modules\\n    - plato, descartes\\n    - chomsky - language acuquisition device\\n    - spelke, tenenbaum - core knowledge of domains\\n    - constraint nativism vs. starting-state nativism\\n  - **empiricism** - connectionism, dynamic systems, associationism\\n    - aristotle, david hume\\n    - behaviorism - led to rl\\n    - connectionists - mclelland, karmiloff-smith\\n    - dynamic systems - thelen and smith\\n    - **emergentist appproach** - complex behavior is emergent from simple neural properties\\n    - john locke\\n    - deep learning\\n  - **constructivism** - \"the theory theory\" (children are scientists), probabilistic models\\n    - carey, wellman, gelman, gopnik\\n    - structural features: abstract, coherent, causal, hierarchical\\n    - theories change in response to evidence\\n    - changes may take place at multiple levels\\n    - playing\\n    - perhaps bayesian learning\\n  - information-processing - memory changes over time etc.\\n    - siegler\\n  - socio-cultural influences - children learn from society / culture\\n    - vyogotsky\\n\\n- unanswered questions\\n\\n  - search problem: how do children search through all possible hypotheses? sampling?\\n  - conceptual change problem: how is radical change in representations possible\\n\\n- **counterfactual** - relating to or expressing what has not happened or is not the case\\n\\n  - e.g. If kangaroos had no tails, they would topple over\\n\\n## probabalistic causal models\\n\\n- abstract representations that provide computational accounts of causal inference\\n  - at marr\\'s highest level: computation\\n- how much do we need to build in?\\n  - dna can build in things that are hard to learn\\n  - start with nothing built in, ex. deep learning (connectionism)\\n  - start with best possible learning algorithm and ask what you need to build in (bayesian modeling)\\n  - bayes rule Bayesian: $\\\\overbrace{p(\\\\theta | x)}^{\\\\text{posterior}} = \\\\frac{\\\\overbrace{p(x|\\\\theta)}^{\\\\text{likelihood}} \\\\overbrace{p(\\\\theta)}^{\\\\text{prior}}}{p(x)}$ where x is data and $\\\\theta$ are hypotheses\\n    - ask what priors explain people\\'s inferences\\n    - humans make very causal priors - restricts hypothesis space of possible bayesian networks\\n    - hierarhical model - get prior from something (e.g. know all bags contain same color)\\n- what develops over time?\\n  - bayesian doesn\\'t really tell us this - just has probabilities evolve over time\\n  - real life we come up with new hypotheses\\n- what representations do we use?\\n\\n## deep rl\\n\\n### dl basics\\n\\n- what does an NN do?\\n  - universal function approximator\\n  - feature learning view - learn features that separate classes linearly\\n- saxe....ng 2010 compared statistics of primary cortical receptive fields and receptive field plasticity\\n  - vision + audio + somatosensory - statistics seem to line up with neuroscience\\n- a single algorithm?\\n  - ex. seeing with your tongue\\n  - ex. blind people echolocating\\n  - ex. ferret optic nerve rewired to auditory system and they learn to still see\\n- lots of inductive bias? or lack of bias?\\n  - gabor filters come from deep learning, ica, sparse coding, k-means - come from data, not necessarily algorithm\\n    - maybe this is only true for gabor filters though\\n  - bigger and simpler with residuals seems to work best\\n    - sequence tasks - lstm started being replaced with \"attention is all you need\" (attention is basically a dot product)\\n    - differentiable neural computer didn\\'t seem to work all that well - tried to build in too much\\n    - some inductive bias has worked: cnn, lstm\\n- structure\\n  - structure is very dependent on optimization\\n- depth seems to help\\n  - compositionality\\n  - increase in representational power?\\n  - information theoretic arguments\\n- low-data regimes: few-shot recognition doesn\\'t work (see Lake, Salakhutidinov, Tenenbaum 2013)\\n  - one type of meta-learning for few-shot learning\\n    - split data set into tiny train / test blocks and learn to do few-shot learning on these\\n    - got really good at this (especially for omniglot)\\n- deep learning: less structure + more data = better results\\n- alternative view - not necessarily dl\\n  - very unconstrained function class + right learning algorithm + right supervision = better results\\n  - optimization methods master (is sgd \"conveniently\" Bayesian?)\\n    - ex. everything that works works because it\\'s Bayesian blog post\\n  - structure of natural dat amatters\\n    - source of supervision really matters\\n  - maybe function class doesn\\'t really matter\\n\\n### supervision\\n\\n- unsupervised learning: learn $p_\\\\theta (x)$\\n  - overview\\n    - pull out features/representation and use them for other tasks\\n    - recognize novel / unexpected events\\n    - hallucinate\\n    - can use all available data\\n    - often has principled, prob. interpretation\\n    - hard to use effectively\\n  - deep belief networks (~2006) - built on restricted Boltzmann machine (both a NN and a graphical model)\\n    - RBM = markov field with binary variables and connections between but not within layers\\n    - originally trained layerwise\\n    - properties\\n      - each neuron is (binary) random variable, so we can sample\\n      - hard to train\\n  - unsupervised gd (2012-2014) - sparse/denoising/bottleneck autoencoder\\n    - ex. if you have linear encoder/decoder with bottleneck, then you get pca\\n    - ex. denoising - blur image and ask it to learn to clean it up\\n      - learns something about the manifold that represents a class / the correct data\\n    - ex. variational autoencders (Kingma & Welling 2013)\\n      - network encoder x->z & decoder z->x\\n      - force z to be spherical Gaussian (ex. add noise to make it spherical Gaussian (with learned variance) - KL divergence regularizer between output and spherical Gaussian)\\n      - then, we can sample z from spherical Gaussian and decoder will give us nice looking x\\n    - ex. GANs (Goodfewllow 2014)\\n      - \"implicit\" density model\\n- supervised learning\\n  - representations are surprisingly good\\n    - labels provide good semantic knowledge\\n    - simple backprop training\\n  - semi-supervised = self-supervised learning\\n    - ex. given patches, tell where they are placed relative to each other (context prediction doersch et al. 2015)\\n    - prediction supervised learning (time series)\\n      - ex. predict video frames from past video frames\\n    - reakes some artistry to figure out objective\\n- reinforcement learning\\n  - non-differentiable reward function - doesn\\'t access to output supervision\\n  - supervised learning is a subset of this\\n  - 3 types (often we do a combination of these together)\\n    - direct policy search\\n    - prediction of value functions\\n    - prediction of future states\\n  - can collect data by just exploring world\\n  - rl needs more generalization\\n    - want more exploration algorithms\\n    - train in more settings\\n    - ex. approximate bayesian experiment design\\n\\n## objects\\n\\n- *deep deep cnns* - not just a chain, combine different layers and maybe represent semantic concepts\\n  - *iterative deep aggregation* - generalizes skip connections - new layers for the skips\\n- *adaptive learning* - too much dataset bias - imagenet things only work on imagenet\\n  - need to know about discrepancy between distributions\\n  - GAN - learn adversarial that decides whether a point comes from one domain or another\\n  - goal: want both domains to be separable with domains for only one\\n- *explainable vision and language agents*\\n  - image captioning doesn\\'t mean it actually understands scene\\n  - now, people focused on visual question answering\\n    - maybe even provide interpretable explanation\\n  - preprogram 5 types of modules (ex. find, relate, count, ...)\\n- concept vs. category learning\\n  - except for NN, need negative examples\\n  - learning nouns\\n    - strong sampling - pick key examples instead of lots of random examples\\n  - bayesian learning can figure out what classes to generalize to\\n    - need to assume some ontology\\n- it\\'s all about the data\\n  - face detection - made things really work\\n  - learning spectrum: *extrapolation* (low samples) -> *interpolation* (high samples - where we are)\\n    - extrapolation = linear reg.  -> interpolation = nearest neighbor / neural network\\n    - nearest neighbors started to work better with more data\\n  - explainability doesn\\'t work if system is fundamentally complicated\\n  - natural world has too many examples for interpolation\\n  - brain doing nearest neighbors?\\n    - capacity of visual long term memory - standing 1973: 10k images, 83% recognition\\n    - clap if you see a repetition - you can really remember a lot\\n    - AB testing at end - check if you seaw things or not\\n    - novel objects: 92%, different examples of same thing: 88%, different states of same thing: 87%\\n    - we can\\'t do this with just textured images though\\n    - big boosts come from data, not necessarily the algorithm\\n    - word2vec works about same as nearest neighbors embedding\\n- top-down vs bottom-up concept learning\\n  - problems with top-down / using labels\\n    - can\\'t ask computer about semantic stuff - it only sees pixels\\n    - cool ex. can see a chair by just looking at silhouette of person sitting in it\\n    - humans don\\'t categorize things in binary ways\\n      - solns: hierarchy, levels of categories...\\n    - used to have to do this (ex. books in a library), but computers don\\'t have to do this (ex. amazon)\\n    - still problematic\\n      - intransitivity - car seat is chair, chair is furniture, ...\\n      - multiple category membership\\n      - \"ontologies are overrated\" blog post\\n    - this helps with interpretability, but might be worse overal\\n  - start from image (bottom) - use association not categorization\\n    - make graph - edges are associations\\n    - task: predict what\\'s behind some blurred out box in an image\\n\\n\\n## language development\\n\\n- language is interesting problem, lots of children have language difficulties, language might be unique to humans\\n- language is somewhat instinctual\\n  - learned via association (John Locke) or by reading people\\'s intentions (St. Augustine)\\n- behaviorist approach (Skinner) - conditioning, children learn because of adult reinforcement\\n  - ex. bird turning to get reward\\n  - \"meaning\" - an association between stimulus and response\\n  - progressilve taught more complex utterances\\n- noam chomsky\\'s ciritique of skinner review (1959)\\n  - parents correct truth of utterance, not its grammar\\n  - stimulus doesn\\'t completely determine response\\n  - children say things they\\'ve never heard (e.g. mommy eated the apple)\\n  - grammar isn\\'t a chain of associated words\\n- chomsky: children have innate knowledge\\n  - introduces \"cognitive revolution\" - explain language you need\\n    - mental representations\\n    - rules that operate on those representations\\n  - figure out what is common between all languages\\n    - ex. verb agrees with subject\\n    - mostly part of speaker\\'s unconscious knowledge of language\\n  - focus on ideal speaker\\'s competence, not their performance\\n- ex. question formation\\n  - *poverty of the stimulus* - children get this info which doesn\\'t seem to be in the stimulus\\n- chomsky theory of universal grammar\\n  - fixed invariant structural principles because human brain is wired to understand them\\n- *nativists* - children guided by universal grammar\\n- *constructivists* - innate **general learning mechanisms** operate on experience\\n- children use babbling\\n- children use pre-linguistic communication for sharing attention\\n- some children have crib speech - talking to themselves, seems systematic\\n- children have U-curves - use ate, then eated, then back to ate\\n\\n## theory of mind\\n\\n- theory of mind - human understanding of agents\\' mental states and how they shape actions\\n  - understand intentional actions - like dots, understand evil, purpose\\n  - we can track this month by month\\n- Theory of mind \\n  - is rapidly acquired in the normal case\\n  - is acquired in an extended series of developmental accomplishments\\n  - encompasses several basic insights that are acquired world-wide on a roughly similar trajectory (but not timetable), (4) requires considerable learning and development based on an infant set of specialized abilities to attend to and represent persons,  (5) is severely impaired in autism, (6) is severely delayed in deaf children of hearing parents, and (6) results from but also contributes to specialized neural substrates associated with reasoning about agency, experience, and mind\\n- Preschool Theory of Mind, Part 1:  Universal Belief-Desire Understanding \\n',\n",
       " '---\\nlayout: notes\\ntitle: ai futures\\ncategory: ai\\n---\\n\\n#  ai futures\\n\\n## human compatible\\n**A set of notes based on the book human compatible, by Stuart Russell 2019**\\n\\n### what if we succeed?\\n\\n- candidates for biggest event in the future of humanity\\n  - we all die\\n  - we all live forever\\n  - we conquer the universe\\n  - we are visited by a superior alien civilization\\n  - we invent superintelligent AI\\n- *defn*: humans are intelligent to the extent that our actions can be expected to achieve our objectives (given what we perceive)\\n  - machines are *beneficial* to the extent that *their* actions can be expected to achieve *our* objectives\\n- Baldwin effect - learning can make evolution easier\\n- **utility** for things like money is *diminishing*\\n  - rational agents maximize **expected utility**\\n- McCarthy helped usher in *knowledge-based systems*, which use *first-order logic*\\n  - however, these didn\\'t incorporate uncertainty\\n  - modern AI uses utilities and probabilities instead of goals and logic\\n  - bayesian networks are like probabilistic propositional logic, along with bayesian logic, probabilistic programming languages\\n- language already encodes a great deal about what we know\\n- *inductive logic programming* - propose new concepts and definitions in order to identify theories that are both accurate and concise\\n- want to be able to learn many useful abstractions\\n- a superhuman ai could do a lot\\n  - e.g. help with evacuating by individually guiding every person/vehicle\\n  - carry out experiments and compare against all existing results easily\\n  - high-level goal: raise the standard of living for everyone everywhere?\\n  - AI tutoring\\n- EU GDPR\\'s \"right to an explanation\" wording is actually much weaker: \"meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing for the data subject.\"\\n- whataboutery - a method for deflecting questions where one always asks \"what about X?\" rather than engaging\\n\\n### harms of ai\\n\\n- ex. surveillance, persuasion, and control\\n- ex. lethal autonomous weapons (these are scalable)\\n- ex. automated blackmail\\n- ex. deepfakes / fake media\\n- ex. automation - how to solve this? Universal basic income?\\n\\n\\n\\n### value alignment\\n\\n- ex. king midas\\n- ex. driving dangerously\\n- ex. in optimizing sea oxygen levels, takes them out of the air\\n- ex. in curing cancer, gives everyone tumors\\n- note: for an AI, it might be easier to convince of a different objective than actually solve the objective\\n- basically any optimization objective will lead AI to disable its own off-switch\\n\\n\\n\\n### possible solns\\n\\n- Oracle AI - can only answer yes/no/probabilistic questions,  otherwise no output to the real world\\n- inverse RL\\n  - ai should be uncertain about utitilies\\n  - utilties should be inferred from human preferences\\n  - in systems that interact, need to express preferences in terms of game theory\\n- complications\\n  - can be difficult to parse human instruction into preferences\\n  - people are different\\n  - AI loyal to one person might harm others\\n  - ai ethics\\n    - consequentalism - choices should be judged according to expected consequences\\n    - deontological ethics, vritue ethics - concerned with the moral character of actions + individuals\\n    - hard to compare utilties across people\\n    - utilitarianism has issues when there is negative utility\\n  - preferences can change\\n- AI should be regulated\\n- deep learning is a lot like our sensory systems - logic is still need to act on these abstractions\\n\\n## possible minds\\n\\n**edited by John Brockman, 2019)**\\n\\n### intro (brockman)\\n\\n- new technologies = new perceptions\\n- we create tools and we mold ourselves through our use of them\\n- Wiener: \"We must cease to kiss the whip that lashes us\"\\n  - initial book *The human use of human beings*\\n  - he was mostly analog, fell out of fashion\\n  - initially inspired the field\\n- ai has gone down and up for a while\\n- gofai - good old-fashioned ai\\n- things people thought would be hard, like chess, were easy\\n- lots of physicists in this book...\\n\\n### wrong but more relevant than ever (seth lloyd)\\n\\n- current AI is way worse than people think it is\\n- wiener was very pessimistic - wwII / cold war\\n- singularity is not coming...\\n\\n\\n\\n### the limitations of opaque learning machines (judea pearl)\\n\\n- 3 levels of reasoning\\n  - statistical\\n  - causal\\n  - counterfactual - lots of counterfactuals but language is good and providing lots of them\\n- \"explaining away\" = \"backwards blocking\" in the conditioning literature\\n- starts causal inference, but doesn\\'t work for large systems\\n- dl is more about speed than learning\\n- dl is not interpretable\\n- example: ask someone why they are divorced?\\n  - income, age, etc...\\n  - something about relationship...\\n- correlations, causes, explanations (moral/rational) - biologically biased towards this?\\n  - beliefs + desires cause actions\\n- randomly picking grants above some cutoff...\\n- pretty cool that different people do things because of norms (e.g. come to class at 4pm)\\n  - could you do this with ai?\\n- facebook chatbot ex.\\n- paperclip machine, ads on social media\\n- states/companies are like ais\\n- **equifinality** - perturb behavior (like use grayscale images instead of color) and they can still do it (like stability)\\n\\n### the purpose put into the machine (stuart russell)\\n\\n- want safety in ai - need to specify right objective with no uncertainty\\n- **value alignment** - putting in the right purpose\\n- ai research studies the ability to achieve objectives, not the design of those objectives\\n  - \"better at making decisions - not making better decisions\"\\n- want provable beneficial ai\\n- can\\'t just maximize rewards - optimal solution is to control human to give more rewards\\n- cooperative inverse-rl - robot learns reward function from human\\n  - this way, uncertainty about rewards lets robot preserve its off-switch\\n  - human actions don\\'t always reflect their true preferences\\n\\n### the third law (george dyson)\\n\\n- 2 eras: before/after digital computers\\n  - before: thomas hobbes, gottfried wilhelm leibniz\\n  - after: \\n    - alan turing - intelligent machines\\n    - john von neumann - reproducing machines\\n    - claude shannon - communicate reliably\\n    - norbert weiner - when would machines take control\\n- analog computing - all about error corrections\\n- nature uses digitial coding for proteins but analog for brain\\n- social graphs can use digital code for analog computing\\n  - analog systems seem to control what they are mapping (e.g. decentralized traffic map)\\n- 3 laws of ai\\n  - ashby\\'s law - any effective control system must be as complex as the system it controls\\n  - von neumman\\'s law - defining characteristic of a complex system is that it constitutes its own simplest behavioral description\\n  - 3rd law - any system simple enough to be understandable will not be complicated enough to behave intelligently and vice versa\\n\\n### what can we do? (daniel dennett)\\n\\n- dennett wrote from bacteria to bach & back\\n- praise: willingness to admit he is wrong / stay levelheaded\\n- rereading stuff opens new doors\\n- import to treat AI as tools - real danger is humans being slaves to the AI coming about naturally\\n  - analogy to our dependence on fruit for vitamin C whereas other animals synthesize it\\n  - tech has made it easy to tamper with evidence etc.\\n  - Wiener: \"In the long run, there is no distinction between arming ourselves and arming our enemies.\"\\n- current AI is parasitic on human intelligence\\n- we are robots made of robots made of robots...with no magical ingredients thrown in along the way\\n- current humanoid embellishments are *false advertising*\\n- need a way to test safety/interpretability of systems, maybe with human judges\\n- people automatically personify things\\n- we need intelligent tools, not conscious ones - more like oracles\\n- very hard to build in morality into ais - even death might not seem bad\\n\\n### the unity of intelligence (frank wilczek)\\n\\n- can an ai be conscious/creative/evil?\\n- mind is emergent property of matter $\\\\implies$ all intelligence is machine intelligence\\n- david hume: \\'reason is, and ought only to be, the slave of the passions\\'\\n- no sharp divide between natural and artificial intelligence: seem to work on the same physics\\n- intelligence seems to be an emergent behavior\\n- key differences between brains and computers: brains can self-repair, have higher connectivity, but lower efficiency overall\\n- most profound advantage of brain: connectivity and interactive development\\n- ais will be good at exploring\\n- defining general intelligence - maybe using language?\\n- earth\\'s environment not great for ais\\n- ai could control world w/ just info, not just physical means\\n- affective economy - sale of emotions (like talking to starbucks barista)\\n- people seem to like to live in human world\\n  - ex. work in cafes, libraries, etc.\\n- future life institute - funded by elon...maybe just trying to make money\\n\\n### lets aspire to more than making ourselves obsolete (max tegmark)\\n\\n- sometimes listed as scaremonger\\n- maybe consciousness could be much more hype - like waking up from being drowsy\\n- survey of AI experts said 50% chance of general ai surpassing human intelligence by 2040-2050\\n- finding purpose if we aren\\'t needed for anything?\\n- importance of keeping ai beneficial\\n- possible AIs will replace all jobs\\n- curiosity is dangerous\\n- 3 reasons ai danger is downplayed\\n    1. people downplay danger because it makes their research seem good - \"It is difficult to get a man to understand something, when his salary depends on his not understanding it\" - Upton Sinclair\\n    \\t- **luddite** - person opposoed to new technology or ways of working - stems from secret organization of english textile workers who protested\\n    2. it\\'s an abstract threat\\n    3. it feels hopeless to think about\\n- AI safety research must precede AI developments\\n- the real risk with AGI isn\\'t malice but competence\\n- intelligence = ability to accomplish complex goals\\n- how good are people at predicting the future of technology?\\n- joseph weizenbbam wrote psychotherapist bot that was pretty bad but scared him\\n\\n### dissident messages (jaan taliin)\\n\\n- voices that stand up slowly end up convincing people\\n- ai is different than tech that has come before - it can self-multiply\\n- human brain has caused lots of changes in the world - ai will be similar\\n- people seem to be tipping more towards the fact that the risk is large\\n- short-term risks: automation + bias\\n- one big risk: AI environmental risk: how to constrain ai to not render our environment uninhabitable for biological forms\\n- need to stop thinking of the world as a zero-sum game\\n- famous survery: katja grace at the future of humanity institute\\n\\n### tech prophecy and the underappreciated causal power of ideas (steven pinker)\\n\\n- \"just as darwin made it possible for a thoughtful observer of the natural world to do without creationism, Turing and others made it possible for a thoughtful observer of the cognitive world to do without spiritualism\"\\n- entropy view: ais is trying to stave off entropy by following specific goals\\n- ideas drive human history\\n- 2 possible demises\\n  - surveillance state\\n    - automatic speech recognition\\n    - pinker thinks this isn\\'t a big deal because freedom of thought is driven by norms and institutions not tech\\n    - tech\\'s biggest threat seems to be amplifying dubious voices not surpressing enlightened ones\\n    - more tech has correlated w/ more democracy\\n  - ai takes over\\n    - seems too much like technological determinism\\n    - intelligence is the ability to deploy novel means to attain a goal - doesn\\'t specify what the goal is\\n    - knowledge are things we know - ours are mostly find food, mates, etc. machines will have other ones\\n- if humans are smart enough to make ai, they are smart enough to test it\\n- \"threat isn\\'t machine but what can be made of it\"\\n\\n### beyond reward and punishment (david deutsch)\\n\\n- david deutsch - founder of quantum computing\\n- thinking - involves coming up w/ new hypotheses, not just being bayesian\\n- knowledge itself wasn\\'t hugely evolutionarily beneficial in the beginning, but retaining cultural knowledge was\\n  - in the beginning, people didn\\'t really learn - just remembered cultural norms\\n  - no one aspired to anything new\\n- so far, the way ais have been developed (e.g. chess-playing) is restricting a search space, but AGI wants them to come up with a new search space\\n- we usually don\\'t follow laws because of punishments - neither will AGIs\\n- open society is the only stable kind\\n- will be hard to test / optimize for directly\\n- AGI could still be deterministic\\n- tension between imitation and learning? (immitation/innovation)\\n- people falsely believe AGI should be able to learn on its own, like Nietzche\\'s *causa sui*, buy humans don\\'t do this\\n- culture might make you more model-free\\n\\n### the artificial use of human beings (tom griffiths)\\n\\n- believes key to ml is human learning\\n\\n- we now have good models of images/text, but not of \\n\\n- value alignment\\n\\n- inverse rl: look at actions of intelligent agent, learn reward\\n\\n- accuracy (heuristics) vs generalizability (often assumes rationality)\\n  - however, people are often not rational - people follow simple heuristics\\n  - ex. don\\'t calculate probabilities, just try to remember examples\\n\\n- people usually tradeoff time with how important a decision is - **bounded optimality**\\n\\n- could ai actually produce more leisure?\\n\\n### making the invisible visible (hans ulrich obrist)\\n\\n- need to use art to better interpret visualizations, like deepdream\\n- ai as a tool, like photoshop\\n- tweaking simulations is art (again in a deep-dream like way)\\n- meta-objectives are important\\n- art - an early alarm system to think about the future, evocative\\n- design - has a clearer purpose, invisible\\n  - fluxist movement - do it yourself, like flash mob, spontanous, not snobby\\n- this progress exhibit - guggenheim where they hand you off to people getting older\\n- art - tracks what people appreciate over time\\n- everything except museums + pixels are pixels\\n- marcel duchamp 1917 - urinal in art museum was worth a ton\\n\\n### algorists dream of objectivity (peter galison)\\n\\n- science historian\\n- stories of dangerous technologies have been repeated (e.g. nanoscience, recombinant DNA)\\n- review in psychology found objective models outperformed groups of human clinicians (\"prediction procedures: the clinical-statistical controversy\")\\n- people initially started w/ drawing things\\n  - then shifted to more objective measures (e.g. microscope)\\n  - then slight shift away (e.g. humans outperformed algorithms at things)\\n- objectivity is not everything\\n- art w/ a nervous system\\n- animations with charcters that have goals\\n\\n### the rights of machines (george church)\\n\\n- machines should increasingly get rights as those of humans\\n- potential for AI to make humans smarter as well\\n\\n### the artistic use of cybernetic beings (caroline jones)\\n\\n- how to strech people beyond our simple, selfish parameters\\n- cybernetics seance art\\n- more grounded in hardware\\n- culture-based evolution\\n- uncanny valley - if things look too humanlike, we find them creepy\\n  - this doesn\\'t happen for kids (until ~10 years)\\n- neil mendoza animal-based aft reflections\\n- is current ai more advanced than game of life?\\n\\n---\\n\\n### David Kaiser: Information for wiener, Shannon, and for Us\\n\\n- wiener: society can only be understood based on analyzing messages\\n  - information = semantic information\\n  - shannon: information = entropy (not reduction in entropy?)\\n  - predictions\\n    - information can not be conserved (effective level of info will be perpetually advancing)\\n    - information is unsuited to being commodities\\n      - can easily be replicated\\n      - science started having citations in 17th century because before that people didn\\'t want to publish\\n        - turned info into currency\\n      - art world has struggled w/ this\\n        - 80s: appropration art - only changed title\\n      - literature for a long time had no copyrights\\n      - algorithms hard to patent\\n  - wiener\\'s warning: machines would dominate us only when individuals are the same\\n    - style and such become more similar as we are more connected\\n      - twitter would be the opposite of that\\n      - amazon could make things more homogenous\\n    - fashion changes consistently\\n      - maybe arbitrary way to identify in/out groups\\n    - comparison to markets\\n    - cities seem to increase diversity - more people to interact with\\n- dl should seek more semantic info not statistical info\\n\\n### Neil Gershenfield: Scaling\\n\\n- ai is more about scaling laws rathern that fashions\\n- mania: success to limited domains\\n- depression: failure to ill-posed problems\\n- knowledge vs information: which is in the world, which is in your head?\\n- problem 1: communication - important that knowledge can be replicated w/ no loss (shannon)\\n- problem 2: computation - import knowledge can be stored (von Neumann)\\n- problem 3: generalization - how to come up w/ rules for reasoning?\\n- next: fabrication - how to make things?\\n  - ex. body uses only 20 amino acids\\n\\n\\n',\n",
       " '---\\nlayout: notes\\ntitle: logic\\ncategory: ai\\ntypora-copy-images-to: ../assets\\n---\\n\\n#  logic\\nSome notes on logic based on Berkeley\\'s CS 188 course and  \"Artificial Intelligence\" Russel & Norvig 3rd Edition + [foundations of rule learning](https://dl.acm.org/citation.cfm?id=2788240) (furnkranz et al. 2014)\\n\\n## logical agents - 7.1-7.7 (omitting 7.5.2)\\n\\n- *knowledge-based agents* - intelligence is based on *reasoning* that operates on internal *representations of knowledge*\\n- **deductive** - general-to-specific\\n- **inductive** - specific-to-general\\n- 3 steps: given a percept, the agent \\n  1. adds the percept to its knowledge base (KB)\\n  2. asks the knowledge base for the best action\\n  3. tells the knowledge base that it has taken that action\\n- 2 approaches\\n  - *declarative* approach - tell sentences until agent knows how to operate\\n    - know something, can verbalize it\\n  - *procedural* approach - encodes desired behaviors as program code\\n    - intuitively know how to do it (ex. riding a bike)\\n- ex. Wumpus World\\n- logical *entailment* between sentences\\n  - $A \\\\vDash B$ means B follows logically from A (A implies B)\\n  - *logical inference* - showing entailment\\n- *model checking* - try everything to see if A $\\\\implies$ B\\n  - this is *sound* = *truth-preserving*\\n  - *complete* - can derive any sentence that is entailed\\n  - TT-ENTAILS\\n    - recursively enumerate all sentences by assigning true, false to each variable\\n    - check if these are valid within the KB\\n      - if they are, they must also match the query (otherwise return false)\\n- *grounding* - connection between logic and real environment (usually sensors)\\n- theorem properties\\n  - *validity* - *tautalogy* - true under all models\\n  - *satisfiable* - true under some model\\n    - ex. boolean-SAT\\n  - *monotonicity* - set of implications can only increase as info is added to the knowledge base\\n    - if $KB \\\\implies A$ then $KB \\\\land B \\\\implies A$\\n\\n### theorem proving\\n\\n- *resolution rule* - resolves different rules with each other - leads to complete inference procedure\\n- *CNF* - *conjunctive normal form* - conjunction (and) of clauses (with ors) \\n  - ex: $ ( \\\\neg A \\\\lor  B) \\\\land \\\\neg C \\\\land (D \\\\lor E)$\\n  - anything can be expressed as this\\n- *horn clause* - at most one positive\\n  - *definite clause* - disjunction of literals with exactly one positive: ex. ($A \\\\lor \\\\neg B \\\\lor \\\\neg C$)\\n  - *goal clause* - no positive: ex. ($\\\\neg A \\\\lor \\\\neg B \\\\lor \\\\neg C$)\\n  - benefits\\n    - easy to understand\\n    - forward-chaining / backward-chaining are applicable\\n    - deciding entailment is linear in size (KB)\\n\\n  - **forward/backward chaining**: checks if q is entailed by KB of definite clauses\\n\\n    - *data-driven*\\n    - keep adding until query is added or nothing else can be added\\n    - ![Screen Shot 2018-08-01 at 6.54.42 PM](../assets/logic_graph.png)\\n\\n    - backward chaining works backwards from the query\\n      - *goal-driven*\\n      - keep going until get back to known facts\\n- checking satisfiability\\n  - complete backtracking\\n    - *davis-putnam* algorithm = *DPLL* - like TT-entails with 3 improvements\\n      1. early termination\\n      2. pure symbol heuristic - *pure symbol* appears with same sign in all clauses, can just set it to the proper value\\n      3. unit clause heuristic - clause with just one literal or one literal not already assigned false\\n    - other improvements (similar to search)\\n      1. component analysis\\n      2. variable and value ordering\\n      3. intelligent backtracking\\n      4. random restarts\\n      5. clever indexing\\n  - local search\\n    - evaluation function can just count number of unsatisfied clauses (MIN-CONFLICTS algorithm for CSPs)\\n    - WALKSAT - randomly chooses between flipping based on MIN-CONFLICTS and randomly\\n      - runs forever if no soln\\n  - *underconstrained* problems are easy to find solns\\n  - *satisfiability threshold conjecture* - for random clauses, probability of satisfiability goes to 0 or 1 based on ratio of clauses to symbols\\n    - hardest problems are at the threshold\\n\\n### agents based on propositional logic\\n\\n-  *fluents* = state variables that change over time\\n  - can index these by time\\n-  *effect axioms* - specify the effect of an action at the next time step\\n-  *frame axioms* - assert that all propositions remain the same under actions\\n  - *succesor-state axiom*: $F^{t+1} \\\\iff  ActionCausesF^t \\\\lor (F^t \\\\land -ActionCausesNotF^t )$\\n    - ex. $HaveArrow^{t+1} \\\\iff (HaveArrow^t \\\\land \\\\neg Shoot^t)$\\n    - makes things stay the same unles something changes\\n-  *state-estimation*: keep track of *belief state*\\n  - can just use 1-CNF (conjunctions of literals: ex. $WumpusAlive \\\\land L_2 \\\\land B$)\\n    - 1-CNF includes all states that are in fact possible given the full percept history\\n    - *conservative approximation* - contains belief state, but also extraneous stuff\\n-  planning\\n   -  could use $A^*$ with entailment\\n   -  otherwise, could use SATPLAN\\n-  *SATPLAN* - how to make plans for future actions that solve the goal by propositional inference\\n  - basic idea\\n    - make assertions\\n    - transitions up to some max time $t_{final}$\\n    - assert that goal is achieved at time $t_{final}$ (ex. havegold)\\n  - present this to a SAT solver\\n    - must add *precondition axioms* - states that action occurrence requires preconditions to be satisfied\\n      - ex. can\\'t shoow without arrow\\n    - must add *action exclusion axioms* - one action at a time\\n      - ex. can\\'t shoot and move at once\\n\\n## first-order logic - 8.1-8.3.3\\n\\n- ![Screen Shot 2018-08-01 at 7.52.25 PM](../assets/logic_table.png)\\n- basically added objects, relations, quantifiers ($\\\\exists, \\\\forall$)\\n- declarative language - semantics based on a truth relation between sentences and possible worlds\\n  - has *compositionality* - meaning decomposes\\n  - *sapir-whorf hypothesis* - understanding of the world is influenced by the language we speak\\n- 3 elements\\n  1. objects - john (cannot appear by itself, need boolean value)\\n  2. relations - set of tuples (ex. brother(richard, john))\\n  3. functions - only one value for given input (ex. leftLeg(john))\\n- sentences return true or false\\n  \\n  - combine these things\\n- first-order logic assumes more about the world than propositional logic\\n  - *epistemological commitments* - the possible states of knowledge that a logic allows with respect to each fact\\n  - *higher-order logic* - views relations and functions as objects in themselves\\n- first-order consists of symbols\\n  1. *constant symbols* - stand for objects\\n  2. *predicate symbols* - stand for relations\\n  3. *function symbols* - stand for functions\\n  - *arity* - fixes number of args\\n  - *term* - logical expresision tha refers to an object\\n  - *atomic sentence* - formed from a predicate symbol optionally followed by a parenthesized list of terms\\n    - true if relation holds among objects referred to by the args\\n    - ex. Brother(Richard, John)\\n  - *interpretation* - specifies exactly which objects, relations and functions are referred to by the symbols\\n\\n## inference in first-order logic - 9.1-9.4\\n\\n- *propositionalization* - can convert first-order logic to propositional logic and do propositional inference\\n  - *universal instantiation* - we can infer any sentence obtained by substituting a ground term for the variable\\n    - replace \"forall x\" with a specific x\\n  - *existential instantiation* - variable is replaced by a new constant symbol\\n    - replace \"there exists x\" with a specific x that give a name (called the *Skolem constant*)\\n  - only need finite subset of propositionalized KB - can stop nested functions at some depth\\n    - *semidecidable* - algorithms exist that say yes to every entailed sentence, but no algorithm exists that also says no to every nonentailed sentence\\n- *generalized modus ponens*\\n- *unification* - finding substitutions that make different logical expressions look identical\\n  - UNIFY(Knows(John,x), Knows(x,Elizabeth)) = fail .\\n    - use different x\\'s - *standardizing apart*\\n    - want most general unifier\\n  - need *occur check* - S(x) can\\'t unify with S(S(x))\\n- storage and retrieval\\n  - STORE(s) - stores a sentence s into the KB\\n  - FETCH(q) - returns all unifiers such that the query q unifies with some sentence in the KB\\n    - only try to unify reasonable facts using *indexing*\\n    - query such as Employs(IBM, Richard)\\n    - all possible unifying queries form *subsumption lattice*\\n- forward chaining: start w/ atomic sentences + apply modus ponens until no new inferences can be made\\n  - *first-order definite clauses* - (remember this is a type of Horn clause)\\n  - *Datalog* - language restricted to first-order definite clauses with no function symbols\\n  - simple forward-chaining: FOL-FC-ASK - may not terminate if not entailed\\n    1. *pattern matching* is expensive\\n    2. rechecks every rule\\n    3. generates irrelevant facts\\n  - efficient forward chaining (solns to above problems)\\n    1. *conjuct odering* problem - find an ordering to solve the conjuncts of the rule premise so the total cost is minimized\\n      - requires heuristics (ex. *minimum-remaining-values*)\\n    2. incremental forward chaining - ignore redundant rules\\n      - every new fact inferred on iteration t must be derived from at least one new fact inferred on iteration t-1\\n      - *rete* algorithm was first to do this\\n    3. irrelevant facts can be ignored by backward chaining\\n      - could also use *deductive database* to keep track of relevant variables\\n- backward-chaining\\n  - simple backward-chaining: FOL-BC-ASK\\n    - is a *generator* - returns multiple times, each giving one possible result\\n  - like DFS - might go forever\\n  - logic programming: algorithm = logic + control\\n    - ex. *prolog*\\n    - a lot more here\\n    - can have parallelism\\n    - redudant inference / infinite loops because of repeated states and infinite paths\\n    - can use *memoization* (similar to the dynamic programming that forward-chaining does)\\n    - generally easier than converting it into FOLD\\n  - *constraint logic programming* - allows variables to be constrained rather than *bound*\\n    - allows for things with infinite solns\\n    - can use *metarules* to determine which conjuncts are tried first\\n\\n## classical planning 10.1-10.2\\n\\n- *planning* - devising a plan of action to achieve one\\'s goals\\n- *Planning Domain Definition Language* (PDDL) - uses *factored representation* of world\\n  - *closed-world* assumption - fluents that aren\\'t present are false\\n  - solving the frame problem: only specify result of action in terms of what changes\\n  - requires 4 things (like search w/out path cost function)\\n    - initial state\\n    - actions\\n    - transitions\\n    - goals\\n  - no quantifiers\\n- set of ground (variable-free) actions can be represented by a single *action schema*\\n  - like a method with precond and effect\\n  - $Action(Fly(p, from, to))$:\\n    - PRECOND: $At(p, from) \\\\land Plane(p) \\\\land Airport(from) \\\\land Airport(to)$\\n    - EFFECT: $\\\\neg At(p, from) \\\\land At(p, to)$\\n      - can only use variables in the precondition\\n- problems\\n  - PlanSAT - try to find plan that solves a planning problem\\n  - Bounded PlanSAT - ask whether there is a soln of length k or less\\n\\n### algorithms for planning as state-space search\\n\\n- forward (progression) state-space search\\n  - very inefficient\\n  - generally forward search is preferred because it\\'s easier to come up with good heuristics\\n- backward (regression) relevant-states search\\n  - PDLL makes it easy to regress from a state description to the predecessor state description\\n  - start with a set of things in the goal (and any other fluent can hae any value)\\n    - keep track of a set at all points\\n  - in going backward, the effects that were added need not have been true before, but preconditions must have held before\\n- heuristics\\n  - ex. ignore preconditions\\n  - ex. ignore delete lists - remove all negative literals\\n  - ex. **state abstractions** - many-to-one mapping from states $\\\\to$ abstract states\\n    - ex. ignore some fluents\\n- decomposition\\n  - requires subgoal independence\\n\\n## [foundations of rule learning](https://dl.acm.org/citation.cfm?id=2788240) (furnkranz et al. 2014)\\n\\n- terminology - rule has a condition (conjunctive of binary features) + a conclusion = implication\\n  - rule has 2 parts\\n    - *antecedent* = *body* - consists of **conditions** = binary features e.g. $X_1 > 0$, $X_2=0$\\n    - **conclusion** = consequent* = *head*\\n  - rule $r$ has length *L*\\n  - $P, N$ - total positives / negatives in the data\\n  - $TP =\\\\hat P, FP =\\\\hat N$ - positives / negatives covered (predicted) by a rule\\n  - $FN, TN$ - positives / negatives not covered by a rule\\n  - $\\\\frac{TP}{P}$ = true positive rate = sensitivity\\n  - $\\\\frac{TN}{N}$ = true negative rate = specificity\\n  - rules evaluated with a heuristic $H(\\\\hat P, \\\\hat N)$\\n\\n### categorization of tasks (ch 1)\\n\\n- historically, a lot of this was developed in the data mining community and gave rise to packages such as WEKA, RAPID-I, KNIME, ORANGE\\n  - historical algos: AQ2 (michalski, 1969), PRISM (cendrowska, 1987), CN2 (Clar & nibett, 1989), FOIL (quinlan, 1990), RIPPER (cohen 1995), PROGOL (muggleton, 1995), [ALEPH](http://www.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/aleph.pl) (srinivasan, 1999) - entire rule workbench in one prolog file, OPUS (webb, 1955), CBA (lui et al. 1998)\\n- predictive rules\\n  - **propositional learning** (just propositional logic) v **relational learning** (first-order logic) = relational data mining = inductive logic programming\\n  - **concept learning** - binary classification task\\n  - **complete** rule set $\\\\mathcal R$ - covers all positive examples (recall = 1)\\n  - **consistent** - rule set $\\\\mathcal R$ - covers no negative examples (precision = 1)\\n- descriptive data mining - usually unsupervised, just learn patterns\\n  - associative rule learning is unsupervised descriptive (e.g. APRIORI)\\n    - here, both the conditions + conclusions can have many features\\n  - subgroup discovery is descriptive, but has a supervised label, so is actually like supervised clustering - goal is to learn subgroups with a significantly different class distribution than the entire population\\n- relational learning - when data doesn\\'t fit in a table but is associated (e.g. customers have many purchases each)\\n\\n### basics of learning rules (ch 2)\\n\\n- finding rules is basically a search problem\\n  - want to find best rules (generally bigger coverage, less complexity, higher accuracy)\\n  - can thing of it as searching on a **refinement graph** - each rule is a node and refinement operators connect rules\\n- heuristics - all basically trading off recall / precision of a rule $r$\\n  - $CovDiff(r) = \\\\hat P - \\\\hat N$\\n  - $RateDiff(r)$ = $\\\\hat P / P - \\\\hat N / N$\\n  - $Precision(r) = TN / (TP + FP)$ (sometimes called confidence or rule accuracy - ignore this kinda confusing)\\n  - $Laplace(r) = (TN + 1)/(TP+1+FP+1)$ - pad all the values by 1 to adjust scores when numbers are small\\n  - *likelihood ratio statistic* - compare distr in rule to distr in full dataset\\n- stopping criteria\\n  - threshold for some heuristic\\n- making final prediction\\n  - final predictions can be made via majority vote, using most accurate rule, or averaging predictions.\\n- algorithms\\n  - sequential covering (remove covered points after each rule)\\n\\n### (binary split) features (ch 4)\\n\\n- here, feature means something binary that we split on\\n- selector is smth of the form $A_i \\\\sim v_{i, j}$ where relation $\\\\sim$ is like $=, >=, <=$\\n  - can also be attribute sets (internal disjunctions) $A_i \\\\in \\\\{v_1, v_2, v_3 \\\\}$, intervals (range operators), or attribute sets (internal conjunctions)\\n  - can also be simple combinations of binary variables\\n- relational features - function between features (e.g. length > height)\\n  - can have splits that are functions of previous splits (like a residual DNN connection)\\n- many algorithms start by making a covering table = table of binary values for all possible (reasonable) splits for all features\\n  - split on all categorical features\\n  - split between all values of continuous features (or ordered discrete)\\n  - can compute relational features (e.g. $A_1 - A_2$) by just adding these as features\\n- feature relevancy\\n  - $pn-pair$: pair of training examples where one is positive and one is negative\\n  - totally irrelevant features - don\\'t distinguish between any positive/negative examples\\n  - a feature $f_1$ is **more relevant** than another $f_2$ if it separates all the $pn$-pairs that $f_2$ does and more\\n  - can also manually set thresholds on TP, FP to decide irrelevance\\n- missing values\\n  - different types\\n    - missing - was not measured but should have been\\n    - not applicable - e.g. pregnant for a male\\n    - don\\'t care - could be anything\\n  - basic strategies\\n    - delete incomplement examples\\n    - treat missing as special value\\n    - impute w/ mean/median/linear prediction\\n    - fill in prob distr. over missing val\\n    - pessimistic value strategy - imputed values shouldn\\'t differentiate between classes - set value so it doesn\\'t get used (e.g. false for positive class and true for neg class)\\n- imprecise values - continuous values with noise\\n  - might want to test variables with $\\\\pm \\\\delta$ handled with pessimistic value strategy\\n  - **fuzzy rules** - probabilistically split\\n\\n### relational features (ch 5)\\n\\n- these kinds of task use relational background knowledge + databases\\n  - ex. from knowledge about things like *female(X)*, *parent(X, Y)*, learn that *daughter(X, Y):= female(X) , parent(Y, X)*\\n  - allow $\\\\forall, \\\\exists$\\n\\n### learning single rules (ch 6)\\n\\n- search problem to maximize some criteria subject to some constraints\\n  - top-down - start with large cover then go to small\\n  - bottom-up - start with high-sensitivity, low cover rules then go larger\\n- ![find_best_rule](../assets/find_best_rule.png)\\n- search algos\\n  - exhaustive search\\n  - hill-climbing = local-search - can make less myopic by considering multiple refinements at a time\\n  - beam-search - keep k best candidates\\n  - best-first search - beam search but keep all candidates\\n  - ordered search - prune the search space based on knowledge (e.g. order splitting values)\\n  - level-wise search (e.g. apriori) - generate in parallel all rules with a certain minimum quality\\n  - stochastic search - involves randomness\\n- search directions: combine top-down (specialization) with bottom-up (generalization)',\n",
       " '---\\nlayout: notes\\ntitle: philosophy\\ncategory: ai\\n---\\n\\n**Notes on philosophy relevant to explanation (particularly in science)**\\n\\n\\n#  philosophy\\n\\n## interpretable ml\\n\\n- [Machine Learning and the Future of Realism](https://arxiv.org/pdf/1704.04688.pdf) (hooker & hooker, 2017)\\n  - lack of interpretability in DNNs is part of what makes them powerful\\n  - *naked predictions* - numbers with no real interpretation\\n    - more central to science than modelling?\\n    - no theory needed? (Breiman 2001)\\n  - old school: realist studied neuroscience (Wundt), anti-realist just stimuli/response patterns (Skinner), now neither\\n  - interpretability properties\\n    - *simplicity* - too complex\\n    - *risk* - too complex\\n    - *efficiency* - basically generalizability\\n    - *unification* - answers *ontology* - the nature of being\\n    - *realism* in a partially accessible world\\n  - overall, they believe there is inherent value of ontological description\\n- [Explainable Artificial Intelligence and Machine Learning: A reality rooted perspective](https://arxiv.org/pdf/2001.09464v1.pdf) (Emmert-Streib et al. 2020)\\n  - explainable AI is not a new field but has been already recognized and discussed for expert systems in the 1980s\\n  1. in some cases, such as simple physics, we can hope to get a **theory** - however, when the underlying process is complicated, interpretation can\\'t hope to simplify it\\n  2. in other cases, we might hope just for a **description**\\n\\n## [scientific explanation](https://plato.stanford.edu/entries/scientific-explanation/) (SEP)\\n\\n- distinction between scientific explanation and non-scientific explanation (although can put things on a spectrum)\\n- distinction between explanations and accounts which are \"merely descriptive\"\\n- these theories focus only on explanations of *why* things happen\\n- concepts: explanation, description, causation\\n\\n### some overarching examples - what causes what?\\n\\n- do birth pills stop pregnancy (both for men/women)?\\n- sun, flagpole, shadow\\n- barometer, air pressure, storm\\n- collision of pool balls\\n- supply/demand curves in economics\\n- gas law $PV=nRT$\\n\\n### DN model = Deductive-Nomological Model\\n\\n- by Popper, Hempel, Oppenheim (popper 1935, hempel 1942)\\n- explanation has 2 constituents\\n  - **explanandum** - sentence describing phemonenon to be explained\\n  - **explanans** - class of sentences used to account for the phenomenon\\n- deductive\\n  - explanans must be true\\n  - explanandum must be a logical consequence of the explanans\\n- nomological = \"lawful\"\\n  - explanans must contain a \"law of nature\"\\n  - the law of nature must be essential to deriving the explanans\\n- still hard to decide exactly what is a law - should be exceptionless generalizations describing regularities\\n  - can have probabilistic laws as well (e.g. prob of recovering after taking penicillin is high)\\n- why this framework\\n  - by framing terms of laws and cricumstances, the explanation shows that the phenomenon *was to be expected* and helps us *understand why* it occured\\n  - sometimes, an *explanation-sketch* uses words like cause, that can be reframed more precisely in the DN model\\n- counterexamples\\n  - assymetry (e.g. shadow length, flagpole height, sun angle) - derive shadow length seems explanatory, but not deriving flapole height\\n  - irrelevant details (e.g. being a man + taking birth control pills explains why you don\\'t get pregnant)\\n  - feels like their is something missing about \"causality\", but this is difficult to pin down - suggests DN model states necessary but not sufficient conditions\\n  - features of an explanation must be recognized / used by users of an explanation\\n\\n### statistical relevance - wesley salmon\\n\\n- starts with (salmon, 1971)\\n- given some class or population *A*, an attribute *C* will be **statistically relevant** to another attribute *B* iff $P(B∣A,C) \\\\neq P(B∣A)$\\n  - find a set of attributes which divide the target into a homogenous partition = even if we split the cells further, they keep the same probability\\n    - like in causal inference, assume no missing variables\\n  - then, the explanation for a new target *x* gives the cells, the prob. for each of the cells, and which cell *x* belongs to\\n- this method is almost information-theoretic - same explanans equally explains the same model with inverted probabilies\\n- ex. (Salmon, 1971) atmospheric pressure, barometer, and storm - barometer is explanatory but not causal\\n\\n### causal mechanical models\\n\\n- starts with (salmon, 1984)\\n- elements\\n  - **causal process** - leaves marks on the world which persist spatiotemporally (these marks hint at counterfactuals)\\n    - contrasts with a pseudoprocess, like a shadow\\n  - **causal interaction** - interaction of such processes, e.g. a car crash\\n- explanation consists of 2 parts which both track causal process + interactions\\n  - **etiological** - leading up to event\\n  - **constitutive parts** - during event\\n- issues: hard to distinguish between explanatory causes (e.g. for a pool ball collision, mass + velocity) vs other so-called causal processes (e.g. chalk mark on ball)\\n  - tends towards overly complex physical descriptions - e.g. for gas law track all particles rather than something global like pressure\\n\\n\\n\\n### unificationist models\\n\\n- important attempts include friedman (1974) and kitcher (1989)\\n- seeks a unified account of a range of different phenomena\\n- best explanations explains most phenomena with as few + as stringent arguments possible\\n- potential issues\\n  - causal asymmetries - equally likely to say planets in future cause motion now then planets now cause motion in the future - this is honestly probably fine\\n  - doesn\\'t easily admit laws at different graunlarities\\n\\n### pragmatic = contextual explanation models\\n\\n- scriven (1962), bromberger (1966), van Fraassen (1980), achinstein (1983)\\n- takes audience into account\\n- others have been after characterizing a single \"true\" explanation and the role of the audience was minimized\\n- \"pragmatic\" here means not just useful but also explicitly considering psychology + context\\n- \"Causal–explanatory pluralism\" (lombrozo 2010 [[PDF](https://www.sciencedirect.com/science/article/abs/pii/S0010028510000253)]) - subjects prefer explanations that appeal to relationships that are relatively stable (in the sense of continuing to hold across changing circumstances)\\n- **constructive empiricism** (Bas van Fraasen)\\n  - just want theories that are \"empirically adequate\"\\n  - explanations are answers to questions - usually **why?** questions\\n    - explanations pick something from a set and tell why it is not any of the other things in the set (the set is given from context)\\n  - explanation is ...\"a t three-term relation between theory, fact, and context\"\\n  - asymmetries coem of rom context\\n- main criticism: this theory is too flexible, ironically it might be too flexible to be meaningfully (practically) useful\\n  - however, it is possible that this is the most flexible framework that is still accurate\\n  - want context to come in for as few steps as possible during an explanation - maybe we don\\'t need to analyze human psych\\n  - somewhat circular - if this is true, then how can we resolve ambiguities?\\n\\n\\n\\n### open areas\\n\\n- understand casuality\\n- more focus on whethery expalanations capture our intuitive judgements and more on the issue of why the info they convey is valuable + relates to our goals\\n- to what extent does a single model work across sciences (e.g. biologists claim to be interested in mechanisms whereas physicists in laws)\\n\\n## stability\\n\\n- [Foundationalism](http://en.wikipedia.org/wiki/Foundationalism) - where the chain of justifications eventually relies on [basic beliefs](http://en.wikipedia.org/wiki/Basic_beliefs) or [axioms](http://en.wikipedia.org/wiki/Axiom) that are left unproven\\n  - Plato’s Republic\\n- the stability of belief: how rational belief coheres with probability (leitgeb, 2017) - introduction\\n- https://projecteuclid.org/download/pdfview_1/euclid.ss/1294167961\\n\\n\\n\\n## the story of philosophy\\n\\n**book by will durant**\\n\\n- states his own view that philosophy should focus on ethics rather the epistemiology (i.e. how we know what we know)\\n  - every science begins as philosophy and ends as art\\n- some definitions of philosophy\\n  - pursuit of fundamental laws\\n  - quest of unity\\n\\n### plato\\n\\n- socrates, plato\\'s teacher pursued stricter definitions and was put to death\\n- plato writes *the Republic* - fictitional dialogue w/ socrates as the protagonist\\n  - argues that democracy failes because people are greedy\\n  - advocates for an absolute meritocracy with 3 classes\\n    - ruling class should live like communists, decent state salary, disallowed from excess\\n    - soldiers / auxiliaries\\n    - general population\\n  - requires equality of education\\n  - requires religion to placated the non-ruling majority\\n  - excess is regulated\\n  - justice = having and doing what is one\\'s own\\n    - each shall receive equivalent to what he produces + perform function for which he is best fit\\n  - juxtapositions\\n    - jesus: kindness to the weak\\n    - nietzsche: bravery of the strong\\n    - plato: effective harmony of the whole\\n- only 3 things: truth, beauty, + justice\\n\\n### aristotle\\n\\n- starts systematic science, library science, and **logic**\\n- advocates for *uinversals* as individuals (e.g. a man, not *man* like Plat argues for)\\n  - this is more grounded in reality\\n- theology: God moves the work like force, but does little else\\n- science: infinitesimal distinctions - boundaries between plant/animal categories are blurry\\n  - form: man, matter: child = possibility of form\\n- politics: ideally a monarchy / aristocracy but more realistically would be constitutional gov. (people determine needs, leaders determine how to meet them)\\n  - restrictions on pop.\\n  - believes in slavery / female inferiority\\n\\n### francis bacon\\n\\n- lived in 1500s/1600s in England\\n- father of the scientific method\\n  - objective and realistic\\n  - in contrast to descartes = subjctive/idealistic\\n    - \"I think therefore I am\"\\n- bacon embraces **epicureanism** - don\\'t want anything\\n- scors knowledge that doesn\\'t lead to action\\n- **science** = organization of knowledge\\n- **philosophy** = organization of science\\n- doubt all assumptions\\n\\n### spinoza\\n\\n- baroch de espinoza\\n- jew who was excommunicated for anti-religious writing\\n- no distinction between body and mind\\n- no free will - only desires that guide everything\\n\\n\\n## effective altruism\\n\\n- [effectivealtruism](https://www.effectivealtruism.org/articles/introduction-to-effective-altruism/)\\n    - promising causes\\n      - Great in *scale* (it affects many lives, by a great amount)\\n      - Highly *neglected* (few other people are working on addressing the problem), and\\n      - Highly *solvable* or *tractable* (additional resources will do a great deal to address it).\\n    - 3 big areas\\n      - fighting extreme poverty (e.g. malaria)\\n      - animal suffering\\n      - improving the long-term future\\n- [rethink priorities jobs](https://rethinkpriorities.freshteam.com/jobs/iX8GfQ1eBLDq/researcher-multiple-positions-remote)\\n- [open philanthropy](https://www.openphilanthropy.org/blog/modeling-human-trajectory)\\n- careers can have greater impacts than altruism ([80k hours](https://80000hours.org/key-ideas/))\\n    - https://80000hours.org/career-guide/\\n    - [80k hours AI careers](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/)\\n- [givewell cost-effective calculations](https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness)\\n    - charities often exaggerate their \"cost to save a life\"',\n",
       " '---\\nlayout: notes\\ntitle: psychology\\ncategory: ai\\n---\\n\\nSome notes on papers / books surrounding psychology, especially evolutionary psychology and the psychology of explanation. These are notes on the authors\\' points and not an endorsement of their views.\\n\\n#  psychology\\n\\n## explanations\\n\\n### [The structure and function of explanations](https://www.sciencedirect.com/science/article/abs/pii/S1364661306002117) (lombrozo 2006)\\n\\n- explanation structures\\n  - accommodate novel info in the context of prior beliefs\\n  - do so in a way that fosters generalization\\n- background\\n  - explanations answer *why* questions\\n  - cognitive science has embraced explanation with regard to concepts and prior knowledge\\n  - explanations affect: (1) prob. assigned to causal claims, (2) how properties are generalized, (3) learning\\n- predominant concepts\\n  - causation\\n  - pattern subsumption - this knowledge constrains what causes are probable/relevant\\n- function of explanations: predict/control the future, constraint for generalization\\n  1. causal inference - depends on prior beliefs + statistical evidence\\n    - explanations constrain causal inference based on prior beliefs\\n    - e.g. \"if provided with evidence that cars of a particular color and size have better gas mileage, children and adults will disregard the confounding factor of color to conclude that car size causes the mileage difference\"\\n    - people often offer explanations over evidence, especially when evidence is sparse\\n    - generating explanations for why a claim might be true provides a way to assess the probability of that claim in light of prior beliefs\\n    - \"when generated from true beliefs, explanations provide an invaluable source of constraint; when generated from false beliefs, explanations can perpetuate inaccuracy.\"\\n  2. generalization of properties\\n    - basics of generalization\\n      - similarity: property is more likely to generalize to a new case if new case is similar\\n      - diversity: for generalizing to a broader category, property is more likely to generalize with more diverse evidence\\n    - explanations can override these basics\\n  3. generalization of knowledge systems\\n    - self-explanation aids learning\\n    - \"Explaining to oneself thus facilitates generalization to transfer problems by isolating \\n    - relevant senses of similarity, helping learners to overcome ‘the frailties of induction’\"\\n- differences between explanation and causal reasoning\\n  \\n  - \"some beliefs are privileged at the expense of others\" - relevance determines which causal factors matter\\n  - \"prior knowledge might not be deployed through other means\" - e.g. \"explaining why a claim might be true or false changes the perceived probability of that claim\"\\n  - \"properties of explanations, such as their generality or simplicity, can influence probabilistic judgements\"\\n\\n### Causal Explanation (lombrozo & vasilyeva 2017)\\n\\n- explanations appeal to causes (although not all explanations are causal e.g. mathematics)\\n- causal inference here not the same as the way it is used in statistics\\n\\n#### causal *inference* w/ explanations\\n\\n- \"inference to the best explanation\" - believe hypothesis that *best explains* the data\\n  - this is not just bayesian inference (a common assumption)\\n    - rather, it includes explanatory considerations such as simplicity, scope, and explanatory power\\n    - these things may improve short-term accuracy and make things easier to communicate/remember/use\\n    - it is possible that these things could be captured by a hierarchical bayesian model with appropriate priors / likelihoods\\n  - simplicity (lombrozo 2007) - explanation simplicity trades off w/ statistical likeliness\\n    - (lombrozo 2012) adults more likely to choose likeliness and all more like to choose likeliness for tasks with less apparent causal explanations\\n    - (pacer & lombrozo, 2017) explanation includes *node simplicity* = number of causes (nodes in graphical model) + *root simplicity* = number of unexplained causes (roots in graphical model)\\n      - people seem to only be sensitive to node simplicity\\n  - explanatory scope - how many things does this explanation imply (even if the others aren\\'t tested)\\n    - e.g. does a diagnosis predict additional effects not yet tested? people prefer diagnoses with narrower scope (khemlani, sussman, and oppenheimer 2011)\\n  - explanatory power - people\\'s explanations better predict their estimates of posterior probability than do objective probabilites on their own (douven & schupbach, 2015a, 2015b)\\n  - other considerations, such as coherence, completeness, manifest scope\\n\\n#### causal *discovery* w/ explanations\\n\\n- causal discovery = causal model learning\\n- engaging in explanation influences causal model learning\\n  - being prompted to explain can promote understanding\\n  - makes them more likely to find underlying causal models\\n  - (walker et al. 2016) - children asked to explain more attune to both evidence and prior beliefs\\n  - also sometimes reinforces people\\'s prior beliefs (right or wrong)\\n  - explaining “involved the integration of new information into existing knowl\\xadedge” (Chi, De Leeuw, Chiu, and LaVancher, 1994)\\n- reasons why explanation alters causal learning\\n  - **attention** - explanation doesn\\'t just boost attention - leads to specific benefits / deficits\\n  - **motivation** - explaining plays a motivational role  (e.g. gopnik 2000 \"Explanation as orgasm\")\\n    - i.e. people seeking good explanations motivates causal understanding\\n  - explanation favors finding hypotheses with \"lovely\" causes\\n  - however, some studies find that children prompted to explain outperform controls even when they don\\'t generate the right explanation\\n\\n#### causal *responsibility* w/ explanations\\n\\n- **causal responsibility** = to which cause(s) do we attribute a given effect?\\n  - ex. \"why did she slip?\" - either \"she is clumsy!\" or \"the staircase is slippery!\"\\n- classic ANOVA model (Kelley 1967) says ppl analyze covariation between factors such as person, stimulus, and situation but more seems to be involved\\n- different questions have different \"contrast class\" (van Fraasesen, 1980, philosophy) - why did *she* slip? vs why did she slip *on the stairs*?\\n  - different questions shift things to causal relevance and not just probability\\n\\n### explanation taxonomy\\n\\n- Aristotle’s 4 \"causes\" or modes of explanation\\n\\n  | cause                                | description                                           | example                                          |\\n  | :----------------------------------- | ----------------------------------------------------- | ------------------------------------------------ |\\n  | **efficient**                        | proximal mechanisms of change                         | a carpenter is an efficient cause of a bookshelf |\\n  | **final** (functional, teleological) | the end, function or goal                             | holding books is a final cause of a bookshelf    |\\n  | **formal**                           | the form or properties that make something what it is | having shelves is a formal cause of a bookshelf  |\\n  | **material**                         | the substance of which something is constituted       | wood is a material cause of a bookshelf          |\\n\\n- final causes\\n  - e.g. camouflage causes zebra stripes\\n  - real cause is a preceding intution\\n  - experiments suggest final causes are only accepted well when there is some causal link\\n    - e.g. adults who believe in God are more likely to accept scientifically unwarranted teleological explanations\\n- another taxonomy: inherent vs. extrinsic explanations (cimpian & salomon, 2014)\\n- formal explanations\\n  - pretty limited to category membership\\n  - e.g. Zach diagnoses ail\\xad ments because he is a doctor\\n  - these can be seen as **constitutive** (not causal): e.g. has four legs bc dog, but not is red bc is barn (even though most barns are read)\\n  - “existence of a whole presupposes the existence of its parts, and thus the existence of a part is rendered intelligible by identifying the whole of which it is a part” (prasada & dillingham 2009)\\n- people who gave different explanations (e.g. functional vs material) also generalized differently to different categories (lombrozo 09)\\n- two types of relationships - when bouth are present people opt for dependence\\n  - **dependence** = counterfactually - if cause didn\\'t occur, effect wouldn\\'t have occurred\\n  - **transference** = physical connection, e.g. continuous mechanism / conserved physical quantity\\n\\n### causal mechanisms\\n\\n- **mechanism** - spells out the intermediate steps between some cause and some effect\\n  - sometimes these are seen as explanations\\n- alternative define mechanisms as complex systems that involve a (typically hierarchi\\xadcal) structure and arrangement of parts and processes, such as that exhibited by a watch, a cell, or a socioeconomic system\\n  - interlevel relationships are *constitutive*, not causal (e.g. saying molecules rub against one another - this *is* heat (contintutive) but people often misconstrue this is causal)\\n  - explanations can accomodate both types of relationships\\n\\n### misc explanation work\\n\\n- Evaluating computational models of explanation using human judgments (pacer, williams, chen, lombrozo, & griffiths, 2013) [[PDF](http://cocosci.princeton.edu/tom/papers/FormalExplanationModelUAI2013.pdf)]\\n  - overton 2012 finds that explanations used something general (ex. model) to explain something specific (ex. data)\\n  - subsequent analysis overton 2012 finds \"inference to the best explanation\" - use specific instances (ex. data) to draw general inferences\\n  - waskan et al 2014 - must be actually intelligible\\n  - lombrozo 2011 - explanations are intrinsically valuable, but also play an important instrumental role in the discovery and confirmation of intuitive theories, which in turn support prediction and intervention\\n  - explanations should be understood in terms of their role in generating understanding (Achinstein 1983; Wilkenfeld 2014), supporting future judgments (Craik 1943; Heider 1958; Quine & Ullian 1970), or **motivating the construction of causal theories (Gopnik 2000)**\\n  - explanations play a role in generalizing from known to novel cases (Rehder 2006; Sloman 1994; Lombrozo & Gwynne 2014)\\n  - sometimes impedes learning about properties that are idiosyncratic\\n  - explanatory errors and “illusions” can help us identify when and why engaging in explanation is so often beneficial\\n  - functional approach: why do we want explanations?\\n    - the best explanation for persuasion or efficient storage of information, for example, may not be the one that best supports future prediction.\\n  - evidence (=the explanandum) provides for some hypothesis (=the explanans).\\n- given causal thing what is best explanation\\n  - most relevant explanation model or explnatory tree model\\n  - human explanatory judgments track something more like evidence, information, or relevance, and not simply the prior or posterior probability of the explanans\\n- desiderata\\n  - **simplicity**\\n    - if simplicity does inform explanatory preferences, it is trumped or made moot by probability\\n    - count simplicity vs root simplicity (root is often preferred)\\n  - **fruitfulness**\\n    - generally explanations with broader scope are better except for causal stuff\\n- explanations + learning\\n  \\n  - explanation magnifies our prior beliefs\\n- \"negative program\" - empirical results disprove philosophical intuitions\\n\\n## the invisible gorilla\\n\\n- We think we experience much more of our physical world than we do\\n- We generally only see what we’re looking for\\n- Our memory is very fake\\n- We have a belief in shortcuts to expand our brain’s abilities\\n  - ex. Lumosity\\n\\n\\n## the moral animal\\n\\n**Why We Are, the Way We Are: The New Science of Evolutionary Psychology** - Robert Wright, 1965. Notes in this section are not an endorsement of the author\\'s views.\\n\\n### sex, romance, and love\\n\\n#### darwin comes of age\\n- *Emile Durkheim* - father of modern sociology\\n- *Wilson* - initial book sociobiology was vehemently opposed\\n\\t- reactive against connotations of *social darwinism*\\n\\t\\t- social darwinism is linked to eugenics\\n- genes affect human nature in two ways\\n\\t1. existence of guilt\\n\\t2. developmental program to calibrate guilt\\n\\n#### male and female\\n- have to consider environment of evolutionary adaptation (EEA)\\n- many studies on !Kung San of the Kalahari desert in Africa\\n- throughout nature, females are more coy while males are more promiscuous\\n\\t- this is true of every known human society\\n\\t- Samoa example was thought to be different, but this study was refuted\\n\\t- true with turkeys (who can be seduced by a wooden female turkey head)\\n- difference is due to amount of male parental investment\\n- apes\\n\\t- ex. gorilla alpha male claims all the women\\n\\t- ex. gibbons are monogamous - live in family units separate from others\\n\\t\\t- sing duets\\n\\n#### gender differences in evolution\\n\\n- ideas that humans are a *pair-bonding species*\\n\\t- humans require high male parental investment (MPI)\\n\\t- vulnerable offspring\\n\\t- more education from two parents\\n- genetically speaking, for males, worst thing is raising a child that isn\\'t theres\\n\\t- experiments suggest they are most angry at sexual infidelity\\n\\t- historically, sometimes killed children that weren\\'t theirs\\n\\t- quantity of sperm depends heavily on the amount of time a male\\'s mate has been out of his sight lately\\n- genetically speaking, for females, worst thing is being abandoned\\n\\t\\n\\t- experiments suggest females are relatively more angry at emotional infidelity\\n\\t- however, can still be genetically useful for a female to be unfaithful\\n\\t\\t1. can extract gifts for sex - \"resource extraction\"\\n\\t\\t2. they don\\'t advertise their ovulation - \"seeds of confusion\"\\n\\t\\n- *madonna-whore dichotomy* - a psychological phenomenon which groups females into 2 categories: marriage / fling\\n\\n  - perhaps example of *frequency-dependent selection*\\n\\n  - ex. blugill sunfish \\n    1. normal males make nests and guard eggs\\n    2. drifter males sneak around and fertilize others\\' eggs\\n    - nature strikes a balance between both\\n  - in actuality, should be able to guage situation and switch between different behaviors\\n  - *self-esteem* might be biological marker that helps with this\\n\\n- males are more likely to gain from leaving a marriage\\n\\t\\n\\t- females only have ~25 fertile years\\n\\n#### the marriage market\\n- polygamy, initially, seems to benefit males\\n\\t- one male can get more females\\n- why monogamy\\n\\t1. *ecologically imposed* - if people are struggling to survive, a woman shouldn\\'t share a man with another\\n\\t\\t- they won\\'t have enough resources\\n\\t2. *socially imposed* - in economic unequal societies (like today)\\n\\t\\t- these societies are the ones that have dowry\\n\\t\\t- imagine 1000 men and 1000 women -> polygyny actually hurts the men\\n\\t\\t- therefore, monogamy likely evolved to stop the dangers of men without wives\\n- current divorce rates are high, hurting all alike\\n- Charles Darwin focused on wealth, Emma focused on looks, they were married happily\\n\\n### social cement\\n\\n#### families\\n- altruism makes sense for kin - wasps, ants\\n\\t- part of kin selection theory by Hamilton\\n\\t- some ants are sterile and only defend nests\\n- genes try to propagate *themselves* not individuals or groups\\n- $r$ - represents *degree of relatedness*\\n\\t- brother = 1/2\\n\\t- aunt = 1/4\\n\\t- cousin = 1/8\\n\\t- for some organisms, like slime mold, r=1\\n\\t- higher average r leads to more altruism\\n- children look after themselves first, then siblings\\n\\t- parents need to teach them to share\\n\\t- children are biologically inclined to listen to their parents when young\\n- biological evidence: wealthy people focus on boy children, poor on girls\\n\\t- measure in how many years after first child for next child\\n\\t- makes sense since male\\'s reproductive potential is more affected by societal status\\n\\t- this same trend should show up for siblings (poor children are nicer to girl siblings)\\n- parents grieve most of children around adolescent age\\n\\t- this maps perfectly the reproductive potential of !Kung people\\n\\n#### darwin and the savages\\n- evolution: kin-selection -> reciprocal altruism (tit for tat) -> higher morals\\n- against group selection - even if it helped a group, it would start to decay within the group\\n\\n#### friends\\n- helping others is not zero-sum\\n\\t- lets you catch big game, spread information\\n- late 1970s Axelrod devises competition for prisoner\\'s dillema programs\\n\\t- *TIT for TAT* programs wins - do what person did last\\n\\t- very simple for early ancestors to implement\\n\\t- designed for *individuals* not *groups*\\n- TIT for TAT doesn\\'t work unless lots of people do it\\n\\t- kin selection gave it a boost\\n\\t- not sure about aunts, uncles, etc.\\n\\t- people try to maintain appearances of dignity\\n- once reciprocal altruism is entrenched, can have \"good for the group\" type genes\\n\\n#### darwin\\'s conscience\\n- moral guidance is made to be guided by peers / parents\\n- lying can be useful\\n\\t- lying can be genetically made exciting to teach its usefulness\\n- modern society generally has more lying\\n\\t- this is to be expected as groups get larger and have more immigration and emigration\\n\\n### social strife\\n\\n#### darwin\\'s delay\\n- darwin studies barnacles for a while, probably because he was afraid to unveil his iconoclastic theory\\n\\n#### social status\\n- hierarchy exists almost everywhere\\n- even societies that have shared resources didn\\'t share social status, females, etc.\\n- group-selection for hierarchy doesn\\'t make sense - individual fitness in anarchy is fine\\n- 2 theories\\n\\t1. *pecking order*\\n\\t\\t- if you leave hens, after some combat they settle into a pecking order\\n\\t\\t- hen A pecks B with impunity, B pecks C, so on in order to settle disputes for resources\\n\\t\\t- they all respect this\\n\\t\\t- genes endowing a chicken with this selective fear should flourish\\n\\t2. *John Maynard Smith\\'s* evolutionary *steady state*\\n\\t\\t- hawk-dove analysis of birds\\n\\t\\t- both strategies make sense in proportions\\n- males genetically tend to be most ambitious, seek social status\\n\\t- Sharifian emperor of Morocco credited with 888 children\\n- hierarchy comes after reciprocal altruism\\n\\t- status assistance could be main purpose of friendship\\n- *Machiavellianism* is \"the employment of cunning and duplicity in statecraft or in general conduct\"\\n- what we call cultural values are expedients to social success\\n\\n#### deception and self-deception\\n- it is more important to give the appearance of altruism than actually be altruistic\\n- some of our motives are hidden from us not incidentally but by design, so that we can credibly act as if they aren\\'t what they are\\n- low *self-esteem* - way to reconcile people to subordinate status\\n- *galvanic skin response* (GSR) - rises if people hear their own voice\\n\\t- people somtimes can\\'t consciously identify their own voice even when GSR can\\n\\t- they recognize it more when they have more confidence\\n- *glumness*\\n\\t1. self-esteem deflator\\n\\t2. negative reinforcement\\n\\t3. course changer\\n- *split-brain* patients make up reasons\\n\\t- one hemisphere gives them command walk\\n\\t- when asked why walking, they make something up\\n- NYT quote: \"In a week\\'s time, both sides have constructed deeply emotional stories explaining their roles, one-sided accounts that are offered with impassioned conviction, although in many respects they do not stand up, in either case, under careful scrutiny.\"\\n\\n#### darwin\\'s triumph\\n- people were willing to profess incorrect opinions about the relative length of two lines if placed in a room with other people who professed them\\n\\n### morals of the story\\n\\n#### darwinian (and freudian) cynicism\\n- Freud\\n\\t1. id - animal\\n\\t2. ego - interprets id to superego\\n\\t3. superego\\n\\n#### evolutionary ethics\\n- John Mill\\'s *utilitarianism* is a good starting point\\n- morality best preserves non-zero sumness\\n\\n#### blaming the victim\\n- \"genetic determinism\" pops up in court cases (cases like insanity)\\n\\t- notion of free will is shrinking\\n- less retributive of justice - more emphasis on deterrence, improving utilitarianism\\n- rage of juries may wane as they come to believe that male philandering is \"natural\"\\n\\n#### darwin gets religion\\n- doctrines thus far likely have \"harmony\" with human nature\\n- we are designed to believe that next rung on ladder will bring bliss, but in reality it will evaporate shortly after we get there\\n- why religion\\n\\t1. power to religion makers\\n\\t2. mutual benefits for leaders and people\\n\\t3. we came to empathize with all people\\n\\t\\n#### general tips\\n1. distinguish between behavior and mental organ governing it\\n2. remember that mental organ, not behavior, is what was actually designed by natural selection\\n3. these organs may no longer ba daptive\\n4. human mind is incredibly complex\\n\\n## The Righteous Mind: Why Good People Are Divided by Politics and Religion\\n\\n**jonathan haidt, 2012**\\n\\n- questions\\n\\t- eating dead dog\\n\\t- ripping up american flag\\n\\t- sex with chicken\\n\\t- incest\\n- *parochial* - having a limited or narrow outlook or scope; of or relating to a church parish\\n\\n### intuitions come first, strategic reasoning second\\n- elephant and rider metaphor\\n\\n#### where does morality come from\\n1. **nativist** - morality is innate\\n2. **empiricist** - morality is from childhood learning\\n3. **rationalist** - morality is self-constructed by children on the basis of their experience with harm\\n\\t- kids know harm is wrong because they hate to be harmed and learn its is wrong to harm others\\n\\t- came to reject this answer\\n- new study\\n\\t- moral domain *varies by culture*\\n\\t\\t- rich, westerners tend to differentiate between social constructions and moral harms while others don\\'t\\n\\t\\t- westerners are *individualistic* - harm and fairness\\n\\t\\t- other cultures are *sociocentric*\\n\\t- disgust and disrepect drive reasoning - moral reasoning is posthoc fabrication\\n- in fact, morality is probably some combination of 1 & 2\\n\\n#### intuitive dog and rational tail\\n1. Plato - reason (mind) is master of emotions\\n2. Hume - reason is the servant of passions\\n3. Jefferson - reason and sentiment are indpeendent co-rulers\\n- Haidt believes in 2\\n- Antonio Damasio writes Descartes\\' error where patients are missing ventromedial prefrontal cortex (vmPFC)\\n\\t- they couldn\\'t have emotion\\n\\t- where difficult to reason without emotion - too many choices\\n- *intuitionism* - calls  reasoning *rider* and intution *elephant*\\n\\t- rider developed to help elephant\\n\\t- *social intuitionism* - other people can alter intuitions\\n\\t\\n#### elephants rule\\n- brain can make snap judgements in 1/10 second\\n\\t- can predict 2/3 outcomes of senate / house elections based on attractiveness in this time\\n- intuitions come first, strategic reasoning second\\n- smells etc can influence our moral judgements\\n\\n#### vote for me (and here\\'s why)\\n- conscious reasoning immediately justifies intuitive response\\n- self-esteem doesn\\'t make evolutionary sense, since being in groups was what mattered\\n\\t- rather, self-esteem measure\\'s one\\'s fitness as a mate / group member\\n- experimental evidence for confirmation bias\\n- moral/political matters - we are often *groupish* rather than selfish\\n\\n\\n### there\\'s more to morality than harm and fairness\\n- be suspicious of moral monomists\\n\\n#### beyond WEIRD mentality\\t\\n- WEIRD - western, educated, industrialized, rich, democratic - outliars, but often used\\n- Schweder\\'s three ethics\\n\\t1. *autonomy* - individual rights\\n\\t2. *community* - group relationships\\n\\t3. *divinity* - purity\\n- there is more to morality than harm and fairness\\n\\n#### taste buds of the righteous mind / 7 - the moral foundations of politics\\n- *deontology* - rule-based ethics\\n- moral psychology should be empirical - how the mind works, not how it ought to work\\n- Moral Foundations Theory\\n\\t1. care\\n\\t\\t- evolved to care for young\\n\\t2. fairness\\n\\t\\t- punish cheaters\\n\\t\\t- finding altruistic partner\\n\\t3. liberty\\n\\t4. loyalty\\n\\t\\t- want people that are good team players\\n\\t5. authority\\n\\t\\t- allows us to thrive in hierarchical settings\\n\\t6. sanctity\\n\\t\\t- starts with omnivore\\'s dillema\\n\\t\\t- survive pathogens\\n\\n#### the conservative advantage\\n- Durkheim - basic unit is family, not individual\\n- liberals only really value first three moral foundations\\n\\n### morality binds and blinds\\n\\n#### why are we so groupish\\n- group selection is controversial\\n- here are 3 exhibits defending it\\n\\t1. major transitions produce superorganisms\\n\\t2. shared intentionality generates moral matrices\\n\\t\\t- chimpanzees have no shared intentionality\\n\\t3. genes and cultures coevolve\\n\\t4. evolution can be fast\\n\\n#### the hive switch\\n- two candidates for hive switch\\n\\t1. oxytocin genes\\n\\t2. mirror neurons\\n- hive switch doesn\\'t seem to be for everyone, but rather just for one\\'s group\\n\\n#### religion is a team sport\\t\\n- descriptive definitions - describe what people think are moral\\n- normative definitions - describe what is truly right\\n\\t- utilitarianism\\n\\t- deontology\\n- belief in supernatural - could be accidental as by-product of hypersensitive agency detection device\\n- religion can effectively surpress free-rider problem\\n\\n#### disagreeing more constructively\\n- people are predisposed to ideologies\\n- then there is serious confirmation bias\\n- liberals and conservatives are both necessary to balance each other out\\n- *Manichaeism* - polarization, believing one side only\\n- imagine world with no countries, religion -> would probably be chaos\\n\\n## Predictably Irrational\\n\\n**by Dan Ariely, 2008**\\n\\n- **arbitrary coherence** - market prices themselves that influence consumers\\' willingness to pay. What this means is that demand is not, in fact, a completely separate force from supply.\\n- Choices are always relatives\\n\\t- Adding a comparable worse option makes the comparable option seem better\\n\\t- Supply and demand doesn’t always work\\n\\t\\t- The price for black pearls was completely made up\\n\\t\\t- People often stay anchored to the prices they first see\\n- Social norms compete with market norms\\n\\t- Fining parents who pick up their children late\\n- High price of ownership\\n\\t- Students who won tickets in a lottery would sell them for much more than buy them\\n\\t- Pepsi wins blind taste tests, coke wins shown ones\\n- [“dollar auction”](http://www.smbc-comics.com/index.php?id=3594)\\n\\n## freud\\n\\n- Id – set of instinctual trends\\n  - \"contrary impulses exist side by side, without cancelling each other out. ... There is nothing in the id that could be compared with negation ... nothing in the id which corresponds to the idea of time.\"\\n- Ego – organized and realistic\\n- Super-ego – analyzes and moralizes – mediates between id and ego',\n",
       " '---\\nlayout: notes\\ntitle: representations\\ncategory: ai\\n---\\n\\n#  representations\\n\\nSome notes on knowledge representation based on Berkeley\\'s CS 188 course and  \"Artificial Intelligence\" Russel & Norvig 3rd Edition\\n\\n## intro\\n\\n- AI - field of study which studies the goal of creating intelligence\\n  - *intelligent agent* - system that perceives its environment and takes actions that maximize its chances of success\\n- expert task examples - medical diagnosis, equipment repair, computer configuration, financial planning\\n\\n1. formal systems - use axioms and formal logic\\n2. *ontologies* - structuring knowledge in graph form\\n3. statistical methods\\n\\n- *turing test* - is human mind deterministic { turing1950computing }\\n- *chinese room argument* - rebuts turing test { cite searle1980minds }\\n- *china brain* - what if different people hit buttons to fire individual neurons\\n\\n## symbol search\\n\\n- computer science - empirical inquiry\\n\\n### symbols and physical symbol systems\\n- intelligence requires the ability to store and manipulate symbols\\n- laws of qualitative structure\\n  - cell doctrine in biology\\n  - plate tectonics in geology\\n  - germ theory of disease\\n  - doctrine of atomism\\n- \"physical\"\\n  1. obey laws of physics\\n  2. not restricted to human systems\\n  - *designation* - then given the expression, the system can affect the object\\n  - *interpretation* - expression designates a process\\n\\n### heuristic searching\\n- symbol systems solve problems with *heuristic search*\\n- *Heuristic Search Hypothesis* - solutions are represented as symbol structures. A physical symbol system exercises its intelligence in problem solving by search--that is, by generating and progressively modifying symbol structures until it produces a solution structure\\n    - from { cite newell1976computer }\\n- there are practical limitations on how fast computers can search\\n- To state a problem is to designate\\n    1. a test for a class of symbol structures (solutions of the problem)\\n    2. a generator of symbol structures (potential solutions). \\n- To solve a problem is to generate a structure, using (2), that satisfies the test of (1).\\n- searching is generally in a tree-form\\n\\n## knowledge representation\\n\\n- *physical symbol system hypothesis* - a physical symbol system has the necessary and sufficient means for general intelligent action\\n  - computers and minds are both *physical symbol systems*\\n  - *symbol* - meaningful pattern that can be manipulated\\n  - symbol system - creates, modifies, destroys symbols\\n  \\n- want to represent\\n  1. *meta-knowledge* - knowledge about what we know\\n  2. *objects* - facts\\n  3. *performance* - knowledge about how to do things\\n  4. *events* - actions\\n  \\n- two levels\\n  1. knowledge level - where facts are described\\n  2. symbol level - lower\\n  \\n- properties\\n  1. representational adequacy - ability to represent\\n  2. inferential adequacy\\n  3. inferential efficiency\\n  4. acquisitional efficiency - acquire new information\\n  \\n- two views of knowledge\\n  1. logic\\n    - a *logic* is a language with concrete rules\\n    - *syntax* - rules for constructing legal logic\\n    - *semantics* - how we interpret / read\\n      - assigns a meaning\\n    - multi-valued logic - not just booleans\\n    - higher-order logic - functions / predicates are also objects\\n    - multi-valued logics - more than 2 truth values\\n      - fuzzy logic - uses probabilities rather than booleans\\n    - match-resolve-act cycle\\n  2. associationist\\n    - knowledge based on observation\\n      - semantic networks - objects and relationships between them\\t\\t- like is a, can, has\\n      - *graphical representation*\\n      - equivalent to logical statements\\n      - ex. nlp - conceptual dependency theory - sentences with same meaning have same graphs\\n      - *frame representations* - semantic networks where nodes have structure\\n        - ex. each frame has age, height, weight, ...\\n      - when agent faces *new situation* - slots can be filled in, may trigger actions / retrieval of other frames\\n      - inheritance of properties between frames\\n      - frames can contain relationships and procedures to carry out after various slots filled\\n  \\n- statistical\\n\\n  - distributed - usually different from sparse code (sparser generally less robust)\\n    - opposite of sparse code = dense code\\n    - have to check multiple indexes\\n    - penti\\'s work: distributed\\n    - usually want these to be robust\\n    - nlp is main place where unsupervised pretraining widely used\\n  - hierarchical\\n\\n  - good representations - *linearly separable*\\n  - representation that *factors*\\n  - information bottleneck method: want simple representation that keeps class but throws away lots of extraneous info\\n\\n## expert systems\\n- *expert system* - program that contains some of the subject-specific knowledge of one or more human experts.\\n- problems\\n  1. planning\\n  2. monitoring\\n  3. instruction\\n  4. control\\n- need lots of knowledge to be intelligent\\n- *rule-based architecture* - condition-action rules & database of facts\\n- acquire new facts\\n  - from human operator\\n  - interacting with environment directly\\n- forward chaining\\n  - until special HALT symbol in DB, keep following logical rule, add result to DB\\n- conflict resolution - which rule to apply when many choices available\\n- *pattern matching* - logic in the if statements\\n- *backward chaining* - check if something is true\\n  - check database\\n  - check if on the right side of any facts\\n- *CLIPS* - expert system shell\\n  - define rules and functions...\\n- *explanation subsystem* - provide explanation of reasoning that led to conclusion\\n- people\\n  1. *knowledge engineer* - computer scientist who designs / implements ai\\n  2. *domain expert* - has domain knowledge\\n1. user interface\\n2. knowledge engineering - art of designing and building expert systems\\n  - determine characteristics of problem\\n  - *automatic knowledge-acquisition* - set of techniques for gaining new knowledge\\n    - ex. parse Wikipedia\\n    - crowdsourcing\\n- creating an expert system can be very hard\\n  - only useful when expert isn\\'t available, problem uses symbolic reasoning, problem is well-structured\\n- *MYCIN* - one of first successful expert systems { cite shortliffe2012computer }\\n  - Stanford in 1970s\\n  - used backward chaining but would ask patient questions - sometimes too many questions\\n- advantages\\n  - can explain reasoning\\n  - can free up human experts to deal with rare problems\\n\\n## Godel, Escher, Bach\\n**Douglas Hofstadter, 1979**\\n\\n### meta\\n\\n- *strange loop* = paradox - self-referential\\n- zen enlightenment\\n  - goal: transcend **dualism** = division into concepts (perception, words do this)\\n  - words give you some truth but always fail to describe some parts of the truth\\n\\n### music\\n\\n- **canons** - repeat w/ subtle changes (e.g. pitch shift)\\n- **fugue** - repeat w/ more substantial changes\\n\\n### ai\\n\\n- essential abilities - **can we do these things unsupervised?**\\n  - to recognize the relative importance of different elements of a situation\\n  - to find similarities between situations despite differences which may separate them;\\n  - to draw distinctions between situations despite similarities which may link them\\n  - to synthesize new concepts by taking old concepts and putting them together in new ways\\n- intelligence consists of rules at different levels\\n  - \"just plain\" rules - like reflexes which respond to stereotyped situations\\n  - metarules - when situations are mixtures of steretoyped situations, requires rules for deciding which \"just plain\" rules to apply\\n  - rules for inventing new rules - when situations can\\'t be classified\\n    - rules may have to change themselves\\n- messages - comparison of DNA to a jukebox\\n  - where is info stored? records? buttons? smashed buttons?\\n  - what could constitute a Rosetta stone for DNA codes?\\n  - 3 parts\\n    - frame - tells you that this is a message\\n    - outer - tells you how to read a message (e.g. language, style)\\n    - inner - actual content\\n  - if decoding is universal, we might call the outer message (e.g. the trigger) the message\\n- memory - same bits can be used for different things - part of each message specifies the instruction type\\n\\n### brain\\n\\n- intelligence involves a calculus of descriptions = symbols\\n  - symbols represent both classes + instances (maybe both depending on amount of activation / context) - def need some context\\n  - can have links to other symbols (+priors on these)\\n  - *top-down logical structure*??\\n  - different ways to combine symbols get blurry\\n    - symbols can be learned to branch, merge\\n  - can harness temporal firing rates to encode more\\n  - can grow incrementally (greedily)\\n- analogy of thoughts as trips on a (poorly fleshed out) map\\n\\n### interpretation\\n\\n- ex. top is decimal expansion of the sum of the second ($\\\\pi/4$)\\n  - 7, 8, 5, 3, 9, 8, 1, 6, ...\\n  - 1, -1/3, +1/5, -1/7, +1/9, -1/11...\\n\\n### math / logic\\n\\n- **godel\\'s thm** - limitation of any formal axiomatic system: cannot make a program to find a complete + consistent set of axioms\\n- **church-turing thesis** - a function on the natural numbers can be calculated by an effective method if and only if it is computable by a Turing machine\\n  - no system can do computation which cannot be broken down into simple elements\\n- **decision procedure** - decides whether something is a theorem - must terminate\\n- we can think of theorems as strings in a formal system\\n- **interpretation** - correspondence between symbols and words\\n  - ideally, these are meaningful isomorphisms between codes and reality\\n  - not all interpretations imply meaningful (or valid) corresponding codes\\n  - there might be multiple, equally valid interpretations\\n  - consistency depends on interpretation:\\n  - **consistency** - when every theorem, upon interpretation, comes out true (in some imaginable world)\\n  - **completeness** - when all statements which are true (in some imageinable world), and which can be expressed as well-formed strings of the system, are theorems\\n- slightly different axioms lead to elliptical/hyperbolic geometry instead of Euclidean geometry\\n- godel numbering - can replace all symbols w/ numbers and all typographic rules w/ arithmetic rules\\n- 2 key idesas\\n  - strings can speak about other strings\\n  - self-scrutiny can be entire concentrated into a single string\\n- every aspect of thinking can be viewed as a high-level description of a system which, on a low level, is governed by simple, even formal rules\\n\\n### causality\\n\\n- what counterfactuals are the most realistic\\n  - different things are stable at different levels\\n\\n### biology\\n\\n- dna -> rna -> proteins = sequence of amino acids\\n  - folds w/ valrious levels of structure (like music)\\n- self-rep - what counts?\\n  - quine? instructions on jukebox? human reproduction?',\n",
       " '---\\nlayout: notes\\ntitle: decisions\\ncategory: ai\\ntypora-copy-images-to: ../assets\\n---\\n\\n#  decisions\\n\\nSome notes on decision theory based on Berkeley\\'s CS 188 course and  \"Artificial Intelligence\" Russel & Norvig 3rd Edition\\n\\n## game trees - R&N 5.2-5.5\\n\\n- like search (adversarial search)\\n- *minimax algorithm*\\n  - *ply* - half a move in a tree\\n  - for multiplayer, the backed-up value of a node n is the vector of the successor state with the highest value for the player choosing at n\\n  - time complexity - $O(b^m)$\\n  - space complexity - $O(bm)$ or even $O(m)$\\n- *alpha-beta* pruning cuts in half the exponential depth\\n  - once we have found out enough about n, we can prune it\\n  - depends on move-ordering\\n    - might want to explore best moves = *killer moves* first\\n  - *transposition table* can hash different movesets that are just transpositions of each other\\n- imperfect real-time decisions\\n  - can evaluate nodes with a heuristic and cutoff before reaching goal\\n  - heuristic uses features\\n  - want *quiescent search* - consider if something dramatic will happen in the next ply\\n    - *horizon effect* - a position is bad but isn\\'t apparent for a few moves\\n    - *singular extension* - allow searching for certain specific moves that are always good at deeper depths\\n  - *forward pruning* - ignore some branches\\n    - *beam search* - consider only n best moves\\n    - PROBCUT prunes some more\\n- search vs lookup\\n  - often just use lookup in the beginning\\n  - program can solve and just lookup endgames\\n- stochastic games\\n  - include *chance nodes*\\n  - change minimax to expectiminimax\\n  - $O(b^m numRolls^m)$\\n  - cutoff evaluation function is sensitive to scaling - evaluation function must be a positive linear transformation of the probability of winning from a position\\n  - can do alpha-beta pruning analog if we assume evaluation function is bounded in some range\\n  - alternatively, could simulate games with *Monte Carlo simulation*\\n\\n\\n### utilities / decision theory -- R&N 16.1-16.3\\n- goal: maximize utility by taking actions (focus on single actions)\\n  - utility function U(s) gives utility of a state\\n  - actions are probabilistic: $P[RESULT(a)=s\\' \\\\vert a,e]$\\n    - s - state, e - observations, a - action\\n- soln: pick action with *maximum expected utility*\\n  - expected utility $EU(a\\\\vert e) = \\\\sum_{s\\'} P(RESULT(a)=s\\' \\\\vert a,e) U(s\\')$\\n- notation\\n  - A>B - agent prefers A over B\\n  - A~B - agent is indifferent between A and B\\n- preference relation has 6 *axioms of utility theory*\\n  1. *orderability* - A>B, A~B, or A<B\\n  2. *transitivity*\\n  3. *continuity*\\n  4. *substitutability* - can do algebra with preference eqns\\n  5. *monotonicity* - if A>B then must prefer higher probability of A than B\\n  6. *decomposability* - 2 consecutive lotteries can be compressed into single equivalent lottery\\n- these axioms yield a utility function\\n  - isn\\'t unique (ex. affine transformation yields new utility function)\\n  - *value function* = *ordinal utility function* - sometimes ranking, numbers not needed\\n  - agent might not be explicitly maximizing the utility function\\n\\n### utility functions\\n\\n- *preference elicitation* - finds utility function\\n  - normalized utility to have min and max value\\n  - assess utility of s by asking agent to choose between s and $(p: \\\\min, (1-p): \\\\max)$\\n- people have complicated utility functions\\n  - ex. *micromort* - one in a million chance of death\\n  - ex. *QALY* - quality-adjusted life year\\n- money\\n  - agents exhibits *monotonic preference* for more money\\n  - gambling has expected monetary value = EMV\\n  - *risk averse* = when utility of money is sublinear\\n    - value agent will accept in lieu of lottery = *certainty equivalent*= *insurance premium*\\n  - *risk-neutral* = linear\\n  - *risk-seeking* = supralinear\\n- *optimizer\\'s curse* - tendency for E[utility] to be too high because we keep picking high utility randomness\\n- *normative theory* - how idealized agents work\\n- *descriptive theory* - how actual agents work\\n  - *certainty effect* - people are drawn to things that are certain\\n  - *ambiguity aversion*\\n  - *framing effect* - wording can influence people\\'s judgements\\n  - *anchoring effect* - buy middle-tier wine because expensive is there\\n\\n## decision theory / VPI -- R&N 16.5 & 16.6\\n\\n- note: here we are just making 1 decision\\n- *decision network* (sometimes called *influence diagram*)\\n  1. *chance nodes* - represent RVs (like BN)\\n  2. *decision nodes* - points where decision maker has a choice of actions\\n  3. *utility nodes* - represent agent\\'s utility function\\n- can ignore chance nodes\\n  - then *action-utility function* = *Q-function* maps directly from actions to utility\\n\\n![decision_nets](../assets/decision_nets.png)\\n\\n- evaluation\\n  1. set evidence\\n  2. for each possible value of decision node\\n    - set decision node to that value\\n    - calculate probabilities of parents of utility node\\n    - calculate resulting utility\\n  3. return action with highest utility\\n\\n### the value of information\\n\\n- *information value theory* - enables agent to choose what info to acquire\\n  - observations only affect agent\\'s belief state\\n  - value of info = difference in best expected value with/without info\\n  - maximum $EU(\\\\alpha|e) = \\\\underset{a}{\\\\max} \\\\sum_{s\\'} P(Result(a)=s\\'|a, e) U(s\\')$\\n- *value of perfect information VPI* - assume we can obtain exact evidence for a variable (ex. variable $T=t$)\\n  - $VPI(T) =  \\\\mathbb{E}_{T}\\\\left[ EU(\\\\alpha|e, T) \\\\right] - \\\\underbrace{EU(\\\\alpha \\\\vert e)}_{\\\\text{original EU}}$\\n  - first term expands to $\\\\sum_t P(T=t \\\\vert e) \\\\cdot EU(\\\\alpha \\\\vert  e, T=t) $\\n    - within each of these EU, we take a max over actions\\n  - VPI not linearly additive, but is order-independent\\n  - intuition\\n    - info is more valuable when it is likely to cause a change of plan\\n    - info is more valuable when the new plan will be much better than the old plan\\n- information-gathering agent\\n  - *myopic* - greedily obtain evidence which yields highest VPI until some threshold\\n  - *conditional plan* - considers more things\\n\\n## mdps and rl - R&N 17.1-17.4\\n\\n- sequences of actions\\n- *fully observable* - agent knows its state\\n- *markov decision process* - all these things are given\\n  - set of states s\\n  - set of actions a\\n  - stochastic transition model $P(s\\' \\\\vert s,a)$\\n  - reward function R(s)\\n    - utility aggregates rewards, for models more complex than mdps reward can be a function of past sequences of actions / observations\\n- want policy $\\\\pi (s)$ - what action to do in state s\\n  - optimal policy yields highest expected utlity\\n- optimizing MDP - *multiattribute utility theory*\\n  - could sum rewards, but results are infinite\\n  - instead define objective function (maps infinite sequences of rewards to single real numbers)\\n    - ex. discounting to prefer earlier rewards (most common)\\n      - discount reward n steps away by $\\\\gamma^n, 0<\\\\gamma<1$\\n    - ex. set a *finite horizon* and sum rewards\\n      - optimal action in a given state could change over time = *nonstationary*\\n    - ex. average reward rate per time step\\n    - ex. agent is guaranteed to get to terminal state eventually - *proper policy*\\n- expected utility executing $\\\\pi$: $U^\\\\pi (s) = \\\\mathbb E_{s_1,...,s_t}\\\\left[\\\\sum_t \\\\gamma^t R(s_t)\\\\right]$\\n  - when we use discounted utilities, $\\\\pi$ is independent of starting state\\n  - $\\\\pi^*(s) = \\\\underset{\\\\pi}{argmax} \\\\: U^\\\\pi (s) = \\\\underset{a}{argmax} \\\\sum_{s\\'} P(s\\' \\\\vert s,a) U\\'(s)$\\n\\n### value iteration\\n\\n- *value iteration* - calculates utility of each state and uses utilities to find optimal policy\\n  - *bellman eqn*: $U(s) = R(s) + \\\\gamma \\\\: \\\\underset{a}{\\\\max} \\\\sum_{s\\'} P(s\\' \\\\vert s, a) U(s\\')$\\n  - start with arbitrary utilities\\n  - recalculate several times with *Bellman update* to approximate solns to bellman eqn\\n- value iteration eventually converges\\n  - *contraction* - function that brings variables together\\n    - contraction only has 1 fixed point\\n    - Bellman update is a contraction on the space of utility vectors and therefore converges\\n    - error is reduced by factor of $\\\\gamma$ each iteration\\n  - also, terminating condition -  if $ \\\\vert  \\\\vert U_{i+1}-U_i \\\\vert  \\\\vert  < \\\\epsilon (1-\\\\gamma) / \\\\gamma$ then $ \\\\vert  \\\\vert U_{i+1}-U \\\\vert  \\\\vert <\\\\epsilon$\\n  - what actually matters is *policy loss* $ \\\\vert  \\\\vert U^{\\\\pi_i}-U \\\\vert  \\\\vert $ - the most the agent can lose by executing $\\\\pi_i$ instead of the optimal policy $\\\\pi^*$\\n    - if $ \\\\vert  \\\\vert U_i -U \\\\vert  \\\\vert  < \\\\epsilon$ then $ \\\\vert  \\\\vert U^{\\\\pi_i} - U \\\\vert  \\\\vert  < 2\\\\epsilon \\\\gamma / (1-\\\\gamma)$\\n\\n### policy iteration\\n\\n- another way to find optimal policies\\n  1. *policy evaluation* - given a policy $\\\\pi_i$, calculate $U_i=U^{\\\\pi_i}$, the utility of each state if $\\\\pi_i$ were to be executed\\n    - like value iteration, but with a set policy so there\\'s no max\\n      - $U_i(s) = R(s) + \\\\gamma \\\\: \\\\sum_{s\\'} P(s\\' \\\\vert s, \\\\pi_i(s)) U_i(s\\')$\\n      - can solve exactly for small spaces, or approximate (set of lin. eqs.)\\n  2. *policy improvement* - calculate a new MEU policy $\\\\pi_{i+1}$ using $U_i$\\n    - same as above, just $\\\\pi^*(s) = \\\\underset{\\\\pi}{argmax} \\\\: U^\\\\pi (s) = \\\\underset{a}{argmax} \\\\sum_{s\\'} P(s\\' \\\\vert s,a) U\\'(s)$\\n- *asynchronous policy iteration* - don\\'t have to update all states at once\\n\\n### partially observable markov decision processes (POMDP)\\n\\n- agent is not sure what state it\\'s in\\n\\n- same elements but add *sensor model* $P(e \\\\vert s)$\\n\\n- have distr $b(s)$ for belief states\\n  - updates like the HMM: $b\\'(s\\') = \\\\alpha P(e \\\\vert s\\') \\\\sum_s P(s\\' \\\\vert s, a) b(s)$\\n  - changes based on observations\\n\\n- optimal action depends only on the agent\\'s current belief state\\n\\n  - use belief states as the states of an MDP and solve as before\\n  - changes because state space is now continuous\\n\\n- value iteration\\n  1. expected utility of executing p in belief state is just $b \\\\cdot \\\\alpha_p$  (dot product)\\n  2. $U(b) = U^{\\\\pi^*}(b)=\\\\underset{p}{\\\\max} \\\\: b \\\\cdot \\\\alpha_p$\\n  - belief space is continuous [0, 1] so we represent it as piecewise linear, and store these discrete lines in memory\\n    - do this by iterating and keeping any values that are optimal at some point\\n      - remove *dominated plans*\\n  - generally this is far too inefficient\\n\\n- *dynamic decision network* - online agent ![](../assets/online_pomdp.png) \\n\\n## reinforcement learning -- R&N 21.1-21.6\\n\\n- *reinforcement learning* - use observed rewards to learn optimal policy for the environment\\n  - in ch 17, agent had model of environment (P(s\\'|s, a) and R(s))\\n- 2 problems\\n  - *passive* - given $\\\\pi$, learn $U^\\\\pi (s)$\\n  - *active* - *explore* states to find utilities and *exploit* to get highest reward\\n- 2 model types, 3 agent designs\\n  - model-based: can predict next state/reward before taking action (for MDP, requires learning $P(s\\'|s,a)$)\\n    - *utility-based agent* - learns utility function on states\\n      - requires model of the environment\\n  - model-free \\n    - *Q-learning agent*: learns *action-utility function* = *Q-function* maps actions $\\\\to$ utility\\n    - *reflex agent*: learns policy that maps directly from states to actions\\n\\n### passive reinforcement learning\\n\\n- given policy $\\\\pi$, learn $U^\\\\pi (s) = \\\\mathbb E\\\\left[ \\\\sum_{t=0}^{\\\\infty} \\\\gamma^t R(S_t)\\\\right]$\\n  \\n  - like policy evaluation, but transition model / reward function are unknown\\n- **direct utility estimation**: treat states independently\\n  - run trials to sample utility\\n  - average to get expected total reward for each state = expected total reward from each state\\n- **adaptive dynamic programming** (ADP) - sample to estimate transition model $P(s\\'|s, a)$ and rewards $R(s)$, then plug into Bellman eqn to find $U^\\\\pi(s)$ (plug in at each step)\\n  - we might want to enforce a prior on the model (two ways)\\n    1. *Bayesian reinforcement learning* - assume a prior $P(h)$ on transition model h\\n      - use prior to calculate $P(h \\\\vert e)$\\n      - use $P(h|e)$ to calculate optimal policy: $\\\\pi^* = \\\\underset{\\\\pi}{argmax} \\\\sum_h P(h \\\\vert e) u_h^\\\\pi$\\n        - $u_h^\\\\pi$= expected utility over all possible start states, obtained by executing policy $\\\\pi$ in model h\\n    2. give best outcome in the worst case over H (from *robust control theory*)\\n      - $\\\\pi^* = \\\\underset{\\\\pi}{argmax}\\\\:  \\\\underset{h}{\\\\min} \\\\: u_h^\\\\pi$\\n- **temporal-difference learning** - adjust utility estimates towards local equilibrium for correct utilities\\n  - like an approximation of ADP\\n  - when we transition $s \\\\to s\\'$, update $U^\\\\pi(s) = U^\\\\pi (s) + \\\\alpha \\\\left[R(s) - U^\\\\pi (s) + \\\\gamma \\\\:U^\\\\pi (s\\') \\\\right]$\\n    - $\\\\alpha$ should decrease over time to converge\\n  - *prioritized sweeping* - prefers to make adjustments to states whose likely successors have just undergone a large adjustment in their own utility estimates\\n    - speeds things up\\n\\n### active reinforcement learning\\n\\n- no longer following set policy\\n\\n  - *explore* states to find their utilities and *exploit* model to get highest reward\\n\\n  - must explore all actions, not just those in the policy\\n\\n- *bandit* problems - determining exploration policy\\n\\n  - *n-armed bandit* - pulling n levelers on a slot machine, each with different distr.\\n  - *Gittins index* - function of number of pulls / payoff\\n\\n- coorect schemes should be *GLIE* - greedy in the limit of infinite exploration - visits all states infinitely, but eventually become greedy\\n\\n#### agent examples\\n\\n- ex. choose random action $1/t$ of the time\\n- ex. active adp agent\\n  - give optimistic utility to relatively unexplored states\\n  - uses *exploration function* f(u, numTimesVisited) around the sum in the bellman eqn\\n    - high utilities will propagate\\n- ex. active TD agent\\n  - now must learn transitions (same as adp)\\n  - update rule same as passive TD\\n\\n#### learning action-utility function\\n\\n- $U(s) = \\\\underset{a}{\\\\max} \\\\: Q(s,a)$\\n  - ADP version: $Q(s, a) = R(s) + \\\\gamma \\\\sum_{s\\'} P(s\\'|s, a) \\\\underset{a\\'}{\\\\max} Q(s\\', a\\')$\\n  - TD version: $Q(s,a) = Q(s,a) + \\\\alpha [R(s) - Q(s,a) + \\\\gamma \\\\: \\\\underset{a\\'}{\\\\max} Q(s\\', a\\')]$ - **this is what is usually referred to as Q-learning**\\n- *SARSA* (state-action-reward-state-action) is related: $Q(s,a) = Q(s,a) + \\\\alpha [R(s) - Q(s,a) + \\\\gamma \\\\: Q(s\\', a\\')]$\\n  - here, a\\' is action actually taken\\n- Q-learning is *off-policy* (only uses best Q-value)\\n  - more flexible\\n- SARSA is *on-policy* (pays attention to actual policy being followed) \\n\\n- can approximate Q-function with something other than a lookup table\\n  - ex. linear function of parameters $\\\\hat{U}_\\\\theta(s) = \\\\theta_1f_1(s) + ... + \\\\theta_n f_n(s)$\\n    - can learn params online with *delta rule* = *wildrow-hoff rule*: $\\\\theta_i = \\\\theta - \\\\alpha \\\\: \\\\frac{\\\\partial Loss}{\\\\partial \\\\theta_i}$\\n\\n### policy search\\n\\n- keep twiddling the policy as long as it improves, then stop\\n  - store one Q-function (parameterized by $\\\\theta$) for each action\\n  - ex. $\\\\pi(s) = \\\\underset{a}{\\\\max} \\\\: \\\\hat{Q}_\\\\theta (s,a)$\\n    - this is discontinunous, instead often use *stochastic policy* representation (ex. softmax for $\\\\pi_\\\\theta (s,a)$)\\n  - learn $\\\\theta$ that results in good performance\\n    - Q-learning learns actual Q* function - could be different (scaling factor etc.)\\n- to find $\\\\pi$ maximize *policy value* $p(\\\\theta) = $ expected reward executing $\\\\pi_\\\\theta$\\n  - could do this with sgd using *policy gradient*\\n  - when environment/policy is stochastic, more difficult\\n    1. could sample mutiple times to compute gradient\\n    2. REINFORCE algorithm - could approximate gradient at $\\\\theta$ by just sampling at $\\\\theta$: $\\\\nabla_\\\\theta p(\\\\theta) \\\\approx \\\\frac{1}{N} \\\\sum_{j=1}^N \\\\frac{(\\\\nabla_\\\\theta \\\\pi_\\\\theta (s, a_j)) R_j (s)}{\\\\pi_\\\\theta (s, a_j)}$\\n    3. PEGASUS - *correlated sampling* - ex. 2 blackjack programs would both be dealt same hands -  want to see different policies on same things\\n\\n---',\n",
       " '\\n{:toc}\\n\\n- companies\\n  - [Neuralink](https://waitbutwhy.com/2017/04/neuralink.html)\\n  - Kernel\\n  - [Facebook](https://www.sciencealert.com/facebook-is-working-on-tech-to-let-you-type-with-your-brain-and-hear-with-your-skin)\\n  - IBM: project joshua blue\\n  - Facebook neural typing interface\\n- gov-sponsored\\n  - human brain project\\n    - blue brain project- large-scale brain simulation\\n  - european brain project',\n",
       " '---\\nlayout: notes\\ntitle: brain basics\\ncategory: neuro\\n---\\n\\n#  brain basics\\n\\n## delineations\\n\\n- 3 things\\n  - cortex\\n  - limbic system\\n    - **thalamus** (has pulvinar), **basal ganglia**, hypothalamus, hippocampus, amygdala, ...\\n    - glands / hormones\\n  - brain stem\\n    - midbrain (has superior colliculus), pons, medulla\\n- cerebellum?? (has 60% of neurons)\\n\\n## numbers\\n\\n- human brain\\n  - 86 bil total\\n  - 16 bil in cortex - largest of anything (proportionally)\\n  - 15 tril synapses\\n  - 1 glial cell : 1 neuron\\n  - cooking allowed us to meet the energy requirements for this\\n\\n## whole brain\\n\\n- *Cerebrum* - The cerebrum is the largest portion of the brain, and contains tools which are responsible for most of the brain\\'s function. It is divided into four sections: \\n  - the temporal lobe\\n  - the occipital lobe\\n  - parietal lobe\\n  - frontal lobe.\\n  - The cerebrum is divided into a right and left hemisphere which are connected by axons that relay messages from one to the other. This matter is made of nerve cells which carry signals between the organ and the nerve cells which run through the body.\\n- *Frontal Lobe* -\\xa0This lobe controls a several elements including creative thought, problem solving, intellect, judgment, behavior, attention, abstract thinking, physical reactions, muscle movements, coordinated movements, smell and personality.\\n- *Parietal Lobe* - this lobe focuses on comprehension. Visual functions, language, reading, internal stimuli, tactile sensation and sensory comprehension will be monitored here.\\n  - *Sensory Cortex* -\\xa0The sensory cortex, located in the front portion of the parietal lobe, receives information relayed from the spinal cord regarding the position of various body parts and how they are moving. This middle area of the brain can also be used to relay information from the sense of touch, including pain or pressure which is affecting different portions of the body.\\n  - *Motor Cortex* -\\xa0This helps the brain monitor and control movement throughout the body. It is located in the top, middle portion of the brain.\\n- Temporal Lobe:\\xa0The temporal lobe controls visual and auditory memories. It includes areas that help manage some speech and hearing capabilities, behavioral elements, and language. It is located in the cerebral hemisphere.\\n  - Wernicke\\'s Area-\\xa0This portion of the temporal lobe is formed around the auditory cortex. While scientists have a limited understanding of the function of this area, it is known that it helps the body formulate or understand speech.\\n- Occipital Lobe:\\xa0The optical lobe is located in the cerebral hemisphere in the back of the head. It helps to control vision.\\n  - Broca\\'s Area-\\xa0This area of the brain controls the facial neurons as well as the understanding of speech and language. It is located in the triangular and opercular section of the inferior frontal gyrus.\\n- Cerebellum\\n  - This is commonly referred to as \"the little brain,\" and is considered to be older than the cerebrum on the evolutionary scale. The cerebellum controls essential body functions such as balance, posture and coordination, allowing humans to move properly and maintain their structure.\\n  - 60% of neurons\\n\\n### thalamocortical system\\n\\n- cortex has layers that are connected in columns through 5 layers\\n  - cotex has different types based on distributions of cells through layers (Brodmann map 1909 has 50ish areas)\\n- thalamus has lots of connections - gates what goes into cortex\\n\\n### limbic system\\n\\n- The limbic system contains glands which help relay emotions. Many hormonal responses that the body generates are initiated in this area. The limbic system includes the amygdala, hippocampus, hypothalamus and thalamus.\\n- Amygdala:The amygdala helps the body respond to emotions, memories and fear. It is a large portion of the telencephalon, located within the temporal lobe which can be seen from the surface of the brain. This visible bulge is known as the uncus.\\n- Hippocampus:\\xa0This portion of the brain is used for learning memory, specifically converting temporary memories into permanent memories which can be stored within the brain. The hippocampus also helps people analyze and remember spatial relationships, allowing for accurate movements. This portion of the brain is located in the cerebral hemisphere.\\n- Hypothalamus:The hypothalamus region of the brain controls mood, thirst, hunger and temperature. It also contains glands which control the hormonal processes throughout the body.\\n- Thalamus:The Thalamus is located in the center of the brain. It helps to control the attention span, sensing pain and monitors input that moves in and out of the brain to keep track of the sensations the body is feeling.\\n\\n### brain stem\\n\\n- All basic life functions originate in the brain stem, including heartbeat, blood pressure and breathing. In humans, this area contains the medulla, midbrain and pons. This is commonly referred to as the simplest part of the brain, as most creatures on the evolutionary scale have some form of brain creation that resembles the brain stem. The brain stem consists of midbrain, pons and medulla.\\n- Midbrain:The midbrain, also known as the mesencephalon is made up of the tegmentum and tectum. These parts of the brain help regulate body movement, vision and hearing. The anterior portion of the midbrain contains the cerebral peduncle which contains the axons that transfer messages from the cerebral cortex down the brain stem, which allows voluntary motor function to take place.\\n- Pons:\\xa0This portion of the metencephalon is located in the hindbrain, and links to the cerebellum to help with posture and movement. It interprets information that is used in sensory analysis or motor control. The pons also creates the level of consciousness necessary for sleep.\\n- Medulla:\\xa0The medulla or medulla oblongata is an essential portion of the brain stem which maintains vital body functions such as the heart rate and breathing.\\n\\n## 1 - introduction\\n- ![](../assets/map.png)\\n### genomics\\n\\n- male Drosophila uses body position and environment to add rhythmic notes to song\\n  - female uses this to gauge male\\'s brain\\n- *neural circuits* make up *neural systems*\\n- neural systems serve 3 general functions\\n  1. sensory systems\\n  2. motor systems\\n  3. associational systems - link the other two, higher order functions\\n- *gene* has coding DNA (*exons*) and regulatory DNA (*introns*)\\n- model organisms\\n  - cat - visual cortex\\n  - squid and sea slug had really large neurons\\n  - 4 species: worm C. elegans, Drosophila, zebrafish Danio rerio, mouse Mus musculus\\n    - complete genome is available for them\\n    - can try *homologous recombination* - splicing in new genes\\n- human genome has ~20k genes, ~14k expressed in brain, ~6k expressed only in brain\\n- single-gene mutations can cause diseases like microcephaly\\n- simulate brain as a computer\\n  - passive cabling equation\\n  - theoretical neuroscience\\n  - blue brain project\\n  - human brain project\\n\\n### cellular components\\n- *neuron doctrine* by Ramon y Cajal / Sherrington replaces Golgi\\'s *reticular theory*\\n  - Cajal uses Golgi\\'s salt-staining method to show neurons are distinct\\n  - Sherrington finds synapses\\n  - there are rare *gap junctions* between neurons (where there are *electrical synapses*)\\n1. neurons\\n  - number of inputs reflects *convergence*\\n  - number of targets reflects *divergence*\\n  - *local circuit neurons* = *interneurons* - short axons\\n  - *projection neurons* - long axons\\n2. glial cells - support and regeneration\\n  - outnumber neurons 3:1\\n  - they are stem cells - can generate new glia\\n  - maintaining ion gradients, modulating nerve signals, modulating synaptic action, scaffolding, aiding recovery\\n  1. *astrocytes* - in CNS, maintain chemical environment, retain stem cell properties\\n  2. *oligodendrocytes* - in CNS, lay down myelin - in PNS, *Schwann cells* do this\\n  3. *microglial* cells - remove debris\\n  - glial stem cells - make more glia and sometimes neurons\\n\\n### cellular diversity\\n\\n- ~10^11 neurons, more glia\\n- histology - microscopic analysis of cells\\n- stains\\n  - Golgi stain - randomly stains only some neurons\\n  - Nissl stain - only stains cell bodies\\n  - Myelin stain - only stains myelin\\n\\n### neural circuits\\n\\n- *neuropil* - bundle of dendrites/axons/glia - where synaptic connectivity occurs\\n- *afferent* neuron - carries info toward the brain\\n- *efferent* neuron - carries info away\\n- *myotatic reflex* example - knee-jerk\\n\\n### organization of the human nervous system\\n1. sensory systems\\n2. motor systems\\n\\n3. *CNS*\\n  - brain\\n  - spinal cord\\n4. *PNS*\\n  - sensory neurons \\n  - *somatic* motor division - connect CNS to skeletal muscles\\n  - *autonomic* or *visceral* motor division - innervate muscles / glands\\n    - *autonomic ganglia* - peripheral motor neurons that take inputs from brainstem / spinal cord\\n    - *enteric system* - small ganglia / neurons in gut that influence gastric motility and secretion\\n    - *sympathetic* division - ganglia lie near the vertebral column and sent their axons to a variety of targets\\n    - *parasympathetic* division - ganglia are found near organs they innervate\\n- groupings\\n  - *ganglia* - accumulations of cell bodies / supporting cells\\n  - *nerve* - bundles of axons in PNS\\n  - *tract* - bundles of CNS axons\\n    - if they cross the brain midline called *commisures*\\n  - *nuclei* - local accumulations of similar neurons\\n  - *cortex* - sheet-like arrays of neurons\\n  - *gray matter* - has more cell bodies\\n  - *white matter* - has more axons\\n\\n### neural systems\\n1. unity of function - divide things into different systems - ex. visual\\n2. representation of information - ex. vision can be *topographic map*, taste can be *computational map*\\n3. subdivision into subsystems - ex. vision has color, form, motion, all in parallel\\n\\n### structural analysis\\n- often-used *lesion studies*\\n- *anterograde* - source to termination\\n- vs *retrograde* - terminus to source\\n\\n### functional analysis of neural systems\\n1. *electrophysiological recording* - uses electrodes, neuron-level\\n  - can determine *receptive field* - region in sensory space where a specific stimulus elicits a spike\\n2. *functional brain imaging* - noninvasive, records local activity\\n  - computerized tomography (CT), magnetic resonance imaging (MRI), diffusion tensor imaging (DTI), PET, SPECT, fMRI, MEG, MSI\\n\\n### analyzing complex behavior\\n- *cognitive neuroscience* - understanding higher-order functions\\n- *neuroethology* - complex behaviors of animals\\n\\n## 2 - electrical signals of nerve cells\\n- *microelectrode* - fine glass tubing filled with good conductor\\n- all cells have a voltage difference across them\\n  - assume resting potential - we\\'ll use -58 mV\\n  - *depolarized* - less negative - we\\'ll use +58 mV\\n- potentials\\n  1. *receptor potential* - (small) due to the activation of sensory neurons by external stimuli\\n    - at terminal of dendrite\\n    - *graded* - depends on how strong the input is\\n  2. *synaptic potential* - (small) caused by activation of synapse\\n    - at middle of dendrite\\n  3. *action potential* - cause by the other 2\\n    - at the axon\\n- *active transporters* create differences in concentrations of specific ions - battery\\n- cells can be depolarized by adding too much K+ outside\\n- *ion channels* - make membranes selectively permeable - wires\\n- ions\\n  - outside: high Na, Cl; low K\\n  - generally 10:1 ratio between inside, outside\\n  - inside of cell has a bunch of negative proteins to balance chlorine\\n  - ions want to spread out because of entropy (then they factor in charge difference)\\n- $V_{ion} = 58/z * log(X_{out} / X_{in}) $\\n  - calculate for each ion, if able to move\\n  - z is charge on ion\\n  - for potassium: 58/1 * log(.1) = -58mV\\n  - Cl Nernst potential is actually -70 mV (even though we assumed -58 before)\\n  - Cl works as an inhibitor - ex. alcohol lets chloride in\\n  - whichever ion leaks, this determines the membrane potential\\n- hodgkin-huxley\\n  - large squid escape neurons\\n  - adding K+ outside depolarizes the cell\\n  - adding Na+ outside raises height of spike\\n\\n## 3 - voltage-dependent membranes\\n- voltage clamp - one electrode outside, one inside\\n  - measured with reference to outside (usually more negative inside)\\n  - keeps voltage constant\\n  - current clamp - keeps current constant\\n- current clamp - just measures the voltage without interfering\\n- patch-clamp - suction part of cell into pipette\\n- passive properties\\n  - current injection: $V_t = V_{\\\\infty} (1-e^{-t/ \\\\tau})$\\n  - voltage decay: $V_t = V_{\\\\infty} e^{-t / \\\\tau}$\\n- block Na+ current with tetrodotoxin\\n- block K+ current with tetraethyl-ammonium\\n- *refractory period* is because Na needs to stop being inactivated\\n  - Na+ is *transient*, K+ is not\\n- *myelin* insulates sections - less ion loss\\n  - called *saltatory conduction*\\n  - faster and more efficient\\n  - concentrates action potential to nodes\\n  - without myelin, 10 m/s with myelin, 100 m/s\\n  - deals with JAM receptor system\\n  - umyelinated can be ok\\n    - might want to regenerate\\n    - don\\'t care about speed\\n  - PNS\\n    - Schwann cells\\n    - loss - Guillan-barre syndrome\\n  - CNS\\n    - oligodendrocytes\\n    - loss - multiple sclerosis (MS)\\n\\n## 4 - ion channel transporters\\n- patch electrode - pull a piece of membrane out ![](../assets/4_A.png)\\n  1. cell-attached - don\\'t break membrane\\n  2. whole-cell - break membrane\\n  3. inside-out - inside of membrane is outside electrode\\n    4. outside-out - outside of membrane is outside electrode\\t- this method is always preferred\\n    - tetrodotoxin binds to outside of cell membrane\\n- some channels are delayed\\n- self-inactivating = *transient* - channels turn off by themselves\\n\\n  - take 10-20 ms\\n- voltage-gated channels\\n\\n  - Na+, K+, Ca, Cl\\n- frog Xenopus Ooctyes ion channels are studied\\n- TRP channels gated by mechanical / heat\\n- 4 K+ channels\\n  1. delayed rectifier\\n  2. fast acting - shapes AP, used for hearing\\n  3. late phase - slow ending, makes AP fire again\\n  4. inward rectifier - open at resting potential - establishes membrane potential\\n- mitten model - protein rotates around\\n- human genes: 10 Na, 10 Ca, 100K, ~5 Cl\\n\\n  - there are more types of potassium channels\\n- channel-opathies - diseases can be caused by altered ion channels\\n- ion transporters\\n  - ATPase Pumps\\n    - *Na+/K+ pump*\\n    - Ca pump\\n  - ion exchangers\\n  - Na+/K+ pump exchanges 3Na for 2K ions\\n    - 1/3 of body\\'s energy\\n    - 2/3 of neuron\\'s energy\\n    - brains use 20% of body\\'s energy\\n    - Ouabain blocks this\\n  - SERT - Na transporter\\n    - co-transporter\\n- ligand-gated channels\\n  - respond to a chemical\\n  - usually allow Na, K, Cl to flow in and out\\n\\n## 5 - synapses\\n### synapse types\\n1. electrical synapses\\n  - gap junction channels\\n  - ions flow through gap junction channels\\n  - presynaptic and postsynaptic cell are almost the same\\n  - delay is fast (.1 ms)\\n  - gap junction proteins have been showing up in diseases\\n  - simple organisms have these\\n2. chemical synapses\\n  - *bouton* - end of presynaptic dendrite\\n  - *spines* - start of postsynaptic dendrite\\n  - voltage-gated Ca comes in and causes vesicles to fuse with presynaptic membrane\\n  - neurotransmitter released\\n  - bind to ligand-gated channels which let ions flow through\\n    - if Cl flows into postsynaptic cell - inhibition\\n  - pumps get rid of neurotransmitters quickly\\n\\n### neurotransmitter types\\n- released when Ca comes in due to depolarization\\n1. peptides\\n  - ex. oxytocin\\n  - require long Ca exposure\\n  - loaded into vesicles up by the cell body - can take days to get to bouton\\n  - neurotransmitter diffuses away - doesn\\'t always have specific target\\n  - can spread to all neurons in the area (ex. substance P)\\n2. small & fast\\n  - glutamate, Ach, GABA\\n  - loaded into vesicles in bouton\\n  - presynaptic cell takes these back up\\n\\n### discovery\\n- neurotransmitter lifecycle\\n  - synthesis -> receptors -> function -> removal\\n  - important that they are removed\\n  - 60 s to recycle\\n- Loewi\\'s experiment showed that neurotransmitters can flow through solution to synchronize heart\\n\\n### synaptic transmission\\n- *minis* = *MEPP* - not big enough to fire the neuron\\n  - you can treat a muscle as a post-synaptic junction\\n  - chatter from single vesicle release\\n  - *quantal basis of neurotransmitter release* - 1,2,3,etc because vesicles release as all-or-none\\n  - synapses / vesicles are all about the same size across different species\\n    - receptors receive these neurotransmitters differently\\n- each vesicle is covered with proteins\\n  - *SNAPs* on plasma membrane\\n  - *SNAREs* on vesicle\\n    - ex. *synaptobrevin*\\n    - botulinum toxin, tetanus toxin - block synaptobrevin - clip other proteins\\n    - render a vesicle inactive\\n  - they recognize each other and lock for *priming* - ready to release when Ca+ enters\\n- need to endocytose membrane to make more vesicles\\n  - endocytosis includes *clathrin* which curves the membrane\\n- can measure single ligand-gated channel by clamping it alone\\n\\n## 6 - neurotransmitters\\n### receptors\\n- ionotropic\\n\\n  | Name   | AMPA GluR-x \\t| NMDA NR-x \\t| Kainate\\t| GABA-A \\t| glycine \\t| Nicotinic Ach \\t| 5HT-3     \\t| P2x purinergic \\t|\\n  |--------|-------------\\t|-----------\\t|---------\\t|--------\\t|---------\\t|---------------\\t|-----------\\t|----------------\\t|\\n  | Abbrev | AMPA        \\t| NMDA      \\t| Kainate \\t| GABA   \\t| Glycine \\t| nACh          \\t| Serotonin \\t| Purines        \\t|\\n  | Ion    | Na          \\t| Na/Ca     \\t| Na      \\t| Cl     \\t| Cl      \\t| Na            \\t| Na        \\t| Na             \\t|\\n- metabotropic\\n\\n  | Name     | mGlu-x    \\t| GABA-B \\t| D-x                 \\t| Alpha, beta, adrenergic \\t| H-x                 \\t| 5HT1-7           \\t| Purinergic A or P \\t| Ach Muscarinic-x \\t|\\n  |----------|-----------\\t|--------\\t|---------------------\\t|-------------------------\\t|---------------------\\t|------------------\\t|-------------------\\t|------------------\\t|\\n  | Abbrev   | Glutamate \\t| GABA_B \\t| Dopamine            \\t| NE,Epi                  \\t| Histamine           \\t| Serotonin        \\t| Purines           \\t| Muscarinic       \\t|\\n  | Function |           \\t|        \\t| cocaine, ADHD drugs \\t| antianxiety             \\t| unkown, probs sleep \\t| 3 is ionotropic! \\t|                   \\t| mushroom drugs   \\t|\\n- ![](../assets/6_1.png)\\n- ![](../assets/6_4.png)\\n### excitatory\\n1. *Acetylcholine* - excites muscle cells\\n  - receptors: nAch\\n  - Acetylcholinesterase breaks down Ach after it is released\\n  - neurotoxins (ex. Serin) break down Acetylcholinesterase so Ach stays and muscles stay on (nerve gases)\\n  - Myasthinia is when you develop antibodies against your own nAch receptors\\n  - you have trouble controlling your eyes\\n2. *Glutamate* - excites pyramidal cells\\n  - VGLUT pumps Glutamate into vesicles\\n  - EATT transports extracellular Glutamate into presynaptic terminal / nearby Glial cells\\n    - Glial cells convert glutamate to glutamine (inactivates) and glutamine is taken up by the presynaptic terminal\\n    - glutamate overload if you overload the inactivating pumps in the glial cells\\n  - glutamate receptors\\n    - AMPA - fast Na only\\n    - NMDA - slow, Na and Ca and also requires depolarization\\n\\n### inhibitory\\n3. ,4. *GABA*/*glycine*\\n  - have simple transporters that move released GABA into presynaptic terminal / Glia\\n  - Ionotropic GABA receptors - depressants; shut down nervous system\\n    - can bind steroids like estrogen - different effects in men / women\\n    - alcohol binds to this, shuts things down\\n- immature neurons have high intracellular Cl - people don\\'t know why\\n\\n### neuromodulators\\n- lifecycle molecules\\n  - synthesis: L-Dopa, trytophan\\n  - reuptake: DAT, NET, SerT\\n  - breakdown: MAO\\n  - vesicular transport: VMAT\\n5. *catecholamines*\\n  - pathway: DOPA -> Dopamine -> Norepinephrine -> Epinephrine\\n  - dopamine - forming of memories\\n    - produced by substantia nigra\\n    - loss of these neurons -> Parkinson\\'s\\n  - norepinephrine \\n    - produced by locus coeruleus\\n6. *serotonin* = 5HT - happiness\\n  - Tryptophan -> serotonin\\n  - serotonin produced by Raphe nuclei\\n  - affected by LSD\\n7. *histamine* - not well-known\\n  - antihistamines can make you hallucinate\\n8. *atp* - sensitivity to pain\\n9. *neuropeptides* - slow\\n   - substance P - pain\\n   - alpha-endorphin - analgesia (block pain) \\n     - vasopressin - blood pressure \\n   - thyrotropin releasing hormone (TRH) - metabolism \\n   - neuropeptide Y - mood/aggression \\n10. *enndocannabinoids* - weed\\n  - ex. anandamide  - binds to CB1 (cannabanoid 1 receptor)\\n  - retrograde signal - post back to pre - inhibit the inhibitor\\n  - this increases the signal\\n11. *nitric oxide* - gas\\n   - binds to guanylyl cyclase\\n\\n## 7 - molecular signaling\\n### localization\\n- chemical signaling mechanisms\\n  1. *synaptic* - local\\n    - ex. Ach\\n  2. *paracrine* - medium distance, neurotransmitter sprinkled and nearby targets take it up\\n    - ex. serotonin\\n  3. *endocrine* - get into your blood stream - body-wide signaling\\n    - ex. vasopressin\\n- amplification = enzyme\\n  - signal that activates enzyme amplifies signal\\n- cell-signaling molecules\\n  1. *cell-impermeant* molecules\\n    - need transmembrane receptors\\n    - ex. glutamate\\n  2. *cell-permeant* molecules (steroids)\\n    - can have intracellular receptors\\n  3. *signaling* molecules\\n    - adhesion molecules - like a lock and key - binds neurons together\\n- *spine* has small neck - hard for proteins to go through it\\n  - keeps information local\\n  - large raft of signaling molecules keeps info local\\n\\n### celllular receptors\\n1. **ionotropic** - channel-linked receptors\\n  - neurotransmitter binds to a channel that opens\\n  - ex. Glu ionotropic receptor\\n  - signal is sodium coming in\\n2. *enzyme-linked* receptors\\n  - ex. TrkA NGF receptor - Tyrosine kinase\\n  - once it binds, it becomes an enzyme\\n3. **metabotropic** - G-protein-coupled receptors\\n  - ex. Glue metabotropic receptor\\n  - activate G-protein that then does something\\n  - these require energy for G-proteins\\n  1. Heterotrimeric G-proteins\\n    - G-protein has 3 subunits: - ![](../assets/7_6.png)\\n    1. Gs - binds norepinephrine\\n      - + cAMP\\n    2. Gq - binds glutamate\\n      - + DAG (diaglycerol) & IP3\\n    3. Gi - binds dopamine\\n      - inhibits cAMP\\n  2. Monomeric G-proteins\\n    - G-protein has just one subunit\\n    - ex. Ras\\n4. *intracellular* receptors\\n  - ex. estrogen receptors\\n  - activates intracellular transcription factors\\n\\n### second messengers\\n- Ca must be pumped out of neuron or\\n- Ca can be stored into internal stores in ER\\n- Adenylyl cyclase turns ATP into cAMP -> PKA\\n- Guanylyl cyclase turns GTP into cGMP -> PKG\\n  - would use inside-out patch to study these\\n- Phospholipase C -> PKC -> IP3 lets Ca out of ER (usually kills cell)\\n\\n### protein control\\n- kinases (on switch) add phosphate to protein and makes them active\\n  - PKA - cAMP binds then catalytic domain can bind\\n    - non-covalent so can diffuse into nucleus\\n  - CAM kinase\\n    - covalent\\n  - protein kinase C\\n     - covalent - very local to membrane\\n- phosphatases (off switch) remove phosphates\\n\\n### long-term alteration\\n- long-term alteration requires epigenetic changes (change transcription factors)\\n- transcription factor *CREB* requires three things at once\\n  1. Ca comes in and binds Cam kinase\\n  2. activate Protein kinase A - this can stay in nucleus for a while\\n    - how much to make\\n  3. MAP kinase, ras\\n    - on/off switch\\n  - when all of these things come in at once - convergence signaling - CREB will make actin, AMPA receptors\\n  - \\n- *TrkA* binds NGF (Nerve growth factor) - peptide and has 3 pathways:\\n  1. PI 3 kinase\\n  2. ras\\n  3. Phospholipase C\\n  - this displays divergence signaling\\n- cerebellar synapses \\n  - mGlu inhibits AMPA with negative feedback\\n  - lets out Ca which depresses AMPA receptor\\n- signal scaling - tyrosine hydroxylase makes dopamine\\n  - more Ca in bouton activates more hydroxylase -> makes more dopamine\\n  - Ca comes in whenever fires\\n  - *use it or lose it*\\n\\n  ## 8 - synaptic plasticity\\n### short-term\\n- measured by firing neuron before muscle and recording response\\n  1. synaptic facilitation - frequency-dependent plasticity - fire faster, get bigger EPSPS\\n    - Ca comes in and persists during next pulse\\n    - ms time scale\\n  2. synaptic depression - transmitters are depleted\\n    - s time scale\\n  3. synaptic potentiation/augmentation - changes in presynaptic proteins\\n    - min time scale\\n- experiments\\n  - *habituation* - decrease vesicles on sensory neuron after too much stimuli\\n  - *sensitization* - associate two stimuli together: ![](../assets/8_5.png)\\n- mechanism\\n  - sensory neuron -> serotinergic neuromodulatary interneuon -> motor neuron\\n  - interneuron releases serotonin \\n  - cAMP produced in sensory neuron\\n    -  long term - CREB in nucleus - synapse growth\\n    -  central signal for LTM\\n    -  activates PKA\\n  - short term - decreases K+ current\\n- sensory neuron doesn\\'t learn, neuromuscular junction gets stronger\\n- mutant genes associated with cAMP identified\\n  - phosphodiesterase - if you remove this, too much cAMP\\n  - adenylate cyclase\\n\\n### long-term\\n- HM lost his memory w/out hippocampus\\n  - site of LTP\\n  - at night memories are moved from hippocampus (flash drive) to cortex (long-term hard drive)\\n- Schaffer collateral pathway is one pathway in hippocampus (also perforant pathway, mossy fiber)\\n  - pre-stimulate with tetanus\\n  - later when stimulated EPSP is bigger\\n  - usually 20 ms between firing - LTP when multiple firing in less time\\n- Schaffer mechanism - NMDA receptor key to this\\n  - both AMPA and NMDA exist\\n  - NMDA -> Ca -> CAM kinase -> LTP\\n  - Mg blocks NMDA unless already depolarized\\n  - requires good timing!\\n- silent synapses \\n  - short-term - more AMPA receptors, long-term new synapses\\n  - all synapses born with only NMDA\\n  - protein synthesis needed for LTP (mostly making more AMPA)\\n  - unclear how synapse decides whether to strengthen / make more synapses\\n- LTD - long-term depression is opposite of LTP - long-term potentiation\\n  - low levels of Ca lead to AMPA being endocytosed\\n  - mGlu -> PKC -> starts LTD\\n- epilepsy - neurons fire together and wire together',\n",
       " '---\\nlayout: notes\\ntitle: Vision\\ncategory: neuro\\n---\\n\\n#  vision\\n\\n## neural signals\\n\\n- ion channel properties\\n  - gating energy - how is the channel activated\\n  - ionic selectivity - which ions pass through\\n- high ap velocity when these are high\\n  - channel density\\n  - channel kinetics\\n  - axon diameter\\n  - axon surface resistance\\n  - $\\\\lambda = \\\\sqrt{\\\\frac{r_m}{r_a}}$\\n- synapse types\\n  - electrical\\n  - chemical\\n- postsynaptic receptors\\n  - ionotropic receptors - directly gated\\n  - metabotropic receptors - indirectly gated through 2nd messengers\\n- If ion reversal potential is 0 (ex. $E_{Cl}$ sometimes) then shunting = divisive inhibition\\n\\n## electrophysiology\\n\\n- EEG - whole brain\\n- ERG (electroretinogram) - whole retina\\n- single cell\\n  - sharp micro-electrode - has problems\\n    - high resistance\\n    - poor seal with membrane\\n    - mechanically unstable\\n  - patch recording - uses suction to overcome powers\\n    - Flaw: bad for studying second-messenger systems because inside of electrode / cell fuse\\n    - different types (whole cell, outside-out, inside-out)\\n- clamp \\n  - ![](../assets/clamp_main.png)\\n  - ![](../assets/clamp_junction.png)\\n  - ![](../assets/clamp_tip.png)\\n- types\\n  - whole-cell\\n  - cell-attached\\n  - inside-out\\n  - outside-out\\n  - whole cell perforated - generally better, but difficult\\n- recording types\\n  - current-clamp - record potential\\n  - voltage clamp - record current - most common\\n  - conductance-clamp - complicated\\n- IV curve - measured with voltage clamp\\n  - V - voltage clamped at\\n  - I - maximal current evoked by clamping at this voltage\\n- ![](../assets/circuit_model.png)\\n\\n## recording + imaging\\n\\n### electrode arrays\\n\\n- multi-electrode array recording (MEA)\\n  - well-suited for retina\\n  - can now get several thousand electrodes\\n  - put retina onto MEA to record ganglion cells\\n  - waves of activity spread accross retina during development - probably important for wiring retina\\n  - patch ~ 500µm accross\\n- spike sorting\\n  - cluster spikes from different neurons based on amplitude, wave shape, refractory period violations\\n- spatiotemporal white noise stimulu - sequence of stimulus frames with randomly assigned pixel intensities (Bernoulli or Gaussian)\\n- *spike-triggered average stimulus* - averagin frames that correspond to spikes - yields receptive field\\n  - requires finding timing (too short and spike won\\'t fire, too long and won\\'t repeatedly fire)\\n- retinal cell ganglion classification\\n  - cluster by STA timecourse and autocorrelation pca\\n  - after clustering, receptive fields of any cluster don\\'t overlap too much\\n- cell mosaics - ganglion cells tile entire retina\\n- ganglion cell receptive field instead of one blob is several small blobs (the cone array)\\n  - each blob corresponds to one cone cell\\n- cones: red, green, blue cones are random\\n  - ganglion midget cells contain color information - make red-green connections\\n  - this is found in STA\\n  - connects to broad set of cells, not just closest\\n\\n### imaging - voltage\\n\\n- **voltage-sensitive dyes** - would be great\\n  - could provide spatially localized, non invasive recordings\\n  - doesn\\'t exists - usually toxic and inefficient (small fluorescence change / voltage change)\\n    - APs short and small area limiting number of photons\\n    - subthreshold PSPs (postsynaptic potentials) only have small voltage change\\n  - electrochromic - fast, low-sensitivity\\n  - quenching/FRET - slow, high capacitance\\n  - photo-induced electron transfer - fast, high sensitivity, low capacitance\\n    - currently being developed by evan miller at berkeley in chemistry\\n    - problem - lights up all the cells - trying to target a cell with genetics\\n- **calcium imaging** \\n  - calcium influxes into cell through a variety of mechanisms\\n  - calcium indicators\\n    - original: aequorin - bioluminescent protein from jellyfish\\n    - calcium indicator - calcium binds to fluorophore and changes its shape, which changes its fluorescence\\n    - fret-based - calcium brings together two proteins\\n    - now most common: GCaMPs\\n  - two-photon imaging in the retina\\n    - infrared stimulus to drive laser (can\\'t use light, would stimulate retina)\\n    - T. Euler has been leader in this field\\n      - can simultaneously attach electrode and measure single spikes while calcium imaging\\n    - Ca signal slower than electrical signal - can lose some things\\n    - lots of functional types of retinal ganglions cells (>32?)\\n      - respond to different stimuli\\n      - different morphology\\n\\n\\n## rod and cone photoreceptor function\\n\\n- retina - large metabolic rate\\n  - at the back of the eye, fairly regular array\\n  - ~1.2 mil optic nerve fibers\\n- ![](../assets/layers.png)\\n- pigment absorbs stray photons to reduce noise\\n- cones\\n  - ~6 mil cones\\n  - low sensitivity\\n  - fast responses\\n  - don\\'t saturate\\n  - selective for the direction of light rays\\n- rods\\n  - ~120 mil \\n  - high sensitivity to light\\n  - slow responses\\n  - saturate\\n- ![](../assets/receptor_densities.png)\\n- *photo-transduction* - converts photons into voltage-changes\\n- terminals\\n  - cone *pedicle*\\n  - rod *spherule*\\n  - glutamate release modulated by voltage + Ca\\n\\n\\n## horizontal cells - outer retinal signaling and lateral inhibition\\n\\n- on-center - responds to white small circle\\n- off-center - responds to black small circle\\n- horizontal cells - if you make circle to big, these inhibit the photoreceptors\\n- adjust for mean by shifting calcium with hc surround antagonism (5 possible biphysical mechanisms)\\n  - extracellular pH\\n  - ephaptic mechanism\\n- natural scenes contain strong spatial correlations\\n  - *predictive coding* - use surrounding regions to predict the center value\\n  - subtract predicted value from actually measured value\\n  - send nothing if you could have predicted, otherwise info you send is *interesting*\\n\\n## signaling pathways through the retina - amacrine cells and inner retinal processing of visual information\\n\\n- bipolar cells begin parallel signalling in the visual system\\n- amacrine cells - generally modulate bipolar cells / ganglion cells\\n  - very structurally diverse: *glycinergic* = narrow-field, *GABAergic* = wide-field\\n- way more cones than ganglion cells\\n- ganglion cells have object motion sensitivity\\n  - generated by lateral inhibition\\n- *starburst amacrine cells* - generate directional signals in the retina\\n\\n\\n## retinal ganglion cells\\n\\n- takes inputs from bipolar cells\\n\\n### primate retina\\n\\n- primate retina has 2 major types of ganglion cells\\n  1. midget ganglion cells  (~75%) - majority, high spatial acuity\\n     1. parvocellular pathway\\n  2. parasol ganglion cells (~15%) - high temporal resolution\\n     1. magnocellular pathway\\n  3. little known about most other types of ganglion cells\\n- we don\\'t have aliasing because visual system filters out the high frequencies\\n- color\\n  - red-green keeps center surround\\n  - blue-yellow doesn\\'t\\n- direction-selective ganglion cells\\n  - activated when the image moves on the retina\\n  - specific allele can get rid of these (affects GABAergic starburst amacrine cells)\\n  - different dendrites represent different directions (inputs are pretty symmetrical)\\n\\n## non-neuronal retina stuff\\n\\n### retinal glia\\n\\n- glia greek for \"glue\"\\n- 3 types ![](../assets/glia.png)\\n- Muller cells\\n  - from multiplotent retinal progenitor cells (same that make neurons)\\n    - in fish, with damage muller cells can become neurons (forced in mammals)\\n  - abundant, tile the retina\\n  - functions\\n    - mechanical support\\n    - energy storage\\n    - clearing waste products\\n    - neurotransmitter recycling (ex. glutamate-glutamine cycling)\\n    - metabolism\\n    - K+ homeostasis (uptake and redistribution)\\n  - disease - *gliosis* - upregulation of intermediate filaments common in many retinal diseases\\n- astrocytes\\n  - originate from brain, enter via optic nerve\\n  - look star shaped\\n  - functions: lots of neurovascular\\n    - cell bodies don\\'t move, but processes constantly move\\n  - disease - become reactive in many retinal diseases\\n- microglia\\n  - myeloid origin\\n  - concentrated in synaptic layers\\n  - immune cells - phagocytosis\\n  - disease - activation occurs with / before retinal cell death\\n  - microglial depletion alters retinal synapses $\\\\implies$ microglia maintain synapses (wang et al. 2016 j neurosci)\\n  - microglia are highly motile and respond dynamically to stimuli (e.g. neurotransmitters)\\n  - microglia respond dynamically to injury\\n\\n### retinal pigment epithelium (rpe)\\n\\n- monolayer of pigmented, hexagonally-shaped epithelium cells\\n- surround outer segments of photoreceptors\\n- cells have tight junctions\\n- functions\\n  - main: light absorption\\n  - epithelial transport - require water + photoreceptor cycling + oxygen\\n    - photo-oxidation causes damage - photoreceptor tip constantly being phagocytosed, base regenerated - completely renewed in 11 days\\n  - visual cycle - recycling retinoids\\n    - number of diseases involve this\\n  - others: phagocytosis, secretion, glia\\n\\n### retinal blood supply\\n\\n- retina has highest metabolic demand of any tissue\\n\\n- vasculature - eye is only place we can noninvasively view vasculature\\n\\n  - often diagnose stuff like hypertension / diabetes fom eye\\n\\n- 2 major blood supplies (non-overlapping)\\n\\n  1. outer retinal blood supply (past RPE) = posterior ciliary arteries\\n     1. outer 1/3\\n     2. bruch\\'s membrane\\n     3. choroid - helps provide nutrients, cool retina\\n  2. inner retinal blood supply  = central retinal artery\\n     1. inner 2/3\\n     2. no inner capillaries in *foveal avascular zone* - area near fovea (these would block light)\\n\\n- *blood-retinal barriers (BRB)*\\n\\n  - outer BRB - tight junctions between RPE cells\\n    - *fenestrated* - leaky\\n  - inner BRB - tight junctions between *capillary endothelial cells* \\n    - *non-fenestrated*\\n\\n- autoregulation of retinal blood supply\\n\\n  - outer - under sympathetic control\\n  - inner - autoregulated by neuronal demands (neurovascular coupling)\\n\\n\\n## retinal diseases\\n\\n- these are the 3 retinal diseases deepmind is studying\\n\\n### age-related macular degeneration\\n\\n- degenerates outer retina  (photoreceptors/RPE/choroid) in **macula** = central 5-6 mm of retina (contains fovea, which is central 1.5mm)\\n- leading cause of vision loss in >50 yo\\n- big spot missing in center of visual field\\n- 2 types\\n  - dry = non-exudative = non-neovascular\\n  - wet = exudative = neovascular\\n- usually get dry then wet (worse)\\n- *OCT* = optical coherence tomography\\n  - visualizes cross-sectional retina view with infrared\\n\\n#### dry\\n\\n- **drusen** - show up as yellow spots on fundus image\\n- **lipofuscin** - undigested material from photoreceptor turniover accumulates in the RPE (and some in Bruch\\'s membrane)\\n- drusen - leads to degeneration of photoreceptors\\n  - inflammation - activates immune cells\\n  - probably death of RPE leads to death of photoreceptors\\n\\n#### wet\\n\\n- bleeding\\n- treatments\\n  - 1980s - burn small holes to reduce oxygen demand\\n  - 1990s - photodynamic therapy - kind of like cauterizing wound\\n  - 2000s - anti-vascular endothelial gworth factor (VEGF) therapies\\n    - stops more leaky vessels from growing\\n    - must be injected into eye every 1-2 months (maybe longer over time)\\n    - can recover a decent amount\\n    - can be very expensive ~10k / dose\\n\\n### diabetic retinopathy\\n\\n- leaky blood vessels - often diagnose diabetes through retinopathy\\n- diabetes types\\n  - type 1 - autoimmune reaction destroys pancreating $\\\\beta$ cells that produces **insulin**: hyperglycemia + hypoglycemia\\n  - type 2  - reduced insulin sensitivity: mainly hyperglycemia\\n    - need insulin injection\\n- diabetic retinopathy types\\n  - proliferative\\n    - very bad\\n  - non-proliferative\\n    - *microangiopathy* - mainly affects capillaries\\n    - patients probably won\\'t notice this unless its in the macula - this is why diagnosis w/ ml could be useful\\n- neuronal changes seem to precede vascular changes\\n- treatment - *intravitreal injections of anti-VEGF treatments*\\n\\n### glaucoma\\n\\n- optic neuropathy with ganglion cell death and visual field loss\\n- people say \"pressure in the eye\" - but this is just a risk factor\\n- lose your periphery slowly\\n- *optic nerve*\\n  - 1-2.2 million ganglion cell axons\\n  - ~40% of total afferent input to the brain/\\n- *cup* - region where there are no axons\\n\\n\\n## experimental methods\\n\\n![](../assets/method_coverage.png)\\n\\n- 7 dimensions\\n\\t- spatial res.\\n\\t- temporal res.\\n\\t- depth - how deep in can you image\\n\\t- toxicity - does it damage the cells\\n\\t- spatial field - how big a region can you see\\n\\t- temporal duration (how long it can stay in)\\n\\t- invasiveness\\n\\n### single cell\\n\\n- fluorescence imaging\\n  - gfp - protein, gets spine-level precision\\n  - calcium imaging - not a protein, but still does fluorescence\\n- microelectrode recording\\n  - extracellular recording\\n    - can measure local field potentials - sum of local currents (most of the volume is in the dendrites - not a great proxy for spikes)\\n    - can go deep\\n    - spikes are less ambiguous than in calcium imaging\\n  - intracellular recording\\n    - can measure membrane potential much more precisely\\n  - strengths\\n    - great temporal resolution\\n  - weaknesses\\n    - invasive\\n    - single neurons has potential biases\\n      - 80 neurons are not representative\\n      - more likely to record from excitatory, bigger neurons\\n      - unnatural stimuli\\n\\n### alteration\\n\\n- optogenetic probes\\n  - light-sensitive opsins are genetically modified - when light shines on it, does something (depolarize, hyperpolarize, alter intracellular signaling)\\n  - delivery\\n    - viral infection\\n    - transgenic animals\\n    - electroporation\\n- transcranial magnetic stimulation\\n  - coil sits on head, induces current\\n  - pulse is brief - 1 ms\\n  - functional effects are long - milliseconds, minutes, days...\\n  - uses\\n    - enhance neural function\\n    - probe excitability\\n    - explore functional anatomy\\n    - \"virtual lesions\" - but lingers, ...\\n- local microstimulation with invasive electrodes possible\\n\\n### measure electromagnetic signals\\n\\n- EEG (electroencephalography)\\n\\n  - recorded on scalp (only gets synchronous activity)\\n\\n  - can analyze frequencies (higher frequencies like gamma are attenuated)\\n\\n    - | delta      | theta | alpha | beta  | gamma |\\n      | ---------- | ----- | ----- | ----- | ----- |\\n      | 0.5-4 (Hz) | 4-8   | 8-13  | 13-30 | 30-50 |\\n\\n  - can analyze event-related potentials (when the signals peak)\\n\\n  - ECOG = electrocorticography - put electrodes on brain (for patients)\\n\\n- MEG (magenetoencephalography)\\n\\n  - measure magnetic fields generated by active neurons\\n  - fMRI type setup\\n  - higher spatial resolution\\n  - signal is not really distorted by skull (magnetic field goes through better)\\n\\n### dyes\\n\\n- voltage-sensitive dyes\\n  - leaks over everything - can\\'t select for single neurons / spikes\\n  - looks at large spatial field, but can\\'t resolve single cells\\n  - dyes are toxic to neurons over time\\n\\n### blood\\n\\n- intrinsic signal optical imaging\\n  - shine in light and see what\\'s reflected - oxygenated is more reddish, deoxygenated more bluish\\n  - have to expose surface of brain (but can do pre-surgical imaging in humans)\\n  - spatial res limited by capillaries (100 micrometers)\\n  - temporal res - slow because neurovascular coupling is slow\\n- fMRI\\n  - applied magnetic field - very large, homogenous magnetic field\\n  - pulse of energy in radiofrequency range excite protons\\n  - protons emit energy at resonance frequency proportional to local magnetic field strength (this called *nuclear magnetic resonance*)\\n  - magnetic field gradients allow for spatial localization of MR signal in 3d\\n  - rate of energy decay in brain depends on local biochemistry + **oxygenated hemoglobin**\\n  - *BOLD* = blood oxygenation level-dependent signal\\n    - blood oxygenation has linear relationship with decay\\n  - pipeline\\n    - activity -> more oxygen use + cerebral blood flow -> magnetic field distortions -> MRI signal intensity\\n      - activity make oxygen go down real quick then blood flow over compensates, then saturates ![](../assets/hrf.png)\\n      - can\\'t separate excitatory / inhibitory\\n    - LFP predicts BOLD well (only slightly better than MUA)\\n      - all are really pretty well correlated\\n  - fMRI analysis\\n    - block design - usually have block of nothing between stimuli blocks\\n      - activation statistics - compare activation between conditions\\n    - event-related design - all trials analyzed over time\\n\\n### PET\\n\\n- PET\\n  - fMRI type setup\\n  - radioactive thing (positron) put into brain\\n  - as it decays, can triangulate things\\n  - cells with most FDG (a tracer) are using the most glucose\\n  - now used for studying neurotransmitter maps\\n  - good for studying specific radiolabeled tracers\\n  - relatively poor spatial res (1 cm), temporal res is minutes\\n  - minor risk\\n\\n### mapping visual cortex\\n\\n- cortical flat mapping\\n  - gray matter has ~ 100,000 somas / mm^3, ~3km axon /mm^3\\n  - folds in cortical topology make some things much farther than they seem\\n  - most stuff is white matter\\n  - sulci - negative curvature (indent)\\n  - gyri - postive curvature (outdent)\\n  - some sulci are common among all people (landmarks)\\n- columns are organized into \"pinwheels\" - columns in a circle prefer different orientations in spinning pattern\\n- weaknesses\\n  - expensive\\n  - poor temporal / spatial resolution\\n- topographic mapping\\n  - pimary visual cortex - visual maps seem to be replicated\\n- history\\n  - gun people - between franco-prussian war, russo-japanese war\\n    - sir joseph whitworth\\n    - alfred drupp\\n    - william ellis metford\\n    - faster bullets would leave, cauterize wound, more local cuts\\n    - tatsuji inouye - studied soldiers after gunshot wounds (1909) - learned map of retina in the back of the brain\\n  - mapping\\n    - each visual areas has some retinotopic mapping\\n    - moving points\\n    -  ![](../assets/vmap.png)\\n      - periodic mapping stimuli - rotating wedge / other things (fig left) $\\\\implies$ between visual areas (e.g. v1 + v2) mirror image tiling of locations (borders between regions are like mirrors)\\n      - eccentricity mapping (fig right) - rings spread out evenly - no mirrors\\n    - cortical magnification - way more v1 for central locations\\n      - more than proportion of retina dedicated to v1\\n      - somehow this is offset to give us our normal perception\\n    - population receptive field mapping\\n      - predefine each fMRI voxel will respond to Gaussian\\n      - over time show stimulus\\n      - optimization model finds best point point for each voxel\\n      - more efficient than periodic mapping...\\n\\n#### magnetic resonance spectroscopy (mrs)\\n\\n- just like NMR - put in something like water\\n- gives chemical composition of a region of brain\\n  - slow, low resolution\\n- basically pick something (like GABA) and just look at where that is\\n  - certain molecules appear the same though....\\n\\n#### event-related optical signal\\n\\n- =near-infrared spectroscopy - changes blood-oxygentation\\n\\n- active neural tissue scatters infrared light\\n\\n  - infrared imaging shows which neurons are active\\n  - some issues....blond haired people can\\'t do it\\n  - great temporal res, poor spatial res\\n\\n- bunch of sources / detectors on a helmet\\n\\n- suitable for a lot of patients that can\\'t do fMRI (children, patients, ...)\\n\\n  - people can walk around\\n\\n\\n## info processing in the visual system\\n\\n- vision is ill-posed problem\\n- light reflection depends on multiple things\\n  - light source\\n  - material\\n  - angle\\n  - atmospheric properties\\n- lens sorts EM waves by direction\\n  - also lets you get in more light compared to pinhole camera\\n- brain must use priors to create repr. of world\\n- ex. image of a cow - data on retina is messy but you perceive something better\\n- ex. mooney faces - can create stories from images after shading\\n- ex. brown and orange are same hue, but different brightness\\n  - 3d shadow helps make this better\\n- *color constancy* - see same colors under different lighting conditions\\n- very large individual differences in ratios of l, m, s cones\\n- eyes develop very suddenly around cambrian explosion ~500 mya\\n  - eyes takes ~500k years (very fast) - nilsson & pelger 1994\\n- fish have evolved spherical lens several times independently\\n  - can understand lenses if we understand optics\\n  - can understand brain if we understand principles...\\n- compound eyes (ex. fly) repeat dots each get slightly shifted version of world\\n  - collects lots of light - operates at very high speed (e.g. fly h1 neuron bialek 2001)\\n- sand wasp can find its nest based on pattern of stuff that surrounds its nest\\n- jumping spider has interesting visual system\\n\\n## dynamic range of rods + cones\\n\\n- spontaneous isomerizations determine lower limit of light detection\\n- rods\\n  - saturate\\n- cones\\n  - turn on then off\\n- lateral inhibition - improves contrast\\n- horizontal cells\\n  - HI (type B)\\n    - dendrites contact rods, M/L cones\\n    - connected via gap junctions\\n  - HII (type A)\\n    - dendrites contact S, M/L cones\\n  - ![](../assets/horizontal.png)\\n- bipolar cells\\n  - rod v. cone\\n  - on v. off\\n  - midget v. diffuse\\n- amacrine cells\\n  - link bipolar cells to ganglion cells\\n\\n## whitening and tiling\\n\\n- **redundancy reduction** (horace barlow 1961) - pixels are correlated, want to compress info\\n  - second-order statistics (auto-correlation function) - pixel correlation vs. spatial separation of pixels\\n  - power spectrum of natural images - average power over angles\\n    - power goes down for higher spatial frequencies (white noise would be flat)\\n    - seems to go down as ~1/freq universally\\n  - optic nerve should send unpredictable signals ~look like white noise, decorrelated\\n    - shouldn\\'t be able to predict one nerve from the others\\n    - this is because nerves + spikes are expensive\\n    - multiply by filter = frequency to get filter that is flat (whitening - atick & redlich 1992)\\n      - this is a lowpass filter\\n      - this filter looks like center-surround 2D filter (looks like edges)\\n    - wasn\\'t feasible to do this experiment (measuring optic nerve) until recently but has been shown to be true\\n  - people have characterized spatiotemporal power spectrum of natural scenes\\n    - ex. dan et al. 1996 - LGN neurons whiten time-varying natural images but not white noise\\n- **efficient coding model** (karklin + simoncelli 2012)\\n  - why need on and off type - duplicates each cone\\n    - hypothesis: RGCs send spikes which are discretized (not continuous like all the signals within retina)\\n    - differences need to be able to send positive/negative rates - this requires having 2 cells\\n    - alternatively could have a baseline and send more or less but baseline signal is wasteful because usually send 0\\n      - still don\\'t send 0 as 0 firing rate because too slow to send a 0\\n  - try to maximize information out of firing rates - penalty on firing rates \\n    - simulation with some neurons on natural images\\n    - yields on-center / off-center neurons\\n  - optic nerve has pretty equal firing rates - don\\'t get pca type solns\\n- tiling  - ratio varies with eccentricity (higher cone:ganglion ratio in periphery)\\n  - bipolar:ganglion is 1:1 in fovea\\n  - dendritic field diameter increases linearly with eccentricity\\n  - cones also increase \\n  - RGCs limit peripheral vision not cones\\n  - picture here!!! smoothing and subsampling by RGCs\\n  - scale-invariant sampling lattice - same amount of RGCs regardless of how far smth is\\n    - but more photoreceptors per ganglion cell (**maybe higher signal to noise ratio**)\\n    - ex. with letters getting bigger in surround\\n\\n## visual pathways\\n\\n### visual pathways\\n\\n- ![](../assets/visual_pathways.png)\\n- organization\\n  - thalamus + cortex always work together = thalamocortical system\\n  - midbrain - reptilian, old\\n  - all sensory must pass through thalamus to get to cortex\\n  - vision must pass through LGN (also stuff goes back to pulvinar)\\n  - different RGCs that do different things have different dendritic field diameters\\n- 6 main targets\\n  - LGN (thalamus)\\n    - has 6 layers\\n  - superior colliculus (midbrain)\\n    - eye movements - sensory map on top of motor map - can make eyes move to a location\\n      - map centered at eye location\\n    - thought to be reflexive\\n    - without LGN, get blindsight - can catch a ball, dodge things, ...\\n  - suprachiasmatic nucleus\\n    - circadian rhythm\\n    - gets input from special photosensitive RGCs that are big + slow\\n  - accessory optic system\\n  - pretectum - pupillary light reflex\\n  - pregeniculate\\n\\n### lgn\\n\\n- has six layers\\n- parvocellular are upper 4 layers\\n  - subdivided by on/off, left/right\\n  - inputs from midget cells\\n- magnocellular are bottom 2 layers\\n  - inputs from parasol cells\\n- losing magno seems to lose spatial frequency, control different temporal frequencies, parvo gives you color\\n- **important** - different spatial/temporal frequencies - differentiate from the beginning\\n  - ![](../assets/magno_parvo.png)\\n\\n### color\\n\\n- L, M, S cones (red, green, blue)\\n- have learned more from psychophysics than from neural recording\\n- psychophysics: adapt to one axis of color\\n  - people habituate to directions based on changes of cones (ex. adapting to S cone doesn\\'t affect L & M cones)\\n- color oponnency in LGN (derrington et al. 1984) - 2 types of color cells in LGN\\n- cone response distributions\\n  - L and M correlate a lot\\n  - L and S correlate a little\\n  - pca goes to luminance, $\\\\alpha$, $\\\\beta$\\n    - luminance dominates\\n- color really takes only ~10% more space than black-and-white\\n  - decompose into luminance image and 2 color difference images\\n  - non-luminance image requires less bits (can be blurred, less bits)\\n\\n\\n### eye movements\\n\\n1. saccades - old, human vision is fundamentally dynamic\\n   1. head also moves while eye moves and they help counteract each other\\n   2. when in bite bar, saccadic movements are bigger\\n2. fixational eye movements\\n   1. new paper - motion helps you see by sampling cone array\\n      1. could also be that image fades on retina (ex. troxler fading)\\n   2. michael land did lots of cool things\\n\\n## v1 (primary)\\n\\n- pathways\\n  - right part of eyes go to right brain (sees left visual field)\\n  - corpus collosum connects left/right brains but nothing else\\n    - when you lose it, like there are 2 people within person\\n    - very difficult stitching problem...\\n  - left and right lgn, superior colliculus, ...\\n  - regions defined by having a topographic map\\n    - for later areas, histological differences, connectivity, physiological properties\\n      - connections are all bidirectional\\n- ![vmap_main](../assets/vmap_main.png)retinotopy\\n  - v1 has map of retina that\\'s flattened (proportional to ganglion cells)\\n  - probably no \"fixed up\" image of the world somewhere\\n  - could understand perception in terms of action\\n- LGN wires don\\'t interact with each other too much\\n  - some inhibitory interneurons\\n- cortex ![column](../assets/column.png)\\n  - ~2mm thick\\n  - layer 1 - mostly axons\\n  - layer 2/3 - association layer\\n    - lots of neurons connect to each other\\n  - layer 4 - input layer\\n    - further subdivided (parvo goes to one, magno to another)\\n  - layer 5 - output layer (usually to motor)\\n- comes back tho LGN (adam sillito)\\n  - unclear what it does - experiments with cooling cortex don\\'t yield too many profound changes\\n- neurons from different eyes go to different regions of layer 4 (and then mix in layer 2/3)\\n  - **ocular dominance columns** (only within layer 4) - one eye has bands, other eye has other bands\\n  - **hypercolumn** - put 2 eyes together\\n    - lots of metabolic blobs tiles this\\n    - contains lots of neurons with different orientation selectivities\\n    - monkey has ~1k hypercolumns in V1\\n    - contains 100k neurons\\n    - 14 x 14 pixel array (coming in from thalamus)\\n    - 1 mm^2 of cortex contains 100k neurons\\n- v1 is highly overcomplete - way more neurons than needed\\n- electrophysiology\\n  - electrode pics up microvolts\\n- v1 properties not in LGN\\n  - orientation selectivity\\n  - direction selectivity\\n  - simple cells sum LGN inputs\\n- retina and other things are wired up before birth\\n- standard model of V1\\n  - neurons have oriented receptive fields (inhibited by bars around bar) with some temporal component\\n  - response normalization, based on neighboring neurons\\n  - pointwise non-linearity\\n- bruno doesn\\'t really believe this leads to perception\\n  - neurons are highly nonlinear\\n  - recurrent circuits of neurons are even more nonlinear\\n  - there is no general method for characterizing nonlinear systems\\n- good model\\n  - should be in the structure of layers\\n  - why do we need so many neurons\\n- problems\\n  - biased sampling - single units, ignore inhibitory, only find neurons that fire for what you want\\n  - biased stimuli - bars/spots/etc.\\n  - biased theories - data-driven vs. theory\\n  - interdependence and context of scene\\n  - ecological deviance\\n- power spectrum\\n  - horizontal spatial frequency of 0 - vertical grating\\n  - fft function assumes image at boundary is tiled - artifacts giving artificial edges (spatial frequencies of 0)\\n    - could attenuate function at edges to fix this\\n\\n\\n## extrastriate cortex\\n\\n- striate cortex - v1 (has some kind of stripe - not striatum)\\n- extrastriate cortex - everything else\\n- all areas have one part on each hemisphere\\n- orientation columns - columns have similar orientation preferences\\n  - doesn\\'t have to do with ocular dominance columns\\n  - laterally within layers get all orientations in very small area\\n  - repeated - one orientation will be represented lots of times\\n  - different columns represent different xy coordinates\\n- overview\\n  - dorsal stream - where\\n    - mt (middle temporal area)\\n    - spatial visual pathway - positional relationships\\n    - vision for action pathway\\n  - ventral stream - what\\n    - v4\\n    - object recognition pathway\\n    - high resolution and form\\n- 10x more feedback, no strict motor areas, but lots of visuomotor areas\\n- V2 / V3 aren\\'t clearly in either stream\\n\\n### dorsal stream\\n\\n- adaptation - like psychophysicist\\'s electrode\\n- area MT (middle temporal of macaque, although farther back in human)\\n  - has preferred motion orientation columns\\n- visual area STS (superior temporal sulcus) responds to biological motion\\n  - ex. 12 dots look like people\\n- parietal cortex also important for spatial attention\\n- biomotionlab is cool\\n\\n### ventral stream\\n\\n- V1 -> V4 -> IT ->LGN\\n- we\\'re constantly adjusting for changes in illumination\\n- v4\\n  - v4 seems to correspond to perceived colors not wavelengths\\n  - damage to v4 stops you from seeing in color\\n  - selectivity of v4 responses\\n  - mixed magno and parvo inputs (ferrera et al. 1994)\\n- IT\\n  - single column thing...\\n    - jennifer aniston cell\\n    - hand\\n    - Halle Berry cell\\n  - columnar architecture in IT also though....\\n    - pseudo semantic columnar architecture (ex. facial perspective)\\n- face perception orientation can be discriminated by newborn baby (meltzoff)\\n  - can also imitate faces\\n  - babies have trouble resolving high frequencies\\n- areas\\n  - FFA - face selective, fusiform face area\\n    - might not be faces, could be expertise\\n  - PPA - places, parahippocampal place area (surrounds hippocampus)\\n  - things are assymetric in unclear ways (although they contain representations of different visual fields)\\n\\n## sparse coding\\n\\n- **THIS ISN\"T REALLY SPARSE CODING MOVE ELSEWHERE**: power spectrum falls off with frequency as $1/f^2$(amplitude falls as 1/f)\\n  - want to decorrelate - multiply by frequency that\\'s $f^2$\\n  - you can\\'t do this for very high frequencies otherwise you amplify noise\\n  - in spatial domain, looks like center surround\\n  \\t similar to finding edges\\t\\n- v1 has map of space, magnified at fovea\\n- want to explain how center-surround ganglion cells -> elongated orientation selective receptive fields\\n- representation - complete repr. with minimum number of possible neurons\\n  - deeper in cortex - cells become more silent\\n- codes: insert pic!!!!\\n  - dense -> sparse -> local (grandmother) codes\\n- sparse coding has questionable empirical evidence\\n  - lgn fibers around 20 spikes / sec\\n  - layer 4 fires ~ 1 spike /sec\\n  - we don\\'t know this at other layers\\n- sparseness seems pretty constant as you go deeper (Rust & DiCarlo)\\n  - tradeoff between complexity and invariance\\n- V1 simple cells are oriented, localized, bandpass\\n- projection pursuit (Field 1994) - project distr. onto low dim: you should get gaussian\\n  - want to find axes that maximize non-gaussianity\\n- project idea\\n  - look at sparsity in different layers\\n  - ***sparse coding is v1 + retina!!! basis transformation***\\n\\n## object recognition\\n\\n- gabor function - convolve Gaussian with sinusoids of different frequency\\n  - from dennis gabor\\n  - in gabor transform, each basis function has same number of wobbles (self-similar)\\n- at top of visual system goes to entorhinal cortex then to hippocampus\\n- map sizes\\n  - V2 little bigger than V1\\n  - they fold over so that map of V1 goes 1-1 with map of V2\\n- neocognitron is unsupervised\\n- comments:\\n  - \"vision is about more than object recognition so deep nets don\\'t work\"\\n  - turing test for vision\\n- *affordance* = prior\\n- not like deep net which is a top box\\n  - lots of outputs from intermediate areas\\n- perception as inference\\n  - generative model we\\'re trying to fit data too\\n  - bayes rules: $P(E|D) \\\\propto P(D|E) \\\\cdot P(E)$ where E is environment and D is data about environment\\n  - lee + mumford, 2003 - hierarchical bayesian inference in visual cortex\\n    - each area makes guesses and higher areas send back corrections\\n    - mumford - fields medalist\\n    - in real life, we are constantly guessing and trying to resolve ambiguities\\n\\n## top-down modulation\\n\\n- attention is most-studied (refers to some different things)\\n  - *endogenous attention* - voluntary, slow, effortful, interruptible\\n  - *exogenous attention* - involuntary, fast effortless, disruptive\\n- attention is about more than where the eye is pointing\\n- *change blindness* - blind to things you\\'re not attending to\\n  - invisible gorrila, door study\\n- *covert attention* - posner cueing task\\n\\n### endogenous\\n\\n- better studied because it\\'s hard to disentangle stimulus vs. attention in exogenous case\\n- v4 very filtered by attention (ex. reynolds + chelazzi 04)\\n- also effects of attention in area v1 (ex. pick a side to attend to while fixating in center)\\n- IPS1 seem to have maps of attended stimuli (but ignore other stimuli)\\n- frontal eye fields - microstimulation forces eye movement\\n  - can stimulate enough to attend, but not to saccade\\n\\n### exogenous\\n\\n- inhibition of return - if we have attended a region, less like we return to that region\\n\\n### visual search\\n\\n- having more similar objects makes it difficult\\n- feature-integration theory - different visual features are coded in parallel in separate feature maps (orientation, size, color)\\n  - *conjunction search* - conjunction of features (ex. red circle) takes longer\\n- plenty of other areas\\n  - ADD, alzheimers, intermodal attention, applied attention, attentional tracking, neurochemistry of attention, feature- and object-based attention...\\n  - ex. driving - people in car will stop talking in serious situations unlike on phone\\n- **predictive coding** - unpredicted response evokes larger response\\n  - fits with bayesian method - only need large response when you don\\'t predict what\\'s going to happen\\n  - in this way, prediction is opposite to attention\\n\\n## visual neuropsychology\\n\\n- blindsight - damage to V1; aren\\'t aware of visual stimuli but can do tasks in forced-choice paradigms\\n  - retina goes through some things (ex. pulvinar) that aren\\'t V1 to get to higher order areas (ex. MT)\\n- dorsal pathway\\n  - MT monkey lesions in monkeys impair motion perception but not contrast detection\\n    - damage to MT causes motion blindness - life is a set of snapshots\\n- ventral pathway\\n  - damage to V4 causes loss of color perception, can\\'t even imagine colors\\n  - patient DF (well-known) - can only do vision for perception + action, couldn\\'t describe it = *visual agnosia* - perception as an object is impaired\\n  - lots of different kinds\\n  - *prosopagnosia* - can\\'t recognize faces - FFA, PPA (up to 1%)\\n- spatial neglect - failure to acknowledge objects in field contralateral to the lesion\\n  - sometimes group things and only look at right sides of groups\\n  - very weird - has strange reference frames\\n  - functionally very similar to having blindness on one side\\n\\n## visual cortical development + plasticity\\n\\n- development things\\n  - neurons of right types generated in appropriate places\\n  - migrate to final positions\\n  - differentiate into final forms\\n  - axons must follow right paths\\n  - neurons must refine synaptic connections\\n  - brain must remain flexible\\n- neurons ride up glial fibers until it stops - cortical layers develop inside-first (tracked)\\n- axon projections - some axons have to travel very far to connect (ex. LGN -> V1)\\n  - follows chemical signal (even if it starts somewhere diff ends same place) - roger sperry 1943\\n- 3 stages in development of retina-lgn-v1 pathways\\n  1. experience-independent development - can occur prenatally\\n     - ex. segregation in eye-specific layers\\n     - retinal waves - spontaneous activity go accross the entire retina\\n  2. critical period of refinement of connections within and between cortical columns\\n     - extremeley sensitive to abnormal experience\\n     - competition to decide who connects where\\n  3. maturation and plasticity in adult life\\n\\n## adult plasticity\\n\\n- these are due to \"fatigue\" of stimulated neurons\\n  - color adaptation - afterim aimages + complementary colors\\n  - slight tilt aftereffect as well\\n- problems with fatigue hypothesis\\n  - doesn\\'t account for long-lasting adaptation effect (ex. McCulloch effect lasts very long time)\\n  - don\\'t see optic flow adaptation in driving, even though we see this in the lab\\n- there is a clear critical period for plasticity, although auditory / somatosensory don\\'t\\n- braille reading in blind subject activates \"visual cortex\" area\\n  - unclear it there is a critical period for this\\n- perceptual learning - can learn new visual tasks\\n  - very sensitive to eye, etc.\\n- trying to reat amblyopia\\n  - very unclear how much perceptual learning generalizes\\n\\n## alzheimer\\'s\\n\\n- manifests in the eye\\n- two biomarkers - can find with PET or MRI\\n  - amyloid $A\\\\beta$ plaque  proteins\\n  - pTau proteins\\n- bunch of things in retina\\n  - ex. RGC loss, NFL atrophy, blood flow rate, inflammation...all exist in other \\n  - main thing - look for AB plaques with stains - requires someone is dead\\n- goal: diagnose alzheimer\\'s via noninvasive retinal imaging + visual function assessment\\n  - imaging: modified spectralis HRA + OCT should look at biomarkers (+ other things ex. look at retinal structure deficits)',\n",
       " '---\\nlayout: notes\\ntitle: comp neuro\\ncategory: neuro\\n---\\n\\n#  comp neuro\\n\\n## introduction\\n\\n### overview\\n\\n- does biology have a cutoff level (likecutoffs in computers below which fluctuations don\\'t matter)\\n- core principles underlying these two questions\\n  - how do brains work?\\n  - how do you build an intelligent machine?\\n- lacking: insight from neuro that can help build machine\\n- scales: cortex, column, neuron, synapses\\n- physics: theory and practice are much closer\\n- are there principles?\\n  - \"god is a hacker\" - francis crick\\n  - theorists are lazy - ramon y cajal\\n  - things seemed like mush but became more clear - horace barlow\\n  - principles of neural design book\\n- felleman & van essen 1991\\n  - ascending layers (e.g. v1-> v2): goes from superficial to deep layers\\n  - descending layers (e.g. v2 -> v1): deep layers to superficial\\n- **solari & stoner 2011 \"cognitive consilience\"** - layers thicknesses change in different parts of the brain\\n  - motor cortex has much smaller input (layer 4), since it is mostly output\\n\\n### historical ai\\n\\n- people: turing, von neumman, marvin minsky, mccarthy...\\n- ai: birth at 1956 conference\\n  - vision: marvin minsky thought it would be a summer project\\n- lighthill debate 1973 - was ai worth funding?\\n- intelligence tends to be developed by young children...\\n- cortex grew very rapidly\\n\\n### historical cybernetics/nns\\n\\n- people: norbert weiner, mcculloch & pitts, rosenblatt\\n- neuro\\n  - hubel & weisel (1962, 1965) simple, complex, hypercomplex cells\\n  - neocognitron fukushima 1980\\n  - david marr: theory, representation, implementation\\n\\n### types of models\\n\\n- three types\\n  1. *descriptive* brain model - encode / decode external stimuli\\n  2. *mechanistic* brian cell / network model - simulate the behavior of a single neuron / network\\n  3. *interpretive* (or normative) brain model - why do brain circuits operate how they do\\n\\n- *receptive field* - the things that make a neuron fire\\n- retina has on-center / off-surround cells - stimulated by points\\n- then, V1 has differently shaped receptive fields\\n- *efficient coding hypothesis* - learns different combinations (e.g. lines) that can efficiently represent images\\n  \\n  1. sparse coding (Olshausen and Field 1996)\\n  2. ICA (Bell and Sejnowski 1997)\\n  3. Predictive Coding (Rao and Ballard 1999)\\n  - brain is trying to learn faithful and efficient representations of an animal\\'s natural environment\\n    - same goes for auditory cortex\\n\\n## biophysical models\\n\\n### modeling neurons\\n\\n- ![](../assets/5_1_1.png)\\n- nernst battery\\n  1. osmosis (for each ion)\\n  2. electrostatic forces (for each ion)\\n  - together these yield Nernst potential $E = \\\\frac{k_B T}{zq} ln \\\\frac{[in]}{[out]}$\\n    - T is temp\\n    - q is ionic charge\\n    - z is num charges\\n  - part of voltage is accounted for by nernst battery $V_{rest}$\\n  - yields $\\\\tau \\\\frac{dV}{dt} = -V + V_\\\\infty$ where $\\\\tau=R_mC_m=r_mc_m$\\n  - equivalently, $\\\\tau_m \\\\frac{dV}{dt} = -((V-E_L) - g_s(t)(V-E_s) r_m) + I_e R_m $\\n- ![](../assets/5_1_2.png)\\n\\n### simplified model neurons\\n\\n- *integrate-and-fire* neuron\\n  - passive membrane (neuron charges)\\n  - when V = V$_{thresh}$, a spike is fired\\n  - then V = V$_{reset}$\\n  - doesn\\'t have good modeling near threshold\\n  - can include threshold by saying\\n    - when V = V$_{max}$, a spike is fired\\n    - then V = V$_{reset}$\\n- modeling multiple variables\\n  - also model a K current\\n  - can capture things like resonance\\n- *theta neuron* (Ermentrout and Kopell)\\n  - ![](../assets/5_3_1.png)\\n  - often used for periodically firing neurons (it fires spontaneously)\\n\\n### a forest of dendrites\\n\\n- cable theory - Kelvin\\n- voltage V is a function of both x and t\\n- ![](../assets/5_4_1.png)\\n- separate into sections that don\\'t depend on x\\n  - coupling conductances link the sections (based on area of compartments / branching)\\n- Rall model for dendrites\\n  - if branches obey a certain branching ratio, can replace each pair of branches with a single cable segment with equivalent surface area and electrotonic length\\n    - $d_1^{3/2} = d_{11}^{3/2} + d_{12}^{3/2}$\\n- dendritic computation (London and Hausser 2005)\\n  - hippocampus - when inputs arrive at soma, similiar shape no matter where they come in = *synaptic scaling*\\n  - where inputs enter influences how they sum\\n  - dendrites can generate spikes (usually calcium) / backpropagating spikes\\n- ex. *Jeffress model*  - sound localized based on timing difference between ears\\n- ex. direction selectivity in retinal ganglion cells - if events arive at dendrite far -> close, all get to soma at same time and add\\n\\n### circuit-modeling basics\\n\\n- membrane has capacitance $C_m$\\n- force for diffusion, force for drift\\n- can write down diffeq for this, which yields an equilibrium\\n- $\\\\tau = RC$\\n  - bigger $\\\\tau$ is slower\\n  - to increase capacitance\\n    - could have larger diameter\\n    - $C_m \\\\propto D$\\n  - axial resistance $R_A \\\\propto 1/D^2$ (not same as membrane lerk), thus bigger axons actually charge faster\\n\\n### action potentials\\n\\n- channel/receptor types\\n  - ionotropic: $G_{ion}$ = f(molecules outside)\\n    - something binds and opens channel\\n  - metabotropic: $G_{ion}$ = f(molecules inside)\\n    - doesn\\'t directly open a channel: indirect\\n  - others\\n    - photoreceptor\\n    - hair cell\\n  - voltage-gated (active - provide gain; might not require active ATP, other channels are all passive)\\n\\n### physics of computation\\n\\n- based on carver mead: drift and diffusion are at the heart of everything\\n- different things realted by the **Boltzmann distr.** (ex. distr of air molecules vs elevation. Subject to gravity and diffusion upwards since they\\'re colliding)\\n  - nernst potential\\n  - current-voltage relation of voltage-gated channels\\n  - current-voltage relation of MOS transistor\\n- these things are all like transistor: energy barrier that must be overcome\\n- neuromorphic examples\\n  - differential pair sigmoid yields sigmoid-like function\\n    - can compute tanh function really simply to simulate\\n  - silicon retina\\n    - lateral inhibition exists (gap junctions in horizontal cells)\\n    - mead & mahowald 1989 - analog VLSI retina (center-surround receptive field is very low energy)\\n- computation requires energy (otherwise signals would dissipate)\\n  - von neumann architecture: CPU - bus (data / address) - Memory\\n    - moore\\'s law ending (in terms of cost, clock speed, etc.)\\n      - ex. errors increase as device size decreases (and can\\'t tolerate any errors)\\n  - neuromorphic computing\\n    - brain ~ 20 Watts\\n    - exploit intrinsic transistor physics (need extremely small amounts of current)\\n    - exploit electronics laws kirchoff\\'s law, ohm\\'s law\\n    - new materials (ex. memristor - 3d crossbar array)\\n    - can\\'t just do biological mimicry - need to understand the principles\\n\\n### spiking neurons\\n\\n- passive membrane model was leaky integrator\\n- voltage-gaed channels were more complicated\\n- can be though of as leaky integrate-and-fire neuron (LIF)\\n  - this charges up and then fires a spike, has refractory period, then starts charging up again\\n- rate coding hypothesis - signal conveyed is the rate of spiking (bruno thinks this is usually too simple)\\n  - spiking irregulariy is largely due to noise and doesn\\'t convey information\\n  - some neurons (e.g. neurons in LIP) might actually just convey a rate\\n- linear-nonlinear-poisson model (LNP) - sometimes called GLM (generalized linear model)\\n  - based on observation that variance in firing rate $\\\\propto$ mean firing rate\\n    - plotting mean vs variance = 1 $\\\\implies$ Poisson output\\n  - these led people to model firing rates as Poisson $\\\\frac {\\\\lambda^n e^{-\\\\lambda}} {n!}$\\n  - bruno doesn\\'t really believe the firing is random (just an effect of other things we can\\'t measure)\\n  - ex. fly H1 neuron 1997\\n    - constant stimulus looks very Poisson\\n    - moving stimulus looks very Bernoulli\\n- spike timing hypothesis\\n  - spiece timing can be very precise in response to time-varying signals (mainen & sejnowski 1995; bair & koch 1996)\\n  - often see precise timing\\n- encoding: stimulus $\\\\to$ spikes\\n- decoding: spikes $\\\\to$ representation\\n- encoding + decoding are related through the joint distr. over simulus and repsonse (see Bialek spikes book)\\n  - nonlinear encoding function can yield linear decoding\\n  - able to directly decode spikes using a kernel to reproduce signal (seems to say you need spikes - rates would not be good enough)\\n    - some reactions happen too fast to average spikes (e.g. 30 ms)\\n  - estimating information rate: bits (usually better than snr - can calculate between them) - usually 2-3 bits/spike\\n\\n\\n## neural coding\\n\\n### neural encoding\\n\\n#### defining neural code\\n\\n- extracellular\\n  - fMRI\\n    - averaged over space\\n    - slow, requires seconds\\n  - EEG\\n    - noisy\\n    - averaged, but faster\\n  - multielectrode array\\n    - record from several individual neurons at once\\n  - calcium imaging\\n    - cells have calcium indicator that fluoresce when calcium enters a cell\\n- intracellular - can use patch electrodes\\n- raster plot\\n  - replay a movie many times and record from retinal ganglion cells during movie\\n- *encoding*: P(response \\\\| stimulus)\\n  - *tuning curve* - neuron\\'s response (ex. firing rate) as a function of stimulus\\n  - orientation / color selective cells are distributed in organized fashion\\n  - some neurons fire to a concept, like \"Pamela Anderson\"\\n  - retina (simple) -> V1 (orientations) -> V4 (combinations) -> ?\\n  - also massive feedback\\n- *decoding*: P(stimulus \\\\| response)\\n\\n#### simple encoding\\n\\n- want P(response \\\\| stimulus)\\n  - response := firing rate r(t)\\n  - stimulus := s\\n- simple linear model\\n\\n  - r(t) = c * s(t)\\n- *weighted linear model* - takes into account previous states weighted by f\\n  1. *temporal filtering*\\n    - r(t) = $f_0 \\\\cdot s_0 + ... + f_t \\\\cdot s_t =  \\\\sum s_{t-k} f_k$ where f weights stimulus over time\\n    - could also make this an integral, yielding a convolution:\\n    - r(t) = $\\\\int_{-\\\\infty}^t d\\\\tau \\\\: s(t-\\\\tau) f(\\\\tau)$\\n    - a linear system can be thought of as a system that searches for portions of the signal that resemble its filter f\\n    - leaky integrator - sums its inputs with f decaying exponentially into the past\\n    - flaws\\n      - no negative firing rates\\n      - no extremely high firing rates\\n      - can add a nonlinear function g of the linear sum can fix this\\n        - r(t) = $g(\\\\int_{-\\\\infty}^t d\\\\tau \\\\: s(t-\\\\tau) f(\\\\tau))$\\n  2. *spatial filtering*\\n    - r(x,y) = $\\\\sum_{x\\',y\\'} s_{x-x\\',y-y\\'} f_{x\\',y\\'}$ where f again is spatial weights that represent the spatial field\\n    - could also write this as a convolution\\n    - for a retinal center surround cell, f is positive for small $\\\\Delta x$ and then negative for large $\\\\Delta x$\\n      - can be calculated as a narrow, large positive Gaussian + spread out negative Gaussian\\n  - can combine above to make *spatiotemporal filtering*\\n    - filtering = convolution = projection\\n\\n#### feature selection\\n\\n- P(response\\\\|stimulus) is very hard to get\\n  - stimulus can be high-dimensional (e.g. video)\\n  - stimulus can take on many values\\n  - need to keep track of stimulus over time\\n  - solution: sample P(response\\\\|s) to many stimuli to characterize what in input triggers responses\\n- find vector *f* that captures features that lead to spike\\n  - dimensionality reduction - ex. discretize \\n  - value at each time $t_i$ is new dimension\\n  - commonly use Gaussian white noise\\n  - time step sets cutoff of highest frequency present\\n  - *prior distribution* - distribution of stimulus\\n    - multivariate Gaussian - Gaussian in any dimension, or any linear combination of dimensions\\n  - look at where spike-triggering points are and calculate *spike-triggered average* *f* of features that led to spike\\n    - use this f as filter\\n- determining the nonlinear input/output function *g*\\n  - replace stimulus in P(spike\\\\|stimulus) with P(spike\\\\|$s_1$), where s1 is our filtered stimulus\\n    - use bayes rule $g=P(spike\\\\|s_1)=\\\\frac{P(s_1\\\\|spike)P(spike)}{P(s_1)}$\\n    - if $P(s_1\\\\|spike) \\\\approx P(s_1)$ then response doesn\\'t seem to have to do with stimulus\\n- incorporating many features *$f_1,...,f_n$*\\n  - here, each $f_i$ is a vector of weights\\n  - $r(t) = g(f_1\\\\cdot s,f_2 \\\\cdot s,...,f_n \\\\cdot s)$\\n  - could use *PCA* - discovers low-dimensional structure in high-dimensional data\\n  - each f represents a feature (maybe a curve over time) that fires the neuron\\n\\n#### variability\\n\\n- hidden assumptions about time-varying firing rate and single spikes\\n\\n  - smooth function RFT can miss some stimuli\\n- statistics of stimulus can effect P(spike\\\\|stimulus)\\n\\n  - Gaussian white noise is nice because no way to filter it to get structure\\n- identifying good filter\\n  - want $P(s_f\\\\|spike)$ to differ from $P(s_f)$ where $s_f$ is calculated via the filter\\n  - instead of PCA, could look for f that directly maximizes this difference (Sharpee & Bialek, 2004)\\n  - *Kullback-Leibler divergence* - calculates difference between 2 distributions\\n    - $D_{KL}(P(s),Q(s)) = \\\\int ds P(s) log_2 P(s) / Q(s)$\\n  - maximizing KL divergence is equivalent to maximizing mutual info between spike and stimulus\\n    - this is because we are looking for most informative feature\\n    - this technique doesn\\'t require that our stimulus is white noise, so can use natural stimuli\\n    - maximization isn\\'t guaranteed to uniquely converge\\n- modeling the noise\\n  - need to go from r(t) -> spike times\\n  - divide time T into n bins with p = probability of firing per bin\\n  - over some chunk T, number of spikes follows binomial distribution (n, p)\\n    - mean = np\\n    - var = np(1-p)\\n  - if n gets very large, binomial approximates Poisson\\n    - $\\\\lambda$ = spikes in some set time\\n      - mean = $\\\\lambda$\\n      - var = $\\\\lambda$\\n    1. can test if distr is Poisson with *Fano factor*=mean/var=1\\n      2. interspike intervals have exponential distribution\\t- if fires a lot, this can be bad assumption (due to refractory period)\\n- generalized linear model adds explicit spike-generation / post-spike filter (Pillow et al. 2008)\\n  - ![](../assets/2_4_1.png)\\n  - post-spike filter models refractory period\\n  - *Paninski* showed that using exponential nonlinearity allows this to be optimized\\n  - could add in firing of other neurons\\n  - *time-rescaling theorem* - tests how well we have captured influences on spiking (Brown et al 2001)\\n    - scaled ISIs ($t_{i-1}-t_i$) r(t) should be exponential\\n\\n### neural decoding\\n\\n#### neural decoding and signal detection\\n\\n- decoding: P(stimulus \\\\| response) - ex. you hear noise and want to tell what it is\\n  - here r = response = firing rate\\n- monkey is trained to move eyes in same direction as dot pattern (Britten et al. 92)\\n  - when dots all move in same direction (100% coherence), easy\\n    - neuron recorded in MT - tracks dots\\n    - count firing rate when monkey tracks in right direction\\n    - count firing rate when  monkey tracks in wrong direction\\n    - as coherence decreases, these firing rates blur\\n  - need to get P(+ or - \\\\| r)\\n    - can set a threshold on r by maximizing likelihood\\n      - P(r\\\\|+) and P(r\\\\|-) are likelihoods\\n    - *Neyman-Pearson lemma* - likelihood ratio test is the most efficient statistic, in that is has the most power for a given size\\n      - $\\\\frac{p(r\\\\|+)}{p(r\\\\|-)} > 1?$\\n- accumulated evidence - we can accumulate evidence over time by multiplying these probabilities\\n  - instead we take sum the logs, and compare to 0\\n  - $\\\\sum_i ln \\\\frac{p(r_i\\\\|+)}{p(r_i\\\\|-)} > 0?$\\n  - once we hit some threshold for this sum, we can make a decision + or -\\n- experimental evidence (Kiani, Hanks, & Shadlen, Nat. Neurosci 2006)\\n  - monkey is making decision about whether dots are moving left/right\\n  - neuron firing rates increase over time, representing integrated evidence\\n  - neuron always seems to stop at same firing rate\\n- priors - ex. tiger is much less likely then breeze\\n  - scale P(+\\\\|r) by prior P(+)\\n  - neuroscience ex. photoreceptor cells P(noise\\\\|r) is much larger than P(signal\\\\|r)\\n    - therefore threshold on r is high to minimize total mistakes\\n- cost of acting/not acting\\n  - loss for predicting + when it is -: $L_- \\\\cdot P[+\\\\|r]$\\n  - loss for predicting - when it is +: $L_+ \\\\cdot P[-\\\\|r]$\\n  - cut your losses: answer + when average Loss$_+$ < Loss$_-$\\n    - i.e. $L_+ \\\\cdot P[-\\\\|r]$ < $L_- \\\\cdot P[+\\\\|r]$\\n  - rewriting with Baye\\'s rule yields new test:\\n    - $\\\\frac{p(r\\\\|+)}{p(r\\\\|-)}> L_+ \\\\cdot P[-] / L_- \\\\cdot P[+]$\\n    - here the loss term replaces the 1 in the Neyman-Pearson lemma\\n\\n#### population coding and bayesian estimation\\n\\n- *population vector* - sums vectors for cells that point in different directions weighted by their firing rates\\n  - ex. cricket cercal cells sense wind in different directions\\n  - since neuron can\\'t have negative firing rate, need overcomplete basis so that can record wind in both directions along an axis\\n  - can do the same thing for direction of arm movement in a neural prosthesis\\n  - not general - some neurons aren\\'t tuned, are noisier\\n  - not *optimal* - making use of all information in the stimulus/response distributions\\n- *bayesian inference*\\n  - $p(s\\\\|r) = \\\\frac{p(r\\\\|s)p(s)}{p( r)}$\\n  - ![](../assets/3_2_1.png)\\n  - maximum likelihood: s* which maximizes p(r\\\\|s)\\n  - MAP = maximum $a\\\\:posteriori$: s* which mazimizes p(s\\\\|r)\\n- simple continuous stimulus example\\n  - setup\\n    - s - orientation of an edge\\n    - each neuron\\'s average firing rate=tuning curve $f_a(s)$ is Gaussian (in s)\\n    - let $r_a$ be number of spikes for neuron a\\n    - assume receptive fields of neurons span s: $\\\\sum r_a (s)$ is const\\n    - ![](../assets/3_2_2.png)\\n  - solving\\n    - maximizing log-likelihood with respect to s\\t\\t\\t- take derivative and set to 0\\n      - soln $s^* = \\\\frac{\\\\sum r_a s_a / \\\\sigma_a^2}{\\\\sum r_a / \\\\sigma_a^2}$\\n      - if all the $\\\\sigma$ are same, $s^* = \\\\frac{\\\\sum r_a s_a}{\\\\sum r_a}$\\n        - this is the population vector\\n    - maximum *a posteriori*\\n      - $ln \\\\: p(s\\\\|r) = ln \\\\: P(r\\\\|s) + ln \\\\: p(s) = ln \\\\: P(r )$\\n      - $s^* = \\\\frac{T \\\\sum r_a s_a / \\\\sigma^2_a + s_{prior} / \\\\sigma^2_{prior}}{T \\\\sum r_a / \\\\sigma^2_a + 1/\\\\sigma^2_{prior}}$\\n      - this takes into account the prior\\n        - narrow prior makes it matter more\\n    - doesn\\'t incorporate correlations in the population\\n\\n#### stimulus reconstruction\\n\\n- decoding s -> $s^*$\\n- want an estimator $s_{Bayes}=s_B$ given some response r\\n  - error function $L(s,s_{B})=(s-s_{B})^2$\\n  - minimize $\\\\int ds \\\\: L(s,s_{B}) \\\\: p(s\\\\|r)$ by taking derivative with respect to $s_B$\\n  - $s_B = \\\\int ds \\\\: p(s\\\\|r) \\\\: s$ - the conditional mean (spike-triggered average)\\n- add in spike-triggered average at each spike\\n  - if spike-triggered average looks exponential, can never have smooth downwards stimulus\\n  - could use 2 neurons (like in H1) and replay the second with negative sign\\n- LGN neurons can reconstruct a video, but with noise\\n- recreated 1 sec long movies - (*Jack Gallant* - Nishimoto et al. 2011, Current Biology)\\n  1. voxel-based encoding model samples ton of prior clips and predicts signal\\n    - get p(r\\\\|s)\\n    - pick best p(r\\\\|s) by comparing predicted signal to actual signal\\n    - input is filtered to extract certain features\\n    - filtered again to account for slow timescale of BOLD signal\\n  2. decoding\\n    - maximize p(s\\\\|r) by maximizing p(r\\\\|s) p(s), and assume p(s) uniform\\n    - 30 signals that have highest match to predicted signal are averaged \\n    - yields pretty good pictures\\n\\n### information theory\\n\\n#### information and entropy\\n\\n- surprise for seeing a spike h(p) = $-log_2 (p)$\\n- entropy = average information\\n- code might not align spikes with what we are encoding\\n- how much of the variability in r is encoding s\\n  - define q as en error\\n    - $P(r_+\\\\|s=+)=1-q$\\n    - $P(r_-\\\\|s=+)=q$\\n    - similar for when s=-\\n  - total entropy: $H(R ) = - P(r_+) log P(r_+) - P(r_-)log P(r_-)$\\n  - noise entropy: $H(R\\\\|S=+) = -q log q - (1-q) log (1-q)$\\n  - mutual info I(S;R) = $H(R ) - H(R\\\\|S) $ = total entropy - average noise entropy\\n    - = $D_{KL} (P(R,S), P(R )P(S))$\\n- *grandma\\'s famous mutual info recipe*\\n  - for each s\\n    - P(R\\\\|s) - take one stimulus and repeat many times (or run for a long time)\\n    - H(R\\\\|s) - noise entropy\\n  - $H(R\\\\|S)=\\\\sum_s P(s) H(R\\\\|s)$\\n  - $H(R ) $ calculated using $P(R ) = \\\\sum_s P(s) P(R\\\\|s)$\\n\\n#### information in spike trains\\n\\n1. information in spike patterns\\n  - divide pattern into time bins of 0 (no spike) and 1 (spike)\\n  - binary words w with letter size $\\\\Delta t$, length T (Reinagel & Reid 2000)\\n    - can create histogram of each word\\n    - can calculate entropy of word \\n  - look at distribution of words for just one stimulus\\n    - distribution should be narrower\\n  - calculate $H_{noise}$ - average over time with random stimuli and calculate entropy\\n    - varied parameters of word: length of bin (dt) and length of word (T)\\n    - there\\'s some limit to dt at which information stops increasing\\n      - this represents temporal resolution at which jitter doesn\\'t stop response from identifying info about the stimulus\\n    - corrections for finite sample size (Panzeri, Nemenman,...)\\n2. information in single spikes - how much info does single spike tell us about stimulus\\n  - don\\'t have to know encoding, mutual info doesn\\'t care\\n  1. calculate entropy for random stimulus\\n    - $p=\\\\bar{r} \\\\Delta t$ where $\\\\bar{r}$ is the mean firing rate\\n  2. calculate entropy for specific stimulus\\n  - let $P(r=1\\\\|s) = r(t) \\\\Delta t$\\n  - let $P(r=0\\\\|s) = 1 - r(t) \\\\Delta t$\\n  - get r(t) by having simulus on for long time\\n  - *ergodicity* - a time average is equivalent to averging over the s ensemble\\n  - ![](../assets/4_2_1.png)\\n  - info per spike $I(r,s) = \\\\frac{1}{T} \\\\int_0^T dt \\\\frac{r(t)}{\\\\bar{r}} log \\\\frac{r(t)}{\\\\bar{r}}$\\n    - timing precision reduces r(t)\\n    - low mean spike rate -> high info per spike\\n  - ex. rat runs through place field and only fires when it\\'s in place field\\n    - spikes can be sharper, more / less frequent\\n\\n#### coding principles\\n\\n- natural stimuli\\n  - huge dynamic range - variations over many orders of magnitude (ex. brightness)\\n  - power law scaling - structure at many scales (ex. far away things)\\n- *efficient coding* - in order to have maximum entropy output, a good encoder should match its outputs to the distribution of its inputs\\n  - want to use each of our \"symbols\" (ex. different firing rates) equally often\\n  - should assign equal areas of input stimulus PDF to each symbol\\n- adaptataion to stimulus statistics\\n  - ![](../assets/4_3_1.png)\\n  - feature adaptation (Atick and Redlich)\\n    - spatial filtering properties in retina / LGN change with varying light levels\\n    - at low light levels surround becomes weaker\\n- coding sechemes\\n  1. redundancy reduction\\n    - population code $P(R_1,R_2)$\\n    - entropy $H(R_1,R_2) \\\\leq H(R_1) + H(R_2)$ - being independent would maximize entropy\\n  2. correlations can be good\\n    - error correction and robust coding\\n    - correlations can help discrimination\\n    - retina neurons are redundant (Berry, Chichilnisky)\\n  3. more recently, sparse coding\\n    - penalize weights of basis functions\\n    - instead, we get localized features\\n- we ignored the behavioral feedback loop\\n\\n\\n## computing with networks\\n\\n### modeling connections between neurons\\n\\n- model effects of synapse by using synaptic conductance $g_s$ with reversal potential $E_s$\\n  - $g_s = g_{s,max} \\\\cdot P_{rel} \\\\cdot P_s$\\n    - $P_{rel}$ - probability of release given an input spike\\n    - $P_s$ - probability of postsynaptic channel opening = fraction of channels opened\\n- basic synapse model\\n  - assume $P_{rel}=1$\\n  - model $P_s$ with kinetic model\\n    - open based on $\\\\alpha_s$\\n    - close based on $\\\\beta_s$\\n    - yields $\\\\frac{dP_s}{dt} = \\\\alpha_s (1-P_s) - \\\\beta_s P_s$\\n  - 3 synapse types\\n    1. AMPA - well-fit by exponential\\n    2. GAMA - fit by \"alpha\" function - has some delay\\n    3. NMDA - fit by \"alpha\" function - has some delay\\n- linear filter model of a synapse\\n  - pick filter (ex. K(t) ~ exponential)\\n  - $g_s = g_{s,max} \\\\sum K(t-t_i)$\\n- network of integrate-and-fire neurons\\n  - if 2 neurons inhibit each other, get *synchrony* (fire at the same time\\n\\n### intro to network models\\n\\n- comparing spiking models to firing-rate models\\n  - advantages\\n    - spike timing\\n    - spike correlations / synchrony between neurons\\n  - disadvantages\\n    - computationally expensive\\n  - uses linear filter model of a synapse\\n- developing a firing-rate model\\n  - replace spike train $\\\\rho_1(t) \\\\to u_1(t)$\\n    - can\\'t make this replacement when there are correlations / synchrony?\\n  - input current $I_s$: $\\\\tau_s \\\\frac{dI_s}{dt}=-I_s + \\\\mathbf{w} \\\\cdot \\\\mathbf{u}$\\n    - works only if we let K be exponential\\n  - output firing rate: $\\\\tau_r \\\\frac{d\\\\nu}{dt} = -\\\\nu + F(I_s(t))$\\n  - if synapses are fast ($\\\\tau_s << \\\\tau_r$)\\n    - $\\\\tau_r \\\\frac{d\\\\nu}{dt} = -\\\\nu + F(\\\\mathbf{w} \\\\cdot \\\\mathbf{u}))$\\n  - if synapses are slow ($\\\\tau_r << \\\\tau_s$)\\n    - $\\\\nu = F(I_s(t))$\\n  - if static inputs (input doesn\\'t change) - this is like artificial neural network, where F is sigmoid\\n    - $\\\\nu_{\\\\infty} = F(\\\\mathbf{w} \\\\cdot \\\\mathbf{u})$\\n    - could make these all vectors to extend to multiple output neurons\\n- recurrent networks \\n  - $\\\\tau \\\\frac{d\\\\mathbf{v}}{dt} = -\\\\mathbf{v} + F(W\\\\mathbf{u} + M \\\\mathbf{v})$\\n    - $-\\\\mathbf{v}$ is decay\\n    - $W\\\\mathbf{u}$ is input\\n    - $M \\\\mathbf{v}$ is feedback\\n  - with constant input, $v_{\\\\infty} = W \\\\mathbf{u}$\\n  - ex. edge detectors\\n  - V1 neurons are basically computing derivatives\\n\\n### recurrent networks\\n\\n- linear recurrent network: $\\\\tau \\\\frac{d\\\\mathbf{v}}{dt} = -\\\\mathbf{v} + W\\\\mathbf{u} + M \\\\mathbf{v}$\\n  - let $\\\\mathbf{h} = W\\\\mathbf{u}$\\n  - want to investiage different M\\n- can solve eq for $\\\\mathbf{v}$ using eigenvectors\\n  - suppose M (NxN) is symmetric (connections are equal in both directions)\\n    - $\\\\to$ M has N orthogonal eigenvectors / eigenvalues\\n    - let $e_i$ be the orthonormal eigenvectors\\n  - output vector $\\\\mathbf{v}(t) = \\\\sum c_i (t) \\\\mathbf{e_i}$\\n  - allows us to get a closed-form solution for $c_i(t)$\\n  - eigenvalues determine network stability\\n    - if any $\\\\lambda_i > 1, \\\\mathbf{v}(t)$ explodes $\\\\implies$ network is unstable\\n      - otherwise stable and converges to steady-state value\\n    - $\\\\mathbf{v}_\\\\infty = \\\\sum \\\\frac{h\\\\cdot e_i}{1-\\\\lambda_i} e_i$\\n    - amplification of input projection by a factor of $\\\\frac{1}{1-\\\\lambda_i}$\\n- ex. each output neuron codes for an angle between -180 to 180\\n  - define M as cosine function of relative angle\\n  - excitation nearby, inhibition further away\\n- memory in linear recurrent networks\\n  - suppose $\\\\lambda_1=1$ and all other $\\\\lambda_i < 1$\\n  - then $\\\\tau \\\\frac{dc_1}{dt} = h \\\\cdot e_1$ - keeps memory of input\\n  - ex. memory of eye position in medial vestibular nucleus (Seung et al. 2000)\\n    - integrator neuron maintains persistent activity\\n- nonlinear recurrent networks: $\\\\tau \\\\frac{d\\\\mathbf{v}}{dt} = -\\\\mathbf{v} + F(\\\\mathbf{h}+ M \\\\mathbf{v})$\\n  - ex. rectification linearity F(x) = max(0,x)\\n    - ensures that firing rates never go below\\n  - can have eigenvalues > 1 but stable due to rectification\\n  - can perform selective \"attention\"\\n    - network performs \"winner-takes-all\" input selection\\n  - *gain modulation* - adding constant amount to input h multiplies the output\\n  - also maintains memory\\n- non-symmetric recurrent networks\\n  - ex. excitatory and inhibitory neurons\\n  - linear stability analysis - find fixed points and take partial derivatives\\n    - use eigenvalues to determine dynamics of the nonlinear network near a fixed point\\n\\n#### hopfield nets\\n\\n- hopfield nets can store / retrieve memories\\n- fully connected (no input/output) - activations are what matter\\n  - can memorize patterns - starting with noisy patterns can converge to these patterns\\n- marr-pogio stereo algorithm\\n- hopfield three-way connections\\n  - $E = - \\\\sum_{i, j, k} T_{i, j, k} V_i V_j V_k$ (self connections set to 0)\\n    - update to $V_i$ is now bilinear\\n- [hopfield nets are all you need](https://arxiv.org/abs/2008.02217)\\n  - keys: each input has a key vector which \"represents info about this input\" (e.g. this is a noun)\\n  - queries: each input has a query vector which \"asks for other inputs that would be useful context\" (e.g. what adjectives describe this word)\\n    - in self-attention these queries also come from the input whereas in just regular attention they come from somewhere else (e.g. the output of a translation task)\\n  - transformer finds similarity between each key with each query then takes softmax - this provides weights for each of the inputs, as context for the original input\\n    - in transformer, these weights are used to weight the values but in hopfield nets we would take a weighted sum of the keys and feed it back as the input\\n  - as we update becomes more skewed towards the things that match the most\\n\\n\\n## learning\\n\\n### supervised learning\\n\\n- net talk was major breakthrough (words -> audio) Sejnowski & Rosenberg 1987\\n- people looked for world-centric receptive fields (so neurons responded to things not relative to retina but relative to body) but didn\\'t find them\\n  - however, they did find gain fields: (Zipser & Anderson, 1987)\\n    - gain changes based on what retina is pointing at\\n  - trained nn to go from pixels to head-centered coordinate frame\\n    - yielded gain fields\\n  - pouget et al. were able to find that this helped having 2 pop vectors: one for retina, one for eye, then add to account for it\\n- support vector networks (vapnik et al.) - svms early inspired from nns\\n- dendritic nonlinearities (hausser & mel 03)\\n- example to think about neurons due this: $u = w_1 x_1 + w_2x_2 + w_{12}x_1x_2$\\n  - $y=\\\\sigma(u)$\\n  - somestimes called sigma-pi unit since it\\'s a sum of products\\n  - exponential number of params...**could be fixed w/ kernel trick?**\\n    - could also incorporate geometry constraint...\\n\\n### unsupervised learning\\n\\n- born w/ extremely strong priors on weights in different areas\\n- barlow 1961, attneave 1954: efficient coding hypothesis = redundancy reduction hypothesis\\n  - representation: compression / usefulness\\n  - easier to store prior probabilities (because inputs are independent)\\n  - relich 93: redundancy reduction for unsupervised learning (text ex. learns words from text w/out spaces)\\n\\n#### hebbian learning and pca\\n\\n- pca can also be thought of as a tool for decorrelation (in pc dimension, tends to be less correlated)\\n- hebbian learning = fire together, wire together: $\\\\Delta w_{ab} \\\\propto <a, b>$ note: $<a, b>$ is correlation of a and b (average over time)\\n- linear hebbian learning (perceptron with linear output)\\n- $\\\\dot{w}_i \\\\propto <y, x_i> \\\\propto \\\\sum_j w_j <x_j, x_i>$ since weights change relatively slowly\\n  - synapse couldn\\'t do this, would grow too large\\n- oja\\'s rule (hebbian learning w/ weight decay so ws don\\'t get too big)\\n  - points to correct direction\\n- sanger\\'s rule: for multiple neurons, fit residuals of other neurons\\n- competitive learning rule: winner take all\\n  - population nonlinearity is a max\\n  - gets stuck in local minima (basically k-means)\\n- pca only really good when data is gaussian\\n  - interesting problems are non-gaussian, non-linear, non-convex\\n- pca: yields checkerboards that get increasingly complex (because images are smooth, can describe with smaller checkerboards)\\n  - this is what jpeg does\\n  - very similar to discrete cosine transform (DCT)\\n  - very hard for neurons to get receptive fields that look like this\\n- retina: does whitening (yields center-surround receptive fields)\\n  - easier to build\\n  - gets more even outputs\\n  - only has ~1.5 million fibers\\n\\n### synaptic plasticity, hebb\\'s rule, and statistical learning\\n\\n- if 2 spikes keep firing at same time, get LTP - long-term potentiation\\n  - if input fires, but not B then could get LTD - long-term depression\\n- *Hebb rule* $\\\\tau_w \\\\frac{d\\\\mathbf{w}}{dt} = \\\\mathbf{x}v$\\n  - $\\\\mathbf{x}$ - input\\n  - $v$ - output\\n  - translates to $\\\\mathbf{w}_{i+1}=\\\\mathbf{w}_i + \\\\epsilon \\\\cdot \\\\mathbf{x}v$\\n  - average effect of the rule is to change based on correlation matrix $\\\\mathbf{x}^T\\\\mathbf{x}$\\n- *covariance rule*: $\\\\tau_w \\\\frac{d\\\\mathbf{w}}{dt} = \\\\mathbf{x}(v-E[v])$\\n  - includes LTD as well as LTP\\n- *Oja\\'s rule*: $\\\\tau_w \\\\frac{d\\\\mathbf{w}}{dt} = \\\\mathbf{x}v- \\\\alpha v^2 \\\\mathbf{w}$ where $\\\\alpha>0$\\n- stability\\n  - Hebb rule - derivative of w is always positive $\\\\implies$ w grows without bound\\n  - covariance rule - derivative of w is still always positive $\\\\implies$ w grows without bound\\n    - could add constraint that $\\\\|\\\\|w\\\\|\\\\|=1$ and normalize w after every step\\n  - Oja\\'s rule - $\\\\|\\\\|w\\\\|\\\\| = 1/\\\\sqrt{alpha}$, so stable\\n- solving *Hebb rule* $\\\\tau_w \\\\frac{d\\\\mathbf{w}}{dt} = Q w$ where Q represents correlation matrix\\n  - write w(t) in terms of eigenvectors of Q\\n  - lets us solve for $\\\\mathbf{w}(t)=\\\\sum_i c_i(0)exp(\\\\lambda_i t / \\\\tau_w) \\\\mathbf{e}_i$\\n  - when t is large, largest eigenvalue dominates\\n- hebbian learning implements PCA\\n  - hebbian learning learns w aligned with principal eigenvector of input correlation matrix\\n  - this is same as PCA\\n\\n### intro to unsupervised learning\\n\\n- ![](../assets/7_2_1.png)\\n  - most active neuron is the one whose w is closest to x\\n- *competitive learning*\\n  - updating weights given a new input\\n    1. pick a cluster (corresponds to most active neuron)\\n    2. set weight vector for that cluster to running average of all inputs in that cluster\\n      - $\\\\Delta w = \\\\epsilon \\\\cdot (\\\\mathbf{x} - \\\\mathbf{w})$\\n  - related to *self-organizing maps* = kohonen maps\\n    - in self-organizing maps also update other neurons in the neighborhood of the winner\\n    - update winner closer\\n    - update neighbors to also be closer\\n    - ex. V1 has orientation preference maps that do this\\n\\n### sparse coding and predictive coding\\n\\n- eigenface - Turk and Pentland 1991\\n  - eigenvectors of the input covariance matrix are good features\\n  - can represent images using sum of eigenvectors (orthonormal basis)\\n- suppose you use only first M principal eigenvectors\\n  - then there is some noise\\n  - can use this for compression\\n  - not good for local components of an image (e.g. parts of face, local edges)\\n- if you assume Gausian noise, maximizing likelihood = minimizing squared error\\n- generative model\\n  - images X\\n  - causes \\n  - likelihood P(X=x\\\\|C=c)\\n    - Gaussian\\n    - proportional to $exp(x-Gc)$\\n  - want posterior P(C\\\\|X)\\n  - prior p(C )\\n    - assume priors causes are independent\\n    - want sparse distribution\\n      - has heavy tail (super-Gaussian distribution)\\n    - then P(C ) = $k \\\\cdot \\\\prod exp(g(C_i))$\\n  - can implement sparse coding in a recurrent neural network\\n  - Olshausen & Field, 1996 - learns receptive fields in V1\\n- sparse coding is a special case of *predicive coding*\\n  - ![](../assets/7_3_1.png)\\n  - there is usually a feedback connection for every feedforward connection (Rao & Ballard, 1999)\\n\\n### sparse, distributed coding\\n\\n- $$\\\\underset {\\\\mathbf{D}} \\\\min \\\\underset t \\\\sum \\\\underset {\\\\mathbf{h^{(t)}}} \\\\min ||\\\\mathbf{x^{(t)}} - \\\\mathbf{Dh^{(t)}}||_2^2 + \\\\lambda ||\\\\mathbf{h^{(t)}}||_1$$\\n  - D is like autoencoder output weight matrix\\n  - h is more complicated - requires solving inner minimization problem\\n  - outer loop is not quite lasso - weights are not what is penalized\\n- barlow 1972: want to represent stimulus with minimum active neurons\\n  - neurons farther in cortex are more silent\\n  - v1 is highly overcomplete (dimensionality expansion)\\n- codes: dense -> sparse, distributed $n \\\\choose k$ -> local (grandmother cells)\\n  - energy argument - bruno doesn\\'t think it\\'s a big deal (could just not have a brain)\\n- PCA: autoencoder when you enforce weights to be orthonormal\\n  - retina must output encoded inputs as spikes, lower dimension -> uses whitening\\n- cortex\\n  - sparse coding different kind of autencoder bottleneck (imposes sparsity)\\n- using bottlenecks in autoencoders forces you to find structure in data\\n- v1 simple-cell receptive fields are localized, oriented, and bandpass\\n- higher-order image statistics\\n  - phase alignment\\n  - orientation (requires at least 3 points stats (like orientation)\\n  - motion\\n- how to learn sparse repr?\\n  - foldiak 1990 forming sparse reprs by local anti-hebbian learning\\n  - driven by inputs and gets lateral inhibition and sum threshold\\n  - neurons drift towards some firing rate naturally (adjust threshold naturally)\\n- use higher-order statistics\\n  - projection pursuit (field 1994) - maximize non-gaussianity of projections\\n    - CLT says random projections should look gaussian\\n    - gabor-filter response histogram over natural images look non-Gaussian (sparse) - peaked at 0\\n  - doesn\\'t work for graded signals\\n- sparse coding for graded signals: olshausen & field, 1996\\n  - $\\\\underset{Image}{I(x, y)} = \\\\sum_i a_i \\\\phi_i (x, y) + \\\\epsilon (x,y)$\\n  - loss function $\\\\frac{1}{2} |I - \\\\phi a|^2 + \\\\lambda \\\\sum_i C(a_i)$\\n  - can think about difference between $L_1$ and $L_2$ as having preferred directions (for the same length of vector) - prefer directions which some zeros\\n  - in terms of optimization, smooth near zero\\n  - there is a network implementation\\n  - $a_i$are calculated by solvin optimization for each image, $\\\\phi$ is learned more slowly\\n  - **can you get $a_i$ closed form soln?** \\n- wavelets invented in 1980s/1990s for sparsity + compression\\n- these tuning curves match those of real v1 neurons\\n- applications\\n  - for time, have spatiotemporal basis where local wavelet moves\\n  - sparse coding of natural sounds\\n    - audition like a movie with two pixels (each ear sounds independent)\\n    - converges to gamma tone functions, which is what auditory fibers look like\\n  - sparse coding to neural recordings - finds spikes in neurons\\n    - learns that different layers activate together, different frequencies come out\\n    - found place cell bases for LFP in hippocampus\\n  - nonnegative matrix factorization - like sparse coding but enforces nonnegative \\n  - can explicitly enforce nonnegativity\\n- LCA algorithm lets us implement sparse coding in biologically plausible local manner\\n- explaining away - neural responses at the population should be decodable (shouldn\\'t be ambiguous)\\n- good project: understanding properties of sparse coding bases\\n- SNR = $VAR(I) / VAR(|I- \\\\phi A|)$\\n- can run on data after whitening\\n  - graph is of power vs frequency (images go down as $1/f$), need to weighten with f\\n  - don\\'t whiten highest frequencies (because really just noise)\\n    - need to do this softly - roughly what the retina does\\n  - as a result higher spatial frequency activations have less variance\\n- whitening effect on sparse coding\\n  - if you don\\'t whiten, have some directions that have much more variance\\n- projects\\n  - applying to different types of data (ex. auditory)\\n- adding more bases as time goes on\\n- combining convolution w/ sparse coding?\\n- people didn\\'t see sparsity for a while because they were using very specific stimuli and specific neurons\\n  - now people with less biased sampling are finding more sparsity\\n  - in cortex anasthesia tends to lower firing rates, but opposite in hippocampus\\n\\n### self-organizing maps\\n\\n- homunculus - 3d map corresponds to map in cortex (sensory + motor)\\n- visual cortex\\n  - visual cortex mostly devoted to center\\n  - different neurons in same regions sensitive to different orientations (changing smoothly)\\n  - orientation constant along column\\n  - orientation maps not found in mice (but in cats, monkeys)\\n  - direction selective cells as well\\n- maps are plastic - cortex devoted to particular tasks expands (not passive, needs to be active)\\n  - kids therapy with tone-tracking video games at higher and higher frequencies\\n\\n## ml analogies\\n\\n### Brain theories\\n- Computational Theory of Mind\\n- Classical associationism\\n- Connectionism\\n  -Situated cognition\\n  -Memory-prediction framework\\n  -Fractal Theory: https://www.youtube.com/watch?v=axaH4HFzA24\\n  -Brain sheets are made of cortical columns (about .3mm diameter, 1000 neurons / column)\\n  -Have ~6 layers\\n\\n### brain as a computer\\n-\\tBrain as a Computer – Analog VLSI and Neural Systems by Mead (VLSI – very large scale integration)\\n  -Brain Computer Analogy\\n  -Process info\\n  -Signals represented by potential\\n  -Signals are amplified = gain\\n  -Power supply\\n  -Knowledge is not stored in knowledge of the parts, but in their connections\\n  -Based on electrically charged entities interacting with energy barriers\\n  -http://en.wikipedia.org/wiki/Computational_theory_of_mind\\n  -http://scienceblogs.com/developingintelligence/2007/03/27/why-the-brain-is-not-like-a-co/\\n  -Brain’ storage capacity is about 2.5 petabytes (Scientific American, 2005)\\n  -Electronics\\n  -Voltage can be thought of as water in a reservoir at a height\\n  -It can flow down, but the water will never reach above the initial voltage\\n  -A capacitor is like a tank that collects the water under the reservoir\\n  -The capacitance is the cross-sectional area of the tank\\n  -Capacitance – electrical charge required to raise the potential by 1 volt\\n  -Conductance = 1/ resistance = mho, siemens\\n  -We could also say the word is a computer with individuals being the processors – with all the wasted thoughts we have – the solution is probably to identify global problems and channel people’s focus towards working on them\\n  -Brain chip: http://www.research.ibm.com/articles/brain-chip.shtml\\n  -Differences: What Can AI Get from Neuroscience? \\n  -Brains are not digital\\n  -Brains don’t have a CPU\\n  -Memories are not separable from processing\\n  -Asynchronous and continuous\\n  -Details of brain substrate matter\\n  -Feedback and Circular Causality\\n  -Asking questions\\n  -Brains has lots of sensors\\n  -Lots of cellular diversity\\n  -NI uses lots of parallelism\\n  -Delays are part of the computation\\n\\n### Brain v. Deep Learning\\n\\n-\\thttp://timdettmers.com/\\n  -\\tproblems with brain simulations:\\n    -\\tNot possible to test specific scientific hypotheses (compare this to the large hadron collider project with its perfectly defined hypotheses)\\n    -\\tDoes not simulate real brain processing (no firing connections, no biological interactions)\\n    -\\tDoes not give any insight into the functionality of brain processing (the meaning of the simulated activity is not assessed)\\n  -\\tNeuron information processing parts\\n    -\\tDendritic spikes are like first layer of conv net\\n    -\\tNeurons will typically have a genome that is different from the original genome that you were assigned to\\xa0at birth. Neurons may have additional or fewer chromosomes and have sequences of information removed or added from certain chromosomes.\\n    -\\thttp://timdettmers.com/2015/03/26/convolution-deep-learning/\\n    -\\tThe adult brain has 86 billion neurons, about 10 trillion synapse, and about 300 billion dendrites (tree-like structures with synapses on them\\n\\n## probabilistic models + inference\\n\\n<details>\\n  <summary>Wiener filter</summary>\\n  has Gaussian prior + likelihood\\n</details>\\n- gaussians are everywhere because of CLT, max entropy (subject to power constraint)\\n\\n  - for gaussian function, $d/dx f(x) = -x f(x)$\\n\\n### boltzmann machines\\n\\n- hinton & sejnowski 1983\\n- starts with a hopfield net (states $s_i$ weights $\\\\lambda_{ij}$) where states are $\\\\pm 1$\\n- define energy function $E(\\\\mathbf{s}) = - \\\\sum_{ij} \\\\lambda_{ij} s_i s_j$\\n- assume Boltzmann distr $P(s) = \\\\frac{1}{z} \\\\exp (- \\\\beta \\\\phi(s))$\\n- learning rule is basically expectation over data - expectation over model\\n  - could use wake-sleep algorithm\\n  - during day, calculate expectation over data via Hebbian learning (in Hopfield net this would store minima)\\n  - during night, would run anit hebbian by doing random walk over network (in Hopfield ne this would remove spurious local minima)\\n- learn via gibs sampling (prob for one node conditioned on others is sigmoid)\\n- can add hiddent units to allow for learning higher-order interactions (not just pairwise)\\n  - restricted boltzmann machine: no connections between \"visible\" units and no connections between \"hidden units\"\\n  - computationally easier (sampling is independent) but less rich\\n- stacked rbm: hinton & salakhutdinov (hinton argues this is first paper to launch deep learning)\\n  - don\\'t train layers jointly\\n  - learn weights with rbms as encoder\\n  - then decoder is just transpose of weights\\n  - finally, run fine-tuning on autoencoder\\n  - able to separate units in hidden layer\\n  - **cool - didn\\'t actually need decoder**\\n- in rbm\\n  - when measuring true distr, don\\'t see hidden vals\\n    - instead observe visible units and conditionally sample over hidden units\\n    - $P(h|v) = \\\\prod_i P(h_i | v)$ ~ easy to sample from\\n  - when measuring sampled distr., just sample $P(h|v)$ then sample $P(v|h)$\\n- ising model - only visible units\\n  - basically just replicates pairwise statistics (kind of like pca)\\n    - pairwise statistics basically say \"when I\\'m on, are my neighbors on?\"\\n  - need 3-point statistics to learn a line\\n- generating textures\\n  - learn the distribution of pixels in 3x3 patches\\n  - then maximize this distribution - can yield textures\\n- reducing the dimensionality of data with neural networks\\n\\n\\n\\n',\n",
       " '---\\nlayout: notes\\ntitle: Sensory Input\\ncategory: neuro\\n---\\n\\n#  sensory input\\n\\nnotes from Neuroscience, 5th edition + Intro to neurobiology course at UVA\\n\\n## 9- somatosensory\\n### cheat sheet\\n- vocab\\n\\t- *nerve* - bundle of axons\\n\\t- *tract* - bundle of axons in CNS\\n\\t- *nucleus* - bundle of neurons related to some function\\n\\t- *midline* - center of nervous system\\n\\t\\t- brain tends to be lateralized - one side is given control\\n\\t\\t- ex. speak almost exclusively from left side of brain\\n- information processing\\n\\t1. feedback (gain) \\n\\t\\t- almost always with glutamatergic  / GABA\\n\\t2. feedforward - anticipation\\n\\t\\t- estimate things before they happen\\n\\t\\t- adjust your behavior in advance of the world (ex. lean before you hit a table)\\n\\t3. center-surround inhibition (spatial gain)\\n\\t\\t- if you touch yourself, brain enhances sensitivity of one point by suppressing information from around it\\n\\n### sensory system overview\\n- we have dorsal root ganglia (DRG) on spinal cord\\n\\t- axon goes to CNS\\n\\t- dendrites go everywhere\\n\\t- *pseudounipolar* - born polar but become uni-polar\\n\\t\\t- dendrite goes straight into axon with cell body off to the side\\n\\t- do very little processing\\n- *dorsal horn* - top layer that controls sensory information\\n- in the brain stem, these are called cranial ganglia\\n\\t- special one is trigeminal ganglia (sensory receptors for face)\\n- oxytocin important clinically\\n- Trp channels - connected mechanically into membrane\\n- *dermatomes*\\n\\t- map of sensory parts to brain\\n\\t- segments of spinal cord correspond to stripes across your body\\n\\t- brain to feet: cervical, thoracic, lumbar, sacral\\n- *shingles* - virus where you get stripes of sores - single DRG\\n\\t- pops out the skin on the dendrite of one DRG\\n- peripheral damage won\\'t give you stripes of pain\\n- feeling resolution - depends on density of neurons innervating skin\\n\\t- more neurons - small receptive fields\\n\\t- two-point discrimination test - poke you at different points and see if you can tell if the points are different\\n\\t- higher discrimination is better\\n\\t- *discrimination* is different that *sensitivity* (like how it hurts when wounded)\\n\\n### 4 neuron classes\\n- they have certain structures that tune them into certain kinds of vibrations\\n\\t1. *Proprioception*\\n\\t\\t1. muscle spindles - on every neuron - fastest\\n\\t\\t\\t- measures stretch on every muscles\\n\\t\\t\\t- lets you know where your arm is\\n\\t\\t2. Golgi tendon organ\\n\\t\\t\\t- measures tension on tendon\\n\\t\\t\\t- safety switches - numb your body if you\\'re over-stressing something (make you let go of hanging on cliff)\\n\\t2. Ia II - *touch neurons*\\n\\t\\t- superficial - most sensitive\\n\\t\\t\\t1. Merkel: hi-res, slow adapt\\n\\t\\t\\t2. Meissner: hi-res, fast adapt\\n\\t\\t- deeper - sense vibrations, pressure\\n\\t\\t\\t3. Ruffini: low-res, slow adapt\\n\\t\\t\\t4. Pacinian: low-res, fast adapt\\n\\t\\t- these are in order of depth\\n\\t\\t- diabetes - tissue loss and pain / numbness are lost\\n\\t3. Adelta - fast pain\\n\\t4. C fibers - pain, temperature, itch\\n\\t\\t- very slow, stay on\\n\\t\\t- no myelination\\n\\t-  Pruritus - newly discovered set of sensory neurons\\n\\t\\t- between pain/touch - itch neurons\\n\\t\\t- new in mice: massage neurons\\n\\t\\t\\t- can only fire by stimulating in certain pattern\\n\\t\\t\\t- goes to emotion center not knowledge - pleasure\\n- speed proportional to diameter, myelination\\n- adaptation \\n\\t- some adapt slowly (you keep feeling something)\\n\\t- some adapt quickly (stop feeling)\\n\\t\\t- if you move finger slightly, start firing again when changed\\n\\t\\t- better if you feel cockroach that starts moving\\n\\n### pathways\\n- upper-body\\n\\t- *S1 cortex* - primary somatic sensory cortex - this is the knowledge of where was touched\\n\\t- *VPL* - everything accumulates here in the thalamus then goes to \\n\\t- *Cuneate nucleus* - everything goes into this\\n- lower-body (trunk down)\\n\\t- everything in the lower body goes to *Gracile nucleus* - in brain stem\\n- special case - sensory for face\\n\\t- *trigeminal ganglion* connects into *vpm* (thalamus) then goes into *S1 cortex*\\n- proprioceptive pathways\\n\\t- starts in lower body\\n\\t\\t- axons split - half go up to *Clark\\'s nucleus*\\n\\t\\t\\t- half go back into muscles\\n\\t\\t- Clark\\'s nucleus goes straight into cerebellum\\n\\t- starts in upper body - goes straight into cerebellum\\n\\t- thus cerebellum have map of where / how tense muscles are\\n\\n### representation\\n- cortex - this is where understanding is\\n\\t- dedicates area based on how many neurons coming in\\n\\t\\t- lips / hands have more area\\n\\t- S1 - primary somatosensory cortex\\n\\t\\t- most body parts\\n\\t\\t- neurons from functionally distinct columns\\n\\t\\t- cortex assigns space based on how much info comes in\\n\\t\\t\\t- after amputation and time, map grows into lost space\\n\\t\\t\\t- map is different when different stimuli are given to fingers\\n\\t- S2 - secondary somatosensory cortex\\n\\t\\t- processes and codes information from S1\\n\\t\\t- throat, tongue, teeth, jaw, gum\\n\\n### pathway\\n- mechanosensory\\n\\t1. DRG\\n\\t2. Cuneate, Gracile\\n\\t3. VPL\\n\\t4. S1\\n- face mechanosensory\\n\\t1. trigeminal ganglion\\n\\t2. principal nucleus of trigeminal complex\\n\\t3. vpm\\n\\t4. S1\\n- proprioception\\n\\t- lower body\\t\\n\\t\\t1. muscle spindles split\\n\\t\\t2. half go to motor neurons\\n\\t\\t3. other half go to Clark\\'s nucleus\\n\\t\\t4. clark\\'s nucleus -> cerebellum\\n\\t- upper body - straight to the cerebellum\\n\\n## 10 - nociception\\n### review\\n- chronic pain is very import clinically\\n- cortex - lets you know if you are sensing something\\n\\t1. *loss-of-function lesion* - piece of cortex is lost - lose awareness\\n\\t\\t- can come from stroke, migraine-aura\\n\\t2. *gain-of-function lesion* = excitatory lesion - like epilepsy\\n\\t\\t- cortex comes on when it shouldn\\'t\\n\\t\\t- increased awareness\\n\\t\\t- can come from stroke / migraine\\n- \"sixth sense\" - measuring stretch of all your muscles in cerebellum\\n- *nociception* = pain\\n\\t- has *nociceptors* - neurons that do nociception\\n\\t- *thermoceptors* - neurons that sense temperature\\n- two classes of linking receptors\\n\\t1. Adelta fibers - fast pain\\n\\t2. C fibers - slow and chronic\\n- Trp channels - mechanically or thermally gated\\n\\t- let Na+ in\\n\\t- trpV heat - binds capsaicin\\n\\t\\t- in the class of vanilloids\\n\\t\\t- birds not capsaicin sensitive\\n\\t- trpM cold - binds menthol\\n\\t\\t- adapts in minutes - stop feeling cold after a while\\n- synapses of nociceptors go to dorsal horn of drg\\n\\t- nociceptor goes contralateral (must cross midline) - if you cut left side of spinal chord, lose - mechanoception (ipsilateral) from left and nociception (contralateral) from right\\n\\t- mechanoreceptors, by contrast, send axon up the spinal cord\\n\\t- dorsal horn has laminal structure (has layers)\\n1. know where pain is\\n\\t- somatosensory cortex\\n2. care about pain\\n\\t- *insular cortex* - emotional part of brain\\n\\t\\t- whether or not you care about pain\\n\\t\\t- pairs up with other senses\\n- can have both loss-of-function and gain-of-function lesions in both places\\n- referred pain map - map that refers to a specific problem (ex. esophagus)\\n- *visceral pain* - don\\'t know where the pain is\\n- *hyperalgesia* - increased pain sensitivity\\n\\t- pain sensing neurons are hyperactive because of inflammation\\n\\t- pain sensing neuron releases substance P into Mast cell or neutrophil which releases histamine which strengthens receptor\\n\\t- prostaglandins activate nococeptors\\n- *allodynia* - when mechanosensation hurts - not understood\\n- turning off pain - add serotonin\\n\\t1. exercise\\n\\t2. lack of serotonin ~ mood disorders\\n\\t- central sensitization: allodynia\\n- these mechanisms work through *introception*\\n\\t- senses chemical imbalances\\n- phantom limbs and phantom pain - if you lose a limb and still feel pain\\n- *mechanoreceptors inhibit nociceptors*\\n\\n### pathway\\n- nociception\\n    - ![](../assets/10_1.png)\\n\\t- same as mechanosensory except goes all the way to thalamus\\n\\t\\t- doesn\\'t stop in brainstem\\n\\t- crosses the midline after first synapse\\n- visceral pain\\n\\t- axons mainline straight up, go through vpl, go straight to insular cortex\\n\\n## 11 - vision (eye)\\n\\n- most of visual system is to read faces\\n- eye\\n\\t- aqueous humor\\n\\t- posterior chamber\\n\\t- lens\\n\\t- ciliary muscles\\n\\t- retina\\n\\t- fovea\\n\\t- optic disk\\n\\t- optic nerve and retinal vessels\\n- to see far, stretch lens = *accomodation*\\n- retina - rods and cones are at back\\n\\t- cones - color\\n\\t- retinal ganglion cells sends down signal\\n- 12 days to turnover whole photoreceptor disks into PE (pigment epithelium)\\n\\t- PE is what the rods / cones are in\\n\\t- PE contains optic disks containing rhodopsin protein that is sensitive to light that break off of rods / cones\\n- light leads to inhibition\\n- melanopsin - receptor for blue light\\n\\n\\n### circuits\\n- accomodation - stretching lens uncrosses lines\\n- function photoreceptor\\n\\t- usually cGMP is letting in Na/Ca\\n\\t\\t- Ca provides negative feedback here\\n\\t- when light hits, retinal inside rohodopsin activates phosphodiesterase - breaks down cGMP so channel closes and they aren\\'t let in\\n- light on middle\\n\\t- depolarizes cone\\n\\t- excites oncenter\\n\\t- inhibits offcenter\\n\\t- these adjust quickly\\n- horizontal cells - takes positive input from photoreceptor and inhibits it back\\n\\t- inhibits horizontal cells else around it - creates contrast\\n\\t- have these for each color\\n\\n### pathway\\n- ![](../assets/11_1.png)\\n1. rods / cones\\n(2). horizontal cells - regulate gain control, how fast adapts, contrast adaptation\\n3. bipolar cells\\n(4). amacrine cells - processing of movements\\n5. retinal ganglion cells\\n6. go into thalamus then to cortex\\n(6). small amount go into brain stem and control mood / circadian rhythms\\n\\n## 12 - central visual system\\n- cortex is a pizza box\\n- has columns\\n- autophagy - process by which cells eat parts of themselves\\n\\t- nobel 2016\\n- cones - color\\n- 12 day cycle for processing optic disks\\n- photoreceptors have cyclic G-activated channel\\n\\t- light shuts down photoreceptors\\n\\t- cell decreases in activity\\n- very roughly - each cone connects to cone bipolar cell\\n\\t- this gets represented by one column in the cortex\\n- 15-30 rods connect to 1 rod bipolar cells\\n- cortex has 6 layers\\n\\t- each has tons of neurons, mostly pyramidal neurons\\n\\t- column is a section through the 6 layers - all does about the same thing\\n\\t- orientation columns responds to specific x,y\\n\\t\\t- has subregions that respond to specific orientations\\n- ocular dominance column - both eyes for same coordinate go to same spot\\n\\t- dominated by one eye\\n- distance\\n\\t- far cells\\n\\t- tuned cells\\n\\t- near cells\\n- *V4* in temporal lobe - object recognition\\n\\n### pathways\\n- ![](../assets/12_1.png)\\n- overall\\n\\t1. V1\\n\\t2. V2\\n\\t3. V4 or MT\\n- central projections\\n\\t- retinal ganglions\\n\\t- all go through optic stuff\\n\\t\\n## 13 - auditory system\\n- ear parts\\n\\t- outer\\n\\t- middle\\n\\t\\t- tympanic membrane\\n\\t- inner\\n\\t\\t- cochlea - senses the sound\\n\\t\\t- oval window\\n\\t\\t- round window - not understood\\n- conductive hearing loss - in the outer/middle ear\\n- sensorineural hearing loss - in the cochlea\\n\\t- can\\'t be fixed with hearing aids\\n- humans\\n\\t- 2-5kHz ~= human speech (can sometimes hear more)\\n\\t- 30-100x boost for tympanic membrane\\n\\t\\t- this differs between people\\n\\t- 200x focus onto oval window\\n- cochlea\\n\\t- 4 layers\\n\\t\\t- inner hair cells - what you hear with\\n\\t\\t- outer hair cells - generate sound\\n\\t\\t\\t- generates noise at every frequency except one you want to hear\\n\\t- otoacoustical emmision - low buzz that is produced\\n\\t- tenitis - ringing in the ears\\n\\t\\t- can be internal\\n\\t\\t- can be peripheral - generated by otoacoustical emmision\\n\\t- high frequencies right next to cochlea\\n\\t- low frequencies on distal tip\\n\\t- human high frequency cells die with age\\n- hair cells\\n\\t- bundle of cilia\\n\\t- have an orientation\\n\\t- kinocilium is the tallest\\n\\t- tall ones are in the back\\n\\t- dying hair cells - can\\'t be replaced\\n\\t\\t1. \\u200b\\n\\t\\t2. loud sounds\\n\\t\\t3. certain antibiotics\\n- auditory pathwayz\\n\\t- *MSO - medial superior olive* - decides where the sounds is coming from\\n\\t\\t- takes input from right / left ear, decides which came in first\\n\\t- medial geniculate complex of the thalamus\\n- brain shape\\n\\t- folds are pretty random\\n\\t- phrenology - shape of skull was based on brain\\n\\t\\t- thought it could determine personality\\n\\t\\t- false\\n\\t\\t- Hsechl\\'s Gyrus folding pattern is not random\\n\\t\\t\\t- argument that if you have one, you are more musical\\n- any sounds is made up of a bunch of frequencies\\n\\n### circuits\\n- K depolarizes hair cells, lets in Ca, releases vesicles\\n\\n## 14 - vestibular system\\n- very related to cochlea\\n\\t- same hair cells\\n- differences\\n    - ![](../assets/14_1.png)\\n\\t1. vestibular system doesn\\'t use cortex (you don\\'t think about it)\\n\\t\\t- goes right into spinal chord\\n\\t2. controls eye movements\\n\\t\\t- one of the fastest circuits in the brain\\n\\t- clinically important\\n\\t\\t- you have to be able to have your balance\\n1. each column is computational unit of the cortex\\n2. ocular dominance column\\n\\t- one for each eye\\n- labyrinth and its innervation\\n\\t- semicircular canals\\n\\t\\t- can only measure one axis of rotation\\n\\t\\t- remember *horizontal canal* - measures turning head left to right\\n\\t\\t\\t- this measures acceleration\\n\\t\\t\\t- like a hoola hoop filled with glitter\\n\\t\\t\\t- has ampulla at one place in the hoop\\n\\t\\t\\t- cupula - sits over the ampulla\\'s hair cells\\n\\t\\t\\t- if the \"glitter\" hits the cupula, it will bend the hair cells\\n\\t\\t\\t- if you keep spinning, fluid starts moving and you stop detecting movement\\n\\t\\t\\t\\t- this means the canals adapt mechanically\\n\\t\\t\\t- if you stop spinning, fluid keeps moving and system thinks you\\'re spinning the other way\\n\\t\\t\\t- right horizontal canal activated by turn to the right\\n\\t\\t\\t\\t- same for left\\n\\t- scarpa\\'s ganglion - has hair cells inside\\n\\t\\t- sends axons into vestibular nuclei\\n\\t- lots of fluid (high in K+)\\n\\t- macula - place where all the hair cells are\\n\\t\\t- Ampullae - at base of canals\\n\\t\\t\\t- hair cells all in the same direction\\n\\t\\t- utricle and saccule - measure head tilt\\n\\t\\t\\t- hair cells in multiple orientations\\n\\t\\t\\t- these contain otoconia\\n\\t\\t\\t\\t- these are little crystals that move with gravity\\n\\t\\t\\t\\t- measure acceleration and tilt\\n- tilts do not adapt - they keep firing while you\\'re leaned back\\n\\t- they basically report tilt / position at all times\\n- tiplink - connect cilia together for hair cells\\n\\t- when they move, tiplink move, pull on ion channels\\n\\t- motor on connected hair cell moves up and down to generate correct amount of tension\\n\\t\\t- motor uses myosin and actin\\n\\t\\t- harming these proteins can cause deafness\\n- both eyes must always be looking in the same direction\\n\\t- also must be sitting over image for a while\\n- *ipsilateral* - same side\\n- contralateral - different side\\n- vestibular ocular reflex VOR - turn your head to the right, eyes move left\\n\\t- doesn\\'t require cortex\\n\\t- only have to learn excitatory\\n\\t\\n## 15 - chemical senses\\n![](../assets/15_1.png)\\n- cAMP is used by GPCR',\n",
       " '---\\nlayout: notes\\ntitle: memory\\ncategory: neuro\\n---\\n\\n#  memory\\n\\n## history\\n\\n- The Neuron Doctrine – the neuron is the fundamental building block and elementary signaling unit of the brain\\n- Golgi – develops silver staining method which allows Cajal to see entire neuron\\n- Santiago Ramon y Cajal – Spanish anatomist who simplifies neuron forest and looks at individual neurons, develops model with dendrites, cell body, and axon\\n- 4 parts: neuron, synapse, connection specificity (specific neurons connect to specific others), dynamic polarization (signals travel in one direction)\\n- Freud looks into Cajal’s theories, but doesn’t incorporate them\\n- 1906 – they share Nobel despite Golgi hating Cajal’s theories\\n- 1955 – Cajal’s intuitions borne out conclusively\\n\\n### next generation\\n\\n- Sherrington – builds on Cajal’s work – finds that neurons integrate signals and some signals are inhibitory\\n- Shares Nobel with Adrian in 1932 – Adrian is younger and grateful\\n- Phases of Nerve Signaling\\n- Galvani discovers electrical activity in animals, Helmholtz measures speed of electrical signals in neurons\\n- Adrian measures action potentials and sees that they all have the same size and that intensity correlates with their frequency\\n- Bernstein (student of Helmholtz) finds that ions carry the electrical current – he investigates only the potassium ion\\n- Ionic hypothesis – Hodgkin, Huxley (and Katz) – find sodium, potassium in squid axon using voltage clamp, discover voltage-gated channels, win Nobel in 1963\\n- Interneuronal signaling\\n- 1920s – Dale and Loewi find that synaptic transmission is chemical - use acetylcholine in frogs\\n- synaptic potential – has different sizes, slower – only over synapses (can be excitatory or inhibitory, can generate action potential)\\n- long-term potentiation\\xa0(LTP) is a persistent strengthening of\\xa0synapses\\xa0based on recent patterns of activity\\n- Eccles – believed in spark theory (synaptic transmission was electrical), but after talking to Popper disproves it with Katz and believes in soup theory (synaptic transmission is chemical)\\n- Glutamate – major excitatory neurotransmitter\\n- GABA – main inhibitory transmitter\\n- Katz’s lab later showed there are a few synapses that are electrical\\n- Katz – found that neurotransmitters were released by voltage-gated gates letting in Ca ions \\n- Packets of neurotransmitters called quanta are released in synaptic vesicles\\n- Confirmed in in 1955\\n\\n## modern generation\\n\\n- Four Lobes\\n-Frontal – working memory and lots of stuff\\n-Temporal – auditory processing, language, and memory\\n-Parietal – sensory information\\n-Occipital – vision\\n-Brain maps - Marshall showed that, even though the different sensory systems carry different types of information and end up in different regions of the cerebral cortex, they share a common logic in their organization: all sensory information is organized topographically in the brain in the form of precise maps of the body\\'s sensory receptors \\n-Broca and Wernicke find that specific brain regions are in charge of specific functions\\n-Broca’s area- expression of language\\n-Werknicke’s area – perception of language\\n-Patient H.M. – research by Brenda Milner\\n-He couldn’t store new memories although he could learn new skills\\n-Memory is a distinct mental function, clearly separate from other perceptual, motor, and cognitive abilities. \\n-Short-term memory and long-term memory can be stored separately. Loss of medial temporal lobe structures, particularly loss of the hippocampus, destroys the ability to convert new short-term memory to new long-term memory. \\n-There is explicit and implicit memory (implicit is a collection of processes)\\n-Milner showed that at least one type of memory can be traced to specific places in the brain \\n-Early Eric Kandel\\n-Gets lucky start recording in hippocampus\\n-Decides to start recording in Aplysia – large and has only 20,000 neurons separated into nine ganglia (human brain ~ 100 billion) \\n-Hypothesizes that persistent changes in the strength of synaptic connections results in memory storage \\n-just as neurons and their synaptic connections are exact and invariant, so, too, the function of those connections is invariant. \\n-First, we found that the changes in synaptic strength that underlie the learning of a behavior may be great enough to reconfigure a neural network and its information-processing ability \\n-a given set of synaptic connections between two neurons can be modified in opposite ways— strengthened or weakened—by different forms of learning. \\n-Third, the duration of short-term memory storage depends on the length of time a synapse is weakened or strengthened. \\n-Fourth, we were beginning to understand that the strength of a given chemical synapse can be modified in two ways, depending on which of two neural circuits is activated by learning—a mediating circuit or a modulatory circuit\\n-Learning may be a matter of combining various elementary forms of synaptic plasticity into new and more complex forms, much as we use an alphabet to form words. \\n-forgetting had at least two phases: a rapid initial decline that was sharpest in the first hour after learning and then a much more gradual decline that continued for about a month. \\n-Homosynaptic - the depression occurred in the same neural pathway that was stimulated \\n-Strengthening synapses = greater responses\\n-Short-term to Long-term\\n-Memory consolidation – short-term is subject to disruption\\n-Head injuries or seizures can lead to retrograde amnesia – you forget what was in you short-term memory\\n-Electric shocks were able to get rid of short-term memory\\n-A short-term memory lasting minutes is converted—by a process of consolidation that requires the synthesis of new protein—into stable, long-term memory lasting days, weeks, or even longer \\n-Long-term memory results in growing or shedding synapses\\n-As the memory fades, the number of synapses goes almost back to normal, with the difference accounting for relearning a task easier\\n-Recall\\n-Based on cues, in the case of Aplysia gill-withdrawal, external stimulus\\n-Short-term\\n-Short-term memory changes are presynaptic - during short-term habituation lasting minutes, the sensory neuron releases less neurotransmitter, and during short-term sensitization it releases more neurotransmitter\\n-Relies on interneurons:\\n-Mediating circuits – produce behavior directly, sensory neurons that innervate the siphon, the interneurons, and the motor neurons that control the gill-withdrawal reflex \\n-Modulatory circuits - not directly involved in producing a behavior but instead fine-tunes the behavior in response to learning by modulating—heterosynaptically—the strength of synaptic connections between the sensory and motor neurons \\n-For example, in gill-withdrawal sensitization, interneurons release serotonin into the presynaptic terminals of the sensory neurons\\n-This generates a long, slow synaptic potential in the motor neurons\\n-Ionotropic receptors – neurotransmitter-gated, open ion channels\\n-Metabotropic receptors – gated, activate enzymes; these enzymes can make cyclic AMP, act much longer\\n-Serotonin binds to metabotropic receptors in the presynaptic terminal of sensory neurons increasing the amount of cAMP and in turn the amount of glutamate\\n-This was verified in studies of Drosophila \\n-Sensory regions map to specific places in brain, keeping proximity\\n-Pavlov\\n-Habituation – animal repeatedly presented with neutral sensory stimulus learns to ignore it\\n-Sensitization – animal learns strong stimulus is dangerous and enhances its defensive reflexes\\n-Classical Conditioning – pair neutral stimulus with potentially dangerous stimulus, animal learns to respond to neutral stimulus\\n-Long-term memory\\n-Proteins must be made - DNA makes RNA, and RNA makes protein \\n-The serotonin itself was able to make new synapses grow via synthesis of proteins in the nucleus\\n-Genes\\n-There are effector genes which mediate cellular functions and regulatory genes which switch them on and off\\n-Genes have regions called promoters and repressors and regulatory proteins must bind to these in order to express them\\n-With repeated pulses of serotonin, kinase A move into the nucleus – turn on regulatory protein called CREB which turns on some genes\\n-Also requires turning off other genes\\n-MAP kinase also moves into nucleus and depresses CREB-2\\n-Together, activating CREB and deactivating CREB-2 transfers short-term memories to long-term\\n-Synaptic marking – the proteins produced in the nucleus know which synapses to go to because of their short-term changes\\n-Proteins must be synthesized locally at the synapses\\n-Dormant mRNA is sent to all the synapses\\n-There is a protein called CPEB that is activated by serotonin and is required by the synapses to maintain protein synthesis\\n-Resembles a prion – special protein with dominant and recessive conformation, dominant can be harmful\\n-Dominant is self-perpetuating – turns recessive into dominant\\n-Explicit memory - depends on the elaborate neural circuitry of the hippocampus and the medial temporal lobe, and it has many more possible storage sites.\\n-Long-term potentiation - synaptic strengthening mechanism in the hippocampus \\n-long-term potentiation describes a family of slightly different mechanisms, each of which increases the strength of the synapse in response to different rates and patterns of stimulation \\n-glutamate acts on two different types of ionotropic receptors in the hippocampus, the AMPA receptor and the NMDA receptor. The AMPA receptor mediates normal synaptic transmission and responds to an individual action potential in the presynaptic neuron. The NMDA receptor, on the other hand, responds only to extraordinarily rapid trains of stimuli and is required for long-term potentiation flow of calcium ions into the postsynaptic cell acts as a second messenger (much as cyclic AMP does), triggering long-term potentiation - allows calcium ions to flow through its channel if and only if it detects the coincidence of two neural events, one presynaptic and the other postsynaptic: The presynaptic neuron must be active and release glutamate, and the AMPA receptor in the postsynaptic cell must bind glutamate and depolarize the cell. \\n-explicit memory in the mammalian brain, unlike implicit memory in Aplysia or Drosophila, requires several gene regulators in addition to CREB \\n-Cognitive science - Kantian notion that the brain is born with a priori knowledge, \"knowledge that is . . . independent of experience.\" \\n-Visual system\\n-Different layers respond to different things - each cell in the primary visual cortex responds only to a specific orientation of such light-dark contours \\n-The brain does not simply take the raw data that it receives through the senses and reproduce it faithfully. Instead, each sensory system first analyzes and deconstructs, then restructures the raw, incoming information according to its own built-in connections and rules \\n-What and where are different neural pathways\\n-there is no single cortical area to which all other cortical areas report exclusively, either in the visual or in any other system. In sum, the cortex must be using a different strategy for generating the integrated visual image. \\n-Spatial Map\\n-the hippocampus of rats contains a representation—a map—of external space and that the units of that map are the pyramidal cells of the hippocampus, referred to as \"place cells.\"\\n-The brain sometimes codes with coordinates centered on the receiver and sometimes relative to the outside world\\n-Attention acts as a filter, selecting some objects for further processing \\n-A modulating circuit involving dopamine in the hippocampus forms spatial maps\\n-The dopamine comes from the cerebral cortex (a higher level part of the brain)\\n-Eric Kandel tried to translate his research into a cure for age-related memory loss\\n-Alzheimer’s: This degeneration of tissue is caused in large part by the accumulation of an abnormal material known as ß-amyloid in the form of insoluble plaques in the spaces between brain cells. \\n-We found that drugs which activate these dopamine receptors, and thereby increase cyclic AMP, overcome the deficit in the late phase of long-term potentiation. They also reverse the hippocampus-dependent memory deficit. \\n-Various disorders are being solved slowly through research\\n-Consciousness\\n-consciousness in people, is an awareness of self, an awareness of being aware.\\n-The brain does reconstruct our perception of an object, but the object perceived—the color blue or middle on the pia\\tno—appears to correspond to the physical properties of the wavelength of the reflected light or the frequency of the emitted sound \\n-Claustrum is connected to a bunch of different brain parts – could regulate attention\\n-viewing frightening stimuli activates two different brain systems, one that involves conscious, presumably top-down attention and one that involves unconscious, bottom-up attention, or vigilance \\n-readiness potential can measure what a person is going to do before they know they want to do it',\n",
       " '---\\nlayout: notes\\ntitle: Motor system\\ncategory: neuro\\n---\\n\\n#  motor system\\n\\nnotes from Neuroscience, 5th edition + Intro to neurobiology course at UVA\\n\\n## 16 lower\\n- sensory in dorsal spinal cord, motor in ventral\\n  - farther out neurons control farther out body parts (medial=trunk, lateral=arms,legs)\\n- one motor neuron (MN) innervates multiple fibers\\n  - the more fibers/neuron, the less precise\\n  - *MN pool* - group of MNs=motor units\\n- muscle tone = all your muscles are a little on, kind of like turning on the car engine and when you want to, you can move forward\\n  - more firing = more contraction\\n- MN types\\n  1. fast fatiguable - white muscle\\n  2. fast fatigue-resistant\\n  3. slow - red muscles, make atp\\n  - muscles are innervated by a proportion of these MNs\\n- reflex\\n  - whenever you get positive signal on one side, also get negative on other\\n  - *flexor* - curl in (bicep)\\n  - *extensor* - extend (tricep)\\n1. proprioceptors (+) - measure length - more you stretch, more firing of alpha MN to contract\\n  - *intrafusal muscle=spindle* - stretches the proprioceptor so that it can measure even when muscle is already stretched\\n    - $\\\\gamma$ motor neuron - adjusts intrafusal muscles until they are just right\\n      - keeps muscles tight so you know how much muscle is streteched\\n      - if alpha fires a lot, gamma will increase as well\\n        - high gamma allows for fast responsiveness - brainstem modulators (serotonin) also do this\\n    - opposes muscle stretch to keep it fixed\\n    - spindle -> activates muscles -> contracts -> turns off\\n    - sensory neurons / gamma MNs innervate muscle spindle\\n  - homonymous MNs go into same muscle, antagonistic muscle pushes other way\\n2. *golgi tendon* (-) measures pressure not stretch\\n  - safety switch\\n  - inhibits homonymous neuron so you don\\'t rip muscle off\\n- *ALS* = Lou Gehrig\\'s disease \\n  - MNs are degenerating - reflexes don\\'t work\\n  - progressive loss of $\\\\alpha$ MNs\\n  - last neuron to go is superior rectus muscle -> people use eyes to talk with tracker\\n- *CPG* = central pattern generator \\n  - ex. step on pin, lift up leg\\n  - walking works even if you cut cat\\'s spinal cord\\n  - collection of interneurons\\n\\n## 17 upper\\n- ![](../assets/17_1.png)\\n- ![](../assets/17_2.png)\\n- ![](../assets/17_3.png)\\n- ![](../assets/17_4.png)\\n- cAMP is used by GPCR\\n- lift and hold circuit\\n  1. ctx->*lateral white matter*->lateral ventral horn->limb muscles\\n    - lateral white matter - most sensitive to injury\\n  2. brainstem->*medial white matter*->medial horn->trunk\\n    - medial white matter -> goes into trunk\\n- *bulbarspinal tracts*\\n  1. lateral and medial *vestibulospinal* tracts - feedback\\n    - automated system - not much thinking\\n    - posture - reflex\\n    - too slow for learning surfing\\n  2. *reticular* - feedforward = anticipate things before they happen\\n    - command / control system for trunk muscles (posture)\\n    - feedforward - not a reflex, lean back before opening drawer\\n    - *caudal pontine* - feeds into spinal cord\\n  3. *colliculospinal* tract\\n    - has *superior colliculus* - eye muscles, neck-looking\\n    - see ch. 20 - reflex\\n- *corticular bulbar tract* (premotor->primary motor->brainstem)\\n  - motor cortexes - this info is *descending*\\n  - can override reticular reflexes in reticular formation\\n  - premotor cortex (P2) - contains all actions you can do\\n    - has *mirror neurons* that fire ahead of primary neurons\\n      - fire if you think about it or if you do it\\n  - primary motor cortex (P1)\\n    - layer 1 ascending\\n    - layer 4 input\\n    - layer 5 - *Betz cells* - behave like 6 (output)\\n    - layer 6 - descending output\\n    - has map like S1 does\\n      - *Jacksonian march* get seizure that goes from feet to face (usually one side)\\n        - epileptic seizure - neurons fire too much and fire neurons near them\\n          - *insular* - flashes of moods\\n          - *pyriform* - flashes of smells\\n  - *Betz cells* - if they fire, you will do something\\n    - dictate a goal, not single neuron to fire\\n    - axons to *ventral horn* of spinal cord\\n- lesions\\n  1. upper\\n    - *spasticity* - unorganized leg motions\\n    - increased tone - tight muscles\\n    - hyperactive deep reflexes\\n      - ex. *babinski\\'s sign* \\n      - curl foot down a lot because you don\\'t know how much to curl\\n      - curling foot down = *normal plantar*\\n      - more serotonin can cause this\\n  2. lower\\n    - hypoactive deep reflexes\\n    - decreased tone\\n    - severe muscle atrophy \\n- pathways\\n  - Betz cell\\n    - 90% cross midline in brainstem - control limbs\\n    - 10% don\\'t cross - trunk muscles\\n\\n## 18 basal ganglia (choose what you want to do)\\n- ![](../assets/18_1.png)\\n- ![](../assets/18_2.png)\\n- ![](../assets/18_3.png)\\n- \"who you are\"\\n- outputs\\n  1. brainstem\\n  2. motor cortex\\n- 4 loops (last 2 aren\\'t really covered)\\n  - motor loops\\n    1. *body movement loop*\\n      - *SnC -> S (CP) -> (-) Gp -> (-) VA/VL -> motor cortex*\\n    2. *oculomotor loop*\\n      - *cortex -> caudate -> substantia nigra pars reticulata -> superior colliculus*\\n  - non-motor loops\\n    3. *prefrontal loop* - daydreaming (higher-order function)\\n      - spiny neurons corresponding to a silly idea (alien coming after you) filtered out because not fired enough\\n      - schizophrenia - can\\'t filter that out\\n    4. *limbic loop* - mood\\n      - has *nucleus accumbens*\\n      - can make mood better with dopamine\\n- *substantia nigra*\\n  1. *pars compacta* - dopaminergic neurons (input to striatum)\\n    - more dopamine = more strength between cortical pyramidal neurons and spiny neurons (turns up the gain)\\n    - dopamine helps activate a spiny neuron\\n    - may be the ones that learn (positive outcome is saved, will result in more dopamine later)\\n    - *Parkinson\\'s* - specific loss of dopaminergic neurons\\n      - dopaminergic neurons form *melanin* = dark color\\n      - when you get down to 20% what you were born with\\n      - know what they need to do - don\\'t have enough dopamine to act\\n      - treat with L Dopa -> something like dopamine -> take out globus pallidus\\n    - cocaine, amphetamine - too much dopamine\\n    - *Huntington\\'s* - death of specific class of spiny neurons\\n      - have uncontrolled actions\\n    - *Tourette\\'s* - too much dopamine\\n      - also alcohol\\n    - MPPP (synthetic heroin)\\n      - MPTP looks like dopamine but turns into MPP and kills dopaminergic neurons\\n      - treated with L Dopa to reactivate spiny neurons\\n  2. *pars reticulata* \\n    - doesn\\'t have dopamine (output from striatum)\\n1. *striatum* contains spiny neurons\\n  1. *caudate* (for vision) - output to globus pallidus and substantia nigra (pars reticulata)\\n  2. *putamen* - output only to globus pallidus\\n  - each spiny neuron gets input from ~1000 cortical pyramidal cells\\n2. *globus pallidus*\\n  - each spiny neuron connects to one globus pallidus neuron\\n  - deja vu - spiny neuron you haven\\'t fired in a while\\n3. *VA/VL* (thalamus)\\n  - all motor actions must go through here before cortex\\n  - has series of commands of all actions you can do\\n  - has parallel set of betz cells that will illicit those actions\\n  - VA/VL is always firing, globus pallidus inhibits it (tonic connection)\\n\\n## 19 cerebellum (fine tuning all your motion)\\n- ![](../assets/19_1_important.png)\\n- ![](../assets/19_2_important.png)\\n- ![](../assets/19_3.png)\\n- ![](../assets/19_4.png)\\n- ![](../assets/19_5.png)\\n- ![](../assets/19_6.png)\\n- ![](../assets/19_7.png)\\n- redundant system - cortex could do all of this, but would be slow\\n- repeated circuit - interesting for neuroscientists\\n- all info comes in, gets processed and goes back out\\n  - cerebellum gets motor *efferant copy*\\n  - all structures on your brain that do processing send out efferent\\n  - cerebellum sends efferant copy back to itself with time delay (through inferior olive)\\n1. *cerebrocerebellum*\\n   - deals with premotor cortex (mostly motor cortex)\\n2. *spinocerebellum* = *clarke’s nucleus*, knows stretch of every muscle, many proprioceptors go straight into here\\n  - motor cortex\\n  - has a map of muscles\\n3. *vestibular cerebellum* - vestibular->cerebellum->vestibular\\n  - vestibular system leans you back but if wind blows, have to adjust to that\\n- input\\n  - *pontine* nuclei (from cortex)\\n  - *vestibular* nuclei (balance)\\n  - *cuneate* nucleus (somatosensory from spinal upper body)\\n  - *clarke* (proprio from spinal lower body)\\n- processing\\n  - cerebellar deep nuclei\\n- output\\n  - deep cerebellar nuclei\\n    - go to superior colliculus, reticular formation\\n  - VA/VL (thalamus) - back to cortex\\n  - red nucleus\\n- circuit 1 - fine-tuning\\n- circuit 2 - detects differences, adjusts\\n  - cerebellum -> *red nucleus* (is an efferant copy) -> *inferior olive* -> cerebellum\\n  - compare new copy to old copy \\n- cells\\n  - *purkinje cells* - huge number of dendrite branches\\t\\t- dead planar allows good imaging\\n    - GABAergic\\n  - (input) mossy fibers -(+)> granule cells (send parallel fibers) -(+)> purkinje cell -(-)> deep cerebellar nuclei (output)\\n    1. *mossy->granule->parallel fibers* connect to ~100,000 parallel fibers\\n    2. *climbing fiber* - comes from inferior olive and goes back to purkinje cell (this is the efferent copy) = training signal\\n  - loops\\n    - deep excitatory loop (climbing/mossy) -(+)-> deep cerebellar nuclei\\n    - cortical inhibitory loop (climbing/granule) -(+)-> purkinje\\n      - the negative is from purkinje to deep cerebellar nuclei\\n- alcohol \\n  - can create gaps = *folia*\\n  - long-term use causes degeneration = *ataxia* (lack of coordination)\\n\\n## 20 eye movements/integration\\n- ![](../assets/20_1.png)\\n- ![](../assets/20_2.png)\\n- Broca\\'s view - look at people with problems\\n- Ramon y Cajal - look at circuits\\n- 5 kinds of eye movements\\n  1. *saccades*\\n    - use cortex, superior colliculus (visual info -> LGN -> cortex, 10% goes to brainstem)\\n    - constantly moving eyes around (fovea)\\n    - ~scan at 30 Hz\\n    - 5 Hz=200 ms for cortex to process so pause eyes (get 5-6 images)\\n      - there is a little bit of drift\\n    - can\\'t control this\\n    - humans are better than other animals at seeing things that aren\\'t moving\\n  2. *VOR* - vestibular ocular reflex - keeps eyes still\\n    - use vestibular system, occurs in comatose\\n    - fast\\n    - works better if you move your head fast\\n  3. *optokinetic system* - tracks with eyes\\n    - ex. stick head out window of car and track objects as they go by\\n    - slower than VOR (takes 200 ms)\\n    - works better if slower\\n    - reflex\\n    - in cortex (textbooks) but probs brainstem (new)\\n  4. *smooth pursuit* - can track things moving very fast\\n    - suppress saccades and track smoothly\\n    - only in higher apes\\n    - *area MT* is highest area of motion coding and goes up and comes down multiple ways\\n    - high speed processing isn\\'t understood\\n       - could be retina processing\\n  5. *vergence* - crossing your eyes\\n    - suppresses conjugate eye movements\\n    - we can control this\\n    - only humans - bring objects up very close\\n    - reading uses this\\n- eye muscles\\n  - rectus\\n    - vertical\\n      - superior\\n      - inferior\\n      - use complicated *vertical gaze center*\\n        - last to degenerate in ALS\\n        - *locked-in syndrome* - can only move eyes vertically\\n        - controls oculomotor nucleus\\n    - lateral\\n      - medial\\n      - lateral (controlled by *abducens*)\\n      - use *horizontal gaze center=PPRF* which talk to abducens\\n        -*MLF* connects abducents to opposite medial lateral rectus muscle\\n    - oblique - more circular motions\\n      - superior (controlled by *trochlear nucleus*)\\n      - inferior\\n  - everything else controlled by *oculomotor nucleus*\\n- *superior colliculus* has visual map\\n  - controls saccades, connects to gaze centers\\n  - takes input from basal ganglia (oculomotor loop)\\n  - also gets audio input from inferior colliculus (hear someone behind you and turn)\\n  - gets strokes\\n  - redundant with *frontal eye field* in secondary motor cortex\\n    - connects to superior colliculus, gaze center, and comes back\\n    - if you lose one of these, the other will replace it\\n    - if you lose both, can\\'t saccade to that side\\n\\n## 21 visceral (how you control organs, stress levels, etc.)\\n- ![](../assets/21_1.png)\\n- ![](../assets/21_2.png)\\n- ![](../assets/21_3.png)\\n- ![](../assets/21_4.png)\\n- parasympathetic works against sympathetic\\n- divisions\\n  1. *sympathetic* - fight-or-flight (*adrenaline*)\\n    - functions\\n      - neurons to smooth muscle\\n      - pupils dilate\\n      - increases heart rate\\n      - turn off digestive system\\n      - 2 things with no parasympathetic counterpart\\n        - increase BP\\n        - sweat glands\\n    - location\\n      - neurons in spinal cord lateral horn\\n        - send out neurons to sympathetic trunk (along the spinal cord)\\n        - all outgoing connections are adrenergic\\n    - beta-adrenergic drugs block adrenaline\\n      - beta agonist - activates adrenaline receptors (do this before EKG)\\n  2. *parasympathetic* - relaxing (*ACh*) \\n    - location\\n      1. brainstem\\n        1. *edinger westphal nucleus* - pupil-constriction\\n        2. *salivatory nucleus*\\n        3. *vagus nucleus* - digestive system, sexual function\\n        4. *nucleus ambiguous* - heart\\n        5. **nucleus of the solitary tract**\\n          - all input/output goes through this\\n          1. *rostral* part (front) - taste neurons\\n          2. *caudal* part (back) contains all sensory information of viscera (ex. BP, heart rate, sexual \\n      2. *sacral* spinal cord (bottom) - gut/bladder/genitals\\n      - not parallel to sympathetic – poor design - may cause stress-associated diseases \\n    - hard to make drugs with ACh\\n  3. *enteric* nervous system - in your gut\\n    - takes input through *vagus nerve* from vagus nucleus\\n    - also has sensory neurons and sends afferents back to brainstem\\n- pathway \\n  - *insular cortex* - what you care about\\n  - *amygdala* - contains emotional memories\\n  - *hypothalamus* - controls a lot\\n    - mostly peptinergin neurons\\n    - aging, digestion, mood, straight to bloodstream & CNS\\n    - releases hormones\\n    - ex. *leptin* - stops you eating when you eat calories\\n  - *reticular formation* - feedforward, prepares digestion before we eat\\n- three examples\\n  1. heart rate\\n    - starts at nucleus ambiguous\\n    - also takes input from chemoreceptors (ex. pH)\\n    - *SA node* at heart generates heartbeat - balances Ach and adrenaline\\n      - sympathetic sends info from thoracic spinal cord\\n    - heart sends back *baroreceptor afferents*\\n  2. bladder function\\n    1. parasympathetic in sacral lateral horn make you pee (contracts bladder)\\n    2. turn off sympathetic NS\\n    3. open sphincter muscle (voluntary)\\n    - can also control this via skeletal nervous system\\n    - circuit\\n      - *amygdala* (can\\'t pee when nervous)\\n      - *pontine micturation center* -> *parasympathetic preganglionic neurons* -> *parasympathetic ganglionic neurons*\\n      - *inhibitory local* circuit neurons -> *somatic MNs*\\n  3. sexual function\\n    - *Viagra* turns on parasympathetic NS\\n      - also gives temporal color blindness\\n    - sympathetic involved in ejaculation\\n      - temporal correlation (\"Point and Shoot\")',\n",
       " \"---\\nlayout: notes\\ntitle: Development\\ncategory: neuro\\n---\\n\\n#  development\\n\\nnotes from Neuroscience, 5th edition + Intro to neurobiology course at UVA\\n\\n## 22 early development\\n- ways to study\\n\\t1. top-down: rosy retrospection\\n\\t2. bottom-up: e.g. LTP/LTD\\n\\t3. human disease: stroke-by-stroke\\n\\t4. development=*ontogeny*\\n- timeframe\\n\\t- month 1 - gastrulation\\n\\t\\t- most sensitive time for mom\\n\\t- month 2-5 - cells being born\\n\\t- up to year 2 - axon guidance / synapse formation\\n1. *gastrulation* - process by which early embryo undergoes folds = shapes of NS\\n\\t- diseases\\n\\t\\t- *spina bifida* - neural tube fails to seal\\n\\t\\t\\t- vitamin B12 can fix this\\n\\t\\t- *anencephaly* - neural tube fails to close higher up\\n\\t- parts\\n\\t\\t1. *roofplate* at top (back)\\n\\t\\t2. *floorplate* on bottom (stomach)\\n\\t\\t3. *neural crest* - pinches off top of roofplate\\n2. *neuroblasts* = classic stem cells\\n\\t- assymetric division - cells generate themselves and differentiated progeny\\n\\t- ultimate stem cell - fertilized eggs\\n3. *differentiation*\\n\\t- cells made by neuroblasts decide what they are going to become\\n\\t- *morphogens*\\n\\t\\t- *BMP* - roofplate\\n\\t\\t\\t- *cyclopia* - fatal defect in BMP\\n\\t\\t- *Hedge hogs* - at floor plate\\n\\t\\t- *Retinoids* - axial, affect skin\\n\\t\\t\\t- affected by *thalidomide* - helps morning sickness but causes missing limb segments\\n\\t\\t\\t- also affected by *accutane*\\n\\t\\t- *FGFs* - axial symmetry\\n\\t\\t- *Wnts* - skin, gut, hair\\n\\t\\t\\t- loss of wnts is loss of hair\\n\\t- floor plate loses function after embryogenesis except glioblastoma\\n\\t- measure BMP and HH gradient to figure out where you are\\n\\t\\t- treat ALS by adding HH to make more alpha motor neurons\\n\\t1. dorsal direction\\n\\t\\t- roofplate makes BMP\\n\\t\\t- low HH - interneurons, sensory neurons (ex. nociceptors)\\n\\t\\t- even BMP/HH - sympathetic\\n\\t\\t- high HH - more motor neurons\\n\\t\\t- *floorplate* makes HH (hedge hog)\\n\\t2. axial specification (anterior/posterior)\\n\\t\\t- tube swells into bulbs that become cerebellum, superior colliculus, cortex\\n\\t- *homeotic genes* = *hox genes* - set of genes (transcription factors) in order on chromosome\\n\\t\\t- order corresponds to order of your body parts\\n\\t\\t- *rhombomeres* - segments in brainstem made by hox gene patterns\\n4. *lineages*\\n\\t- when neuroblast is born, starts producing progeny (family tree of neuron types)\\n\\t- very often, cells are produced in certain order\\n\\t- timing: cell-cell interations and tyrosine kinases determine order\\n\\t- first alpha neurons, then GABAergic to control those, last is glia\\n\\t- neural crest function\\n\\t\\t- migratory - moves out and divides: ![](../assets/22_1.png)\\n\\t\\t- *neuroblastoma* - developed early - severe problem because missing parts of NS\\n\\t\\t- makes DRG and associated glial cells (schwann cells)\\n\\t\\t- makes sympathetic NS and target ganglia, enteric NS, parasympathetic NS targets\\n\\t\\t- makes *melanocytes* - know how to migrate and divide but can make *melanoma* (cancer)\\n\\t- cortex is made inside out (6->1)\\n\\t\\t- starts with stem cells called *radial glia*\\n\\t\\t- *cortical dysplasia* - missing a layer / duplicating a layer\\n\\t\\t\\t- small part with 2 layer 3s - severe epilepsy\\n5. *cell death*\\n\\t\\n\\t- 1/2 of cells die in development\\n6. *axon guidance* (ch 23)\\n\\t- each cell born and axon grows and are guided to a target\\n\\t- dendrite basically follows same rules\\n7. *synapse formation* (ch 23, 24)\\n\\t- pruning and plasticity\\n\\t- NMDA receptor type\\n\\t- form synapses and if they don’t look right - get rid of them\\n\\t- K1/K-1 synapses breaking and forming \\n\\t- after age 21, K-1 starts increasing and net loss of synapses\\n\\n## 23 circuit formation\\n- *growth cone* - motile tip of axon\\n\\t- *actin* tip\\n\\t\\t1. *lamellipodium* - sheet (hand)\\n\\t\\t2. *filopodium* - huge curves (fingers)\\n\\t\\t\\t- chemo attraction (actin assembly) and chemo repulsion (actin disassembly)\\n\\t- microtubule shaft - *tubulin* is much more cemented in\\n\\t- *mauthner cell* of tadpole - first recorded growth cone\\n\\t- can't regrow (that's why we can't regrow spinal cord)\\n- signals in growing axons\\n\\t1. *pioneer axons* (Betz cells) are first - often die\\n\\t2. *follower axons* (other Betz cells) can jump onto these and connect before pioneer dies\\n\\t- *trophic support* - neuron survives on contact\\n- frog *tectum* (has superior colliculus) with map of retina: ![](../assets/23_1.png)\\n\\t- *ephrin* (EPH) repulses axon\\n\\t\\t- retinal NT -> tectum AP\\n\\t\\t- axons have different amount of EPH receptors (in retina temporal has more than nasal)\\n\\t\\t- gradient of EPH (in tectum anterior has less than posterior)\\n\\t\\t- if we flip eye upside down (on nasal-temporal axis), image will be upside down\\n- 3 classes of axon guidance molecules: ![](../assets/23_2.png)\\n\\t1. *ECM/integrins*\\n\\t2. *NCAM* (homophilic—binds to another neuron that is NCAM),\\n\\t\\t- follower neurons bind to pioneer through NCAM-NCAM interactions\\n\\t3. *Cadherin* (homophilic)\\n\\t\\t- involved in recognition of being some place\\n- 4 important ligands/receptors\\n\\t1. *ephrins/eph*\\n\\t\\t- gradient of eph receptor\\n\\t2. *netrin/dcc* = guidance moleculereceptor = DCC\\n\\t\\t- attracts axons to floorplate (midline)\\n\\t\\t- cells without DCC don't cross midline\\n\\t3. *slit/robo* - receptor is slit\\n\\t\\t- chases axons off (away from midline)\\n\\t\\t- axons not destined to cross midline are born expressing robo\\n\\t\\t- axons destined to cross the midline only express robo after crossing\\n\\t\\t- if DCC (-) and robo (-) will continue wandering around\\n\\t\\t- *robo 4* is associated with Tourette's\\n\\t4. *semaphorins/plexins*\\n\\t- combinatorial code - use combinations of these to guide axons\\n\\t- these are the same genes that move cancer around\\n- synaptic formation\\n\\t- *neuroexins* - further recognition\\n\\t\\t- turn up in autism and schizophrenia\\n\\t- *DSCAM*\\n\\t\\t- associated with Down's syndrome\\n\\t\\t- doesn't use gradients\\n\\t\\t- makes different kinds of proteins by differential slicing\\n- competition\\n\\t- *neurotrophins* are secreted by muscle\\n\\t\\t- in early development, a muscle fiber has many alpha motor neurons innervating it\\n\\t\\t- all innervating neurons suck up neurotrophin and whichever sucks up most, kills all the others\\n\\t\\t- eventually, each muscle fiber is innervated by one alpha motor neuron\\n\\t\\t- only enough neurotrophin in target cells for a certain number of synapses\\n\\t- happens everywhere\\n\\t\\t- ex. sympathetic ganglia\\n\\t\\t- ex. sensory neurons in skin get axons to correct cell types based on neurotrophin\\n\\t\\t\\t- merkel - **BDNF**\\n\\t\\t\\t- proprioceptor - *NT3*\\n\\t\\t\\t- nociceptor - *NGF*\\n\\t\\t- ex. muscles - produce NGF\\n\\t\\t\\t- treating ALS with NGF hyperactivates sensory neurons with trkA -> causes chicken pox\\n\\t- signals/receptors\\n\\t\\t1. NGF - *trk a* (*Trk receptor* - survival signaling pathways)\\n\\t\\t2. BDNF - *trk b*\\n\\t\\t3. NT3 - trk b and *c*\\n\\t\\t4. NT4/5 - trk b\\n\\t\\t- all bind *p75* (death receptor)\\n\\t\\t- want to keep neurotrophins local, because there aren't that many of them\\n\\n## 24 plasticity in systems\\n- experience-dependent plasticity - ![](../assets/24_1.png)\\n\\t- ex. ducks imprinting is non-reversable\\n\\t- learning is crystallized during *critical period*\\n\\t\\t- *CREB* and protein synthesis\\n\\t\\t- *NMDA* receptors\\n\\t\\t- *epigenetics* - histones control transcription and other things\\n\\t- follow *Hebb's postulate* - fire together, wire together\\n\\t\\t- different eyes firing together will sync up (NMDA receptors to strengthen synapses)\\n- systems\\n\\t1. *ocular dominance*\\n\\t\\t- left/right neurons terminate in adjacent zones\\n\\t\\t- LGN in cortex uses efferents just like superior colliculus\\n\\t\\t- label injected into retina can make it into cortex\\n\\t\\t- cat experiments\\n\\t\\t\\t- some cells see only one eye, some see both\\n\\t\\t\\t- cats need to form visual map in short critical period (<6 days)\\n\\t\\t\\t- this is why you need cochlear implant early\\n\\t\\t\\t- both eyes open - equal OD columns\\n\\t\\t\\t\\t- one eye closed - unequal OD columns\\n\\t\\t\\t\\t- branches coming out of LGN neurons grow more branches based on relative light exposure (they compete for eye's real estate)\\n\\t\\t- *strabismus* = lazy eye - poor coordination with one of the muscles\\n\\t\\t\\t- one eye is not quite seeing\\n\\t\\t\\t - treat with patch on good eye -> allows bad eye to catch up since eyes compete for ocular dominance columns\\n\\t\\t\\t- more stimulus = more branches\\n\\t\\t- dye from retina goes through thalamus into cortex\\n\\t\\t\\t- rabies virus does same thing: cell->ganglion->brain\\n\\t2. *tonotopic map*\\n\\t\\t- connection between MSO and inferior/superior colliculus\\n\\t\\t- playing one tone increases representation\\n\\t\\t- playing white noise disorganizes map\\n\\t\\t- *birdsong*\\n\\t\\t\\t- hear song 10-20 times when young - crystallized\\n\\t\\t\\t- afterwards can't learn new skills\\n\\t3. *stress* \\n\\t\\t- early stress sets stress points later in life\\n\\t\\t- uses serotonin\\n- shifts\\n\\t- *superior colliculus* - integrate visual, auditory, motor to get X,Y coordinate\\n\\t- auditory map - plastic (but only when young)\\n\\t- visual map - not plastic\\n\\t- if you shift visual map (with a prism), auditory map can shift over to meet the visual\\n\\t- *optic neuritis* - ms optic nerve disease that shifts map\\n\\t- only young animals can shift unless they were shifted before and are now unadapting\\n\\n## 25 repair and regeneration\\n1. full repair - human *PNS* - skin, muscles\\n\\t- 1-2 mm/day growth - speed of slow axonal transport\\n\\t- thinnest axons first (thermal receptors and nociceptors)\\n\\t- proprioceptors last\\n\\t- process\\n\\t\\t- *perinerium* / *schwann cells* surrounds axons - helps regeneration\\n\\t\\t- growth cones that are cut form stumps -> distal axons degenerate = *walerian degeneration*\\n\\t\\t- macrophages come in and eat up the damaged stuff\\n\\t\\t- neurotrophins are involved\\n\\t- miswiring is common - regrow and may not find right target\\n\\t\\t- *bell's palsy* - loss of facial nerve - recovers with miswiring (salivary / tear)\\n\\t- *neuromuscular junctions* (NMJ)\\n\\t\\t- damaged cells leave *synaptic ghost* = glia and protein matrix for nerve to regrow into\\n\\t\\t- repairs easily after heavy training\\n2. no repair / glial scar - human *CNS*\\n\\t- no ghost because so spread out\\n\\t- glia cover wound (scar) but can't develop further\\n\\t- has *astrocytes* and *oligodendrocytes* (types of glial cells)\\n\\t\\t- don't support regrowth\\n\\t\\t- involved in scarring\\n\\t- *microglia* - from immune system\\n\\t\\t- control inflammation\\n\\t\\t- release cytokines\\n\\t- *nogo* - protein that blocks regrowth (but there are other proteins as well)\\n\\t- we try repairing with *shunts* - piece of sciatic nerve from other part of body with schwann cells from PNS to try to repair a connection in the CNS\\n3. stem cell regeneration - put new neurons being formed, 2 places in humans\\n\\t- non-human examples\\n\\t\\t- floor plate of lizards can make new tail\\n\\t\\t- fish retina always making new cells\\n\\t\\t- canary brain part has stem cells that learn new song every year\\n\\t- small C14 incorporation after early development\\n\\t\\t\\t- suggests we don't regenerate neurons\\n\\t\\t\\t- C14 was from nuclear testing\\n\\t- human areas that do regenerate\\n\\t\\t1. *hippocampus*\\n\\t\\t\\t- memories you store temporarily\\n\\t\\t2. *subventricular zone* makes *glomeruli* in *olfactory bulb* cells\\n\\t\\t\\t- turnover daily\\n\\t\\t\\t- sensory neurons and their targets constantly die and regenerate\\n\\t\\t3. *niche* - places where stem cells stay alive\\n\\t\\t\\t- ex. places in CNS with WINT molecular signals\\n- damage control - remove these signals for *apoptosis* = cell death\\n\\t- *glutamate increase* - excitotoxicity\\n\\t\\t- can stop with *NMDA blockers*\\n\\t\\t- induce a coma by cooling them down or *GABA drugs*\\n\\t- *cytokines increase* - immune system (like neurotrophins), inflammation\\n\\t- *hypoxia/stress*\\n\\t- *neurotrophin withdrawal*\\n\\t\\t- in stress times neurotrophin goes down\\n\\n## 26 diseases\\n### alzheimer's\\n- ![](../assets/26_1.png)\\n- overview\\n\\t- age-associated - tons of people get it\\n\\t- doesn't kill you, secondary complications like pneumonia will kill you\\n\\t- rate is going up\\n\\t- very expensive to treat\\n- *declarative* memories are affected by Alzheimer's\\n\\t- these are memories that you know\\n- first 2 areas to go in Alzheimer's\\n\\t1. *hippocampus*\\n\\t\\t- patient HM had no hippocampus\\n\\t\\t\\t- no *anterograde* memory - learning new things\\n\\t\\t- hippocampus stores 1 day of info\\n\\t\\t\\t- offloading occurs during sleep (REM sleep) to prefrontal cortex, temporal lobe, V4\\n\\t\\t\\t- dreaming - might see images as you are offloading\\n\\t2. *basal forebrain* - spread synapses all over cortex\\n\\t\\t- uses *Ach*\\n\\t\\t- ignition key for entire cortex\\n- alzheimer's characteristics only found in autopsy\\n\\t- *amyloid plaques*\\n\\t\\t- maybe *A-beta* causes it\\n\\t\\t- A-beta comes from *APP*\\n\\t\\t- *A-beta42* binds to itself \\n\\t\\t\\t- prion (starts making more of itself)\\n\\t\\t\\t- this cycle could be exacerbated by injury\\n\\t\\t\\t- clumps and attracts immune system which kills local important cells\\n\\t\\t\\t\\t- this could cause Alzheimer's\\n\\t\\t\\t- rare genetic mutations in A-beta increase probability you get Alzheimer's\\n\\t\\t\\t- anti-inflammation may be too late\\n\\t\\t\\t- can take drugs that increase Ach functions - ex. cholinergic agonists, cholinesterase inhibitors\\n\\t- *tangles*\\n\\t\\t- tangles made of protein called *Tau*\\n\\t- most people think these are just dead cells resulting from Alzheimer's but some think they cause it\\n\\n### parkinson's\\n- loss of substantia nigra pars compacta dopaminergic neurons\\n\\t- when you get down to 20% what you were born with\\n\\t- dopaminergic neurons form *melanin* = dark color\\n\\t- hits to head can give inflammation\\n- know what they need to do - don't have enough dopamine to act\\n- treat with L Dopa -> something like dopamine -> take out globus pallidus\\n- *Lewy bodies* are clumps of *alpha synuclein* - appear at dopaminergic synapses\\n\\t- clumps like A-beta42\\n\\t- associated with early-onset Parkinson's (rare) associated with genetic mutations\\n- *bradykinesia* - slowness of movement\\n- age can give parksinson's\\n- no evidence that toxins can induce parkinsons\\n- *PTP/ pesticides* can induce Parkinson's in test animals\\n- 1/500 people\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# documents = [open(f) for f in text_files]\\ntfidf = TfidfVectorizer().fit_transform(contents)\\n\\n# no need to normalize, since Vectorizer will return normalized tf-idf\\npairwise_similarity = tfidf * tfidf.T\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
    "tfidf = vect.fit_transform(contents)                                                                                                                                                                                                                       \n",
    "pairwise_similarity = (tfidf * tfidf.T).todense()\n",
    "sim = pairwise_similarity.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1650x1350 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11, 9), dpi=150)\n",
    "plt.imshow(sim)\n",
    "plt.yticks(np.arange(71), labels=fnames, fontsize=8)\n",
    "plt.xticks(np.arange(71), labels=fnames, fontsize=8, rotation='vertical')\n",
    "plt.ylim((-0.5, 70))\n",
    "plt.colorbar(label='Similarity (tf-idf)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
