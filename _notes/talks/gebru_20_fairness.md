- talks available on youtube: https://sites.google.com/view/fatecv-tutorial/schedule
- **timnit gebru** - fairness team at google
	- also emily denton

# FATE/CV 2020 | Computer vision in practice: who is benefiting and who is being harmed?

https://www.youtube.com/watch?time_continue=546&v=0sBE5OyD7fk&feature=emb_logo

- startups
  - faceception startup - profile people based on their image
  - hirevue startup videos - facial recognition for judging interviews
  - clearview ai - search all faces
  - police using facial recognition - harms protestors
  - facial recognition rarely has good uses
  - contributes to mass surveillance
  - can be used to discriminate different ethnicities (e.g. Uighurs in china)
- gender shades work - models for gender classification were worse for black women
  - datasets were biased - PPB introduced to balance things somewhat
- gender recognition is harmful in the first place
  - collecting data without consent is also harmful
- [letter to amazon](https://www.theverge.com/2019/4/3/18291995/amazon-facial-recognition-technology-rekognition-police-ai-researchers-ban-flawed): stop selling facial analysis technology
- combating this technology
  - fashion for fooling facial recognition

# data ethics

https://www.youtube.com/watch?v=v_XBJd1Fxqc&feature=emb_logo

- different types of harms
  - sometimes you need to make sure there aren't disparate error rates across subgroups
  - sometimes the task just should not exist
  - sometimes the manner in which the tool is used is problematic because of who has the power
- technology amplifies our intent
- most people feel that data collection is the most important place to intervene
- people are denied housing based on data-driven discrimination
- collecting data
  - wild west - just collect everything
  - curatorial data - collect very specific data (this can help mitigate bias)
- datasets are value-laden, drive research agendas
- ex. celeba labels gender, attractiveness
- ex. captions use gendered language (e.g. beautiful)

# where do we go?

https://www.youtube.com/watch?v=vpPpwa7W93I&feature=emb_logo

- technology is not value-neutral -- it's political
- model types and metrics embed values
- science is not *neutral, objecive, perspectiveless*
- be aware of your own **positionality**
- concrete steps
  - ethics-informed model evaluations (e.g. disaggregegated evaluations, counterfactual testing)
  - recognize limitations of technical approaches
  - transparent dataset documentation
  - think about perspectives of marginalized groups